{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise V:<br> GANs\n",
    "</center></h1>\n",
    "\n",
    "## Short summary\n",
    "In this exercise, we will design a generative network to generate the last rgb image given the first image. These folder has **three files**: \n",
    "- **configGAN.py:** this involves definitions of all parameters and data paths\n",
    "- **utilsGAN.py:** includes utility functions required to grab and visualize data \n",
    "- **runGAN.ipynb:** contains the script to design, train and test the network \n",
    "\n",
    "Make sure that before running this script, you created an environment and **installed all required libraries** such \n",
    "as keras.\n",
    "\n",
    "## The data\n",
    "There exists also a subfolder called **data** which contains the traning, validation, and testing data each has both RGB input images together with the corresponding ground truth images.\n",
    "\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Ex | Exercise 1| A class definition of a network model  |\n",
    "| 3 | Loading | Needed | Loading parameters and initializing the model |\n",
    "| 4 | Stats | Needed | Show data distribution | \n",
    "| 5 | Data | Needed | Generating the data batches |\n",
    "| 6 | Debug | Needed | Debugging the data |\n",
    "| 7 | Device | Needed | Selecting CPU/GPU |\n",
    "| 8 | Init | Needed | Sets up the timer and other neccessary components |\n",
    "| 9 | Training | Exercise 1-2 | Training the model   |\n",
    "| 10 | Testing | Exercise 1-2| Testing the  method   |  \n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells. It is important that you do this in the correct order, starting from the top and continuing with the next cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "\n",
    "There is no need to provide any report. However, implemented network architecuture and observed experimental results must be presented as a short presentation in the last lecture, May 28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We first start with importing all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating network model using gpu 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from configGAN import *\n",
    "cfg = flying_objects_config()\n",
    "if cfg.GPU >=0:\n",
    "    print(\"creating network model using gpu \" + str(cfg.GPU))\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(cfg.GPU)\n",
    "elif cfg.GPU >=-1:\n",
    "    print(\"creating network model using cpu \")  \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utilsGAN import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "import pprint\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv3D, Conv2D, Conv1D, Convolution2D, Deconvolution2D, Cropping2D, UpSampling2D\n",
    "from keras.layers import Input, Conv2DTranspose, ConvLSTM2D, TimeDistributed\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Concatenate, concatenate, Reshape\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.layers import Input, merge\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout, LeakyReLU\n",
    "import keras.backend as kb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Here, we have the network model class definition. In this class, the most important functions are **build_generator()** and **build_discriminator()**. As defined in the exercises section, your task is to update the both network architectures defined in these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel():\n",
    "    def __init__(self, batch_size=32, inputShape=(64, 64, 3), dropout_prob=0.25): \n",
    "        self.batch_size = batch_size\n",
    "        self.inputShape = inputShape\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Calculate the shape of patches\n",
    "        patch = int(self.inputShape[0] / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "  \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=Adam(0.0002, 0.5),metrics=['accuracy'])\n",
    " \n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        first_frame = Input(shape=self.inputShape)\n",
    "        last_frame = Input(shape=self.inputShape)\n",
    "\n",
    "        # By conditioning on the first frame generate a fake version of the last frame\n",
    "        fake_last_frame = self.generator(first_frame)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # Discriminators determines validity of fake and condition first image pairs\n",
    "        valid = self.discriminator([fake_last_frame, first_frame])\n",
    "\n",
    "        self.combined = Model(inputs=[last_frame, first_frame], outputs=[valid, fake_last_frame])\n",
    "        self.combined.compile(loss=['mse', 'mae'], # mean squared and mean absolute errors\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "    def build_generator(self):\n",
    "        inputs = Input(shape=self.inputShape)\n",
    "        \n",
    "        conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(inputs)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "        conv1= Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv1)\n",
    "        conv1 = BatchNormalization(momentum=0.8)(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "        conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool1)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "        conv2= Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv2)\n",
    "        conv2 = BatchNormalization(momentum=0.8)(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        \n",
    "        conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool2)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "        conv3= Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv3)\n",
    "        conv3 = BatchNormalization(momentum=0.8)(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        \n",
    "        conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool3)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "        conv4= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv4)\n",
    "        conv4 = BatchNormalization(momentum=0.8)(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        '''pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "        \n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(pool4)\n",
    "        conv5= Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv5)\n",
    "        #pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        \n",
    "        up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "        up6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        up6 = Concatenate(axis=3)([drop4, up6])\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up6)\n",
    "        conv6 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv6)'''\n",
    "\n",
    "        up7 = UpSampling2D(size=(2, 2))(conv4)\n",
    "        up7 = Conv2D(filters=256,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "        up7 = BatchNormalization(momentum=0.8)(up7)\n",
    "        up7 = Concatenate(axis=3)([conv3, up7])\n",
    "        conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "        conv7 = Conv2D(filters=256,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv7)\n",
    "        conv7 = BatchNormalization(momentum=0.8)(conv7)\n",
    "\n",
    "        up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "        up8 = Conv2D(filters=128,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "        up8 = BatchNormalization(momentum=0.8)(up8)\n",
    "        up8 = Concatenate(axis=3)([conv2, up8])\n",
    "        conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up8)\n",
    "        conv8 = BatchNormalization(momentum=0.8)(conv8)\n",
    "        conv8 = Conv2D(filters=128,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "        up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "        up9 = Conv2D(filters=64,kernel_size=2,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "        up9 = BatchNormalization(momentum=0.8)(up9)\n",
    "        up9 = Concatenate(axis=3)([conv1, up9])\n",
    "        conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(up9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        conv9 = Conv2D(filters=64,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        conv9 = Conv2D(filters=32,kernel_size=3,activation='relu',padding='same',kernel_initializer='he_normal')(conv9)\n",
    "        conv9 = BatchNormalization(momentum=0.8)(conv9)\n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "        model.summary()\n",
    " \n",
    "        '''inputs = Input(shape=self.inputShape)\n",
    "        print(inputs.shape)\n",
    " \n",
    "        down1 = Conv2D(32, (3, 3),padding='same')(inputs)\n",
    "        down1 = Activation('relu')(down1) \n",
    "        down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "         \n",
    "        down2 = Conv2D(64, (3, 3), padding='same')(down1_pool)\n",
    "        down2 = Activation('relu')(down2) \n",
    "         \n",
    "\n",
    "        up1 = UpSampling2D((2, 2))(down2)\n",
    "        up1 = concatenate([down1, up1], axis=3)\n",
    "        up1 = Conv2D(256, (3, 3), padding='same')(up1) \n",
    "        up1 = Activation('relu')(up1) \n",
    "        \n",
    "        \n",
    "        up2 = Conv2D(256, (3, 3), padding='same')(up1) \n",
    "        up2 = Activation('relu')(up2) \n",
    "        \n",
    "        nbr_img_channels = self.inputShape[2]\n",
    "        outputs = Conv2D(nbr_img_channels, (1, 1), activation='sigmoid')(up2)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "        model.summary()'''\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "  \n",
    "        last_img = Input(shape=self.inputShape)\n",
    "        first_img = Input(shape=self.inputShape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([last_img, first_img])\n",
    "  \n",
    "        d1 = Conv2D(32, (3, 3), strides=2, padding='same')(combined_imgs) \n",
    "        d1 = Activation('relu')(d1) \n",
    "        d2 = Conv2D(64, (3, 3), strides=2, padding='same')(d1)\n",
    "        d2 = Activation('relu')(d2) \n",
    "        d3 = Conv2D(128, (3, 3), strides=2, padding='same')(d2)\n",
    "        d3 = Activation('relu')(d3) \n",
    "         \n",
    "        validity = Conv2D(1, (3, 3), strides=2, padding='same')(d3)\n",
    "\n",
    "        model = Model([last_img, first_img], validity)\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We import the network **hyperparameters** and build a simple network by calling the class introduced in the previous step. Please note that to change the hyperparameters, you just need to change the values in the file called **configPredictor.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 6)    0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 16, 32)   1760        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 16, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 64)     18496       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 8, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    73856       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4, 4, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2, 2, 1)      1153        activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 95,265\n",
      "Trainable params: 95,265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   896         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 16, 16, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 8, 8, 128)    73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 8, 8, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4, 4, 256)    1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 4, 4, 256)    590080      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4, 4, 256)    1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 256)    262400      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 256)    1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 384)    0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 256)    884992      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 256)    1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 256)    590080      batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 256)    1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 128)  131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 192)  0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  221312      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  147584      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 128)  0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 64)   32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 96)   0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 64)   55360       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 64)   36928       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 32)   18464       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 3)    99          batch_normalization_16[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,562,339\n",
      "Trainable params: 3,557,923\n",
      "Non-trainable params: 4,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (cfg.IMAGE_HEIGHT, cfg.IMAGE_WIDTH, cfg.IMAGE_CHANNEL)\n",
    "modelObj = GANModel(batch_size=cfg.BATCH_SIZE, inputShape=image_shape,\n",
    "                                 dropout_prob=cfg.DROPOUT_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) We call the utility function **show_statistics** to display the data distribution. This is just for debugging purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "##################### Training Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 10817\n",
      "total class number \t 3\n",
      "class square \t 3488 images\n",
      "class triangle \t 3703 images\n",
      "class circular \t 3626 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Validation Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2241\n",
      "total class number \t 3\n",
      "class circular \t 713 images\n",
      "class square \t 783 images\n",
      "class triangle \t 745 images\n",
      "######################################################################\n",
      "\n",
      "######################################################################\n",
      "##################### Testing Data Statistics #####################\n",
      "######################################################################\n",
      "total image number \t 2220\n",
      "total class number \t 3\n",
      "class circular \t 722 images\n",
      "class square \t 765 images\n",
      "class triangle \t 733 images\n",
      "######################################################################\n"
     ]
    }
   ],
   "source": [
    "#### show how the data looks like\n",
    "show_statistics(cfg.training_data_dir, fineGrained=False, title=\" Training Data Statistics \")\n",
    "show_statistics(cfg.validation_data_dir, fineGrained=False, title=\" Validation Data Statistics \")\n",
    "show_statistics(cfg.testing_data_dir, fineGrained=False, title=\" Testing Data Statistics \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We now create batch generators to get small batches from the entire dataset. There is no need to change these functions as they already return **normalized inputs as batches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch generators are created!\n"
     ]
    }
   ],
   "source": [
    "nbr_train_data = get_dataset_size(cfg.training_data_dir)\n",
    "nbr_valid_data = get_dataset_size(cfg.validation_data_dir)\n",
    "nbr_test_data = get_dataset_size(cfg.testing_data_dir)\n",
    "train_batch_generator = generate_lastframepredictor_batches(cfg.training_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "valid_batch_generator = generate_lastframepredictor_batches(cfg.validation_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "test_batch_generator = generate_lastframepredictor_batches(cfg.testing_data_dir, image_shape, cfg.BATCH_SIZE)\n",
    "print(\"Data batch generators are created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) We can visualize how the data looks like for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (30, 32, 32, 3) float32 0.0 1.0\n",
      "train_y (30, 32, 32, 3) float32 0.0 1.0\n",
      "{'BATCH_SIZE': 30,\n",
      " 'CLASSES': ['square_red',\n",
      "             'square_green',\n",
      "             'square_blue',\n",
      "             'square_yellow',\n",
      "             'triangle_red',\n",
      "             'triangle_green',\n",
      "             'triangle_blue',\n",
      "             'triangle_yellow',\n",
      "             'circular_red',\n",
      "             'circular_green',\n",
      "             'circular_blue',\n",
      "             'circular_yellow'],\n",
      " 'DATA_AUGMENTATION': True,\n",
      " 'DEBUG_MODE': True,\n",
      " 'DROPOUT_PROB': 0.5,\n",
      " 'GPU': 0,\n",
      " 'IMAGE_CHANNEL': 3,\n",
      " 'IMAGE_HEIGHT': 32,\n",
      " 'IMAGE_WIDTH': 32,\n",
      " 'LEARNING_RATE': 0.01,\n",
      " 'LR_DECAY_FACTOR': 0.1,\n",
      " 'NUM_EPOCHS': 5,\n",
      " 'PRINT_EVERY': 20,\n",
      " 'SAVE_EVERY': 1,\n",
      " 'SEQUENCE_LENGTH': 10,\n",
      " 'testing_data_dir': '../data/FlyingObjectDataset_10K/testing',\n",
      " 'training_data_dir': '../data/FlyingObjectDataset_10K/training',\n",
      " 'validation_data_dir': '../data/FlyingObjectDataset_10K/validation'}\n"
     ]
    }
   ],
   "source": [
    "if cfg.DEBUG_MODE:\n",
    "    t_x, t_y,_ = next(train_batch_generator)\n",
    "    print('train_x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "    print('train_y', t_y.shape, t_y.dtype, t_y.min(), t_y.max()) \n",
    "    #plot_sample_lastframepredictor_data_with_groundtruth(t_x, t_y, t_y)\n",
    "    pprint.pprint (cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Start timer and init matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "fake = np.zeros((cfg.BATCH_SIZE,) + modelObj.disc_patch)\n",
    "# log file\n",
    "output_log_dir = \"./logs/{}\".format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not os.path.exists(output_log_dir):\n",
    "    os.makedirs(output_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) We can now feed the training and validation data to the network. This will train the network for **some epochs**. Note that the epoch number is also predefined in the file called **configGAN.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.5381139\n",
      "[Epoch 0/5] [Batch 0/360] [D loss: 0.549909] [G loss: 51.827637] time: 0:00:16.458334\n",
      "(30, 32, 32, 3)\n",
      "0.48409462\n",
      "[Epoch 0/5] [Batch 1/360] [D loss: 0.470331] [G loss: 49.959103] time: 0:00:16.564313\n",
      "(30, 32, 32, 3)\n",
      "0.4876071\n",
      "[Epoch 0/5] [Batch 2/360] [D loss: 0.409592] [G loss: 48.253975] time: 0:00:16.669413\n",
      "(30, 32, 32, 3)\n",
      "0.52264667\n",
      "[Epoch 0/5] [Batch 3/360] [D loss: 0.357141] [G loss: 47.305038] time: 0:00:16.777142\n",
      "(30, 32, 32, 3)\n",
      "0.5700949\n",
      "[Epoch 0/5] [Batch 4/360] [D loss: 0.306269] [G loss: 45.743374] time: 0:00:16.881245\n",
      "(30, 32, 32, 3)\n",
      "0.63782007\n",
      "[Epoch 0/5] [Batch 5/360] [D loss: 0.269805] [G loss: 44.852943] time: 0:00:16.991665\n",
      "(30, 32, 32, 3)\n",
      "0.6291484\n",
      "[Epoch 0/5] [Batch 6/360] [D loss: 0.240555] [G loss: 43.152534] time: 0:00:17.095322\n",
      "(30, 32, 32, 3)\n",
      "0.6695692\n",
      "[Epoch 0/5] [Batch 7/360] [D loss: 0.220810] [G loss: 41.462101] time: 0:00:17.199552\n",
      "(30, 32, 32, 3)\n",
      "0.6267225\n",
      "[Epoch 0/5] [Batch 8/360] [D loss: 0.205313] [G loss: 40.974670] time: 0:00:17.303251\n",
      "(30, 32, 32, 3)\n",
      "0.66407245\n",
      "[Epoch 0/5] [Batch 9/360] [D loss: 0.197263] [G loss: 39.233597] time: 0:00:17.411509\n",
      "(30, 32, 32, 3)\n",
      "0.6583834\n",
      "[Epoch 0/5] [Batch 10/360] [D loss: 0.192547] [G loss: 38.538052] time: 0:00:17.520178\n",
      "(30, 32, 32, 3)\n",
      "0.6724022\n",
      "[Epoch 0/5] [Batch 11/360] [D loss: 0.189016] [G loss: 37.231899] time: 0:00:17.625067\n",
      "(30, 32, 32, 3)\n",
      "0.60974866\n",
      "[Epoch 0/5] [Batch 12/360] [D loss: 0.188150] [G loss: 36.986938] time: 0:00:17.730253\n",
      "(30, 32, 32, 3)\n",
      "0.61999947\n",
      "[Epoch 0/5] [Batch 13/360] [D loss: 0.179536] [G loss: 36.196121] time: 0:00:17.833876\n",
      "(30, 32, 32, 3)\n",
      "0.65101093\n",
      "[Epoch 0/5] [Batch 14/360] [D loss: 0.175919] [G loss: 35.911747] time: 0:00:17.941697\n",
      "(30, 32, 32, 3)\n",
      "0.6095138\n",
      "[Epoch 0/5] [Batch 15/360] [D loss: 0.169033] [G loss: 35.544861] time: 0:00:18.044604\n",
      "(30, 32, 32, 3)\n",
      "0.67077035\n",
      "[Epoch 0/5] [Batch 16/360] [D loss: 0.165249] [G loss: 33.888634] time: 0:00:18.148706\n",
      "(30, 32, 32, 3)\n",
      "0.69659853\n",
      "[Epoch 0/5] [Batch 17/360] [D loss: 0.163879] [G loss: 34.114609] time: 0:00:18.252072\n",
      "(30, 32, 32, 3)\n",
      "0.6259539\n",
      "[Epoch 0/5] [Batch 18/360] [D loss: 0.157708] [G loss: 33.980537] time: 0:00:18.356913\n",
      "(30, 32, 32, 3)\n",
      "0.6964585\n",
      "[Epoch 0/5] [Batch 19/360] [D loss: 0.151712] [G loss: 33.462666] time: 0:00:18.464575\n",
      "(30, 32, 32, 3)\n",
      "0.58049184\n",
      "[Epoch 0/5] [Batch 20/360] [D loss: 0.147949] [G loss: 32.712940] time: 0:00:18.566845\n",
      "(30, 32, 32, 3)\n",
      "0.6377668\n",
      "[Epoch 0/5] [Batch 21/360] [D loss: 0.141870] [G loss: 31.928783] time: 0:00:18.672146\n",
      "(30, 32, 32, 3)\n",
      "0.59136385\n",
      "[Epoch 0/5] [Batch 22/360] [D loss: 0.137951] [G loss: 31.922724] time: 0:00:18.777264\n",
      "(30, 32, 32, 3)\n",
      "0.7405348\n",
      "[Epoch 0/5] [Batch 23/360] [D loss: 0.129335] [G loss: 32.182880] time: 0:00:18.884847\n",
      "(30, 32, 32, 3)\n",
      "0.70759726\n",
      "[Epoch 0/5] [Batch 24/360] [D loss: 0.125162] [G loss: 31.644518] time: 0:00:18.990039\n",
      "(30, 32, 32, 3)\n",
      "0.5806611\n",
      "[Epoch 0/5] [Batch 25/360] [D loss: 0.120643] [G loss: 30.861622] time: 0:00:19.094303\n",
      "(30, 32, 32, 3)\n",
      "0.6999275\n",
      "[Epoch 0/5] [Batch 26/360] [D loss: 0.110915] [G loss: 31.710089] time: 0:00:19.199706\n",
      "(30, 32, 32, 3)\n",
      "0.71770453\n",
      "[Epoch 0/5] [Batch 27/360] [D loss: 0.099595] [G loss: 31.724989] time: 0:00:19.304356\n",
      "(30, 32, 32, 3)\n",
      "0.6596853\n",
      "[Epoch 0/5] [Batch 28/360] [D loss: 0.100301] [G loss: 31.233009] time: 0:00:19.411764\n",
      "(30, 32, 32, 3)\n",
      "0.6640393\n",
      "[Epoch 0/5] [Batch 29/360] [D loss: 0.089297] [G loss: 30.787228] time: 0:00:19.516974\n",
      "(30, 32, 32, 3)\n",
      "0.6757483\n",
      "[Epoch 0/5] [Batch 30/360] [D loss: 0.091615] [G loss: 31.454748] time: 0:00:19.622102\n",
      "(30, 32, 32, 3)\n",
      "0.61277705\n",
      "[Epoch 0/5] [Batch 31/360] [D loss: 0.080779] [G loss: 31.136806] time: 0:00:19.727635\n",
      "(30, 32, 32, 3)\n",
      "0.6747122\n",
      "[Epoch 0/5] [Batch 32/360] [D loss: 0.073253] [G loss: 29.799067] time: 0:00:19.843982\n",
      "(30, 32, 32, 3)\n",
      "0.6934599\n",
      "[Epoch 0/5] [Batch 33/360] [D loss: 0.069213] [G loss: 29.487432] time: 0:00:19.959846\n",
      "(30, 32, 32, 3)\n",
      "0.5833559\n",
      "[Epoch 0/5] [Batch 34/360] [D loss: 0.062853] [G loss: 28.865429] time: 0:00:20.069872\n",
      "(30, 32, 32, 3)\n",
      "0.68377095\n",
      "[Epoch 0/5] [Batch 35/360] [D loss: 0.058583] [G loss: 28.491634] time: 0:00:20.173818\n",
      "(30, 32, 32, 3)\n",
      "0.63461787\n",
      "[Epoch 0/5] [Batch 36/360] [D loss: 0.049610] [G loss: 29.282576] time: 0:00:20.277410\n",
      "(30, 32, 32, 3)\n",
      "0.6783123\n",
      "[Epoch 0/5] [Batch 37/360] [D loss: 0.051948] [G loss: 30.134922] time: 0:00:20.385703\n",
      "(30, 32, 32, 3)\n",
      "0.5555317\n",
      "[Epoch 0/5] [Batch 38/360] [D loss: 0.051951] [G loss: 29.568268] time: 0:00:20.493295\n",
      "(30, 32, 32, 3)\n",
      "0.73613054\n",
      "[Epoch 0/5] [Batch 39/360] [D loss: 0.044106] [G loss: 28.508165] time: 0:00:20.600030\n",
      "(30, 32, 32, 3)\n",
      "0.5873756\n",
      "[Epoch 0/5] [Batch 40/360] [D loss: 0.040118] [G loss: 29.199327] time: 0:00:20.711664\n",
      "(30, 32, 32, 3)\n",
      "0.6894801\n",
      "[Epoch 0/5] [Batch 41/360] [D loss: 0.032591] [G loss: 29.012569] time: 0:00:20.823645\n",
      "(30, 32, 32, 3)\n",
      "0.69776034\n",
      "[Epoch 0/5] [Batch 42/360] [D loss: 0.035274] [G loss: 29.041952] time: 0:00:20.929766\n",
      "(30, 32, 32, 3)\n",
      "0.63328624\n",
      "[Epoch 0/5] [Batch 43/360] [D loss: 0.032419] [G loss: 28.190294] time: 0:00:21.032268\n",
      "(30, 32, 32, 3)\n",
      "0.61188895\n",
      "[Epoch 0/5] [Batch 44/360] [D loss: 0.026282] [G loss: 28.331217] time: 0:00:21.136333\n",
      "(30, 32, 32, 3)\n",
      "0.633965\n",
      "[Epoch 0/5] [Batch 45/360] [D loss: 0.019017] [G loss: 28.585480] time: 0:00:21.243660\n",
      "(30, 32, 32, 3)\n",
      "0.6632505\n",
      "[Epoch 0/5] [Batch 46/360] [D loss: 0.025914] [G loss: 28.620256] time: 0:00:21.355235\n",
      "(30, 32, 32, 3)\n",
      "0.6726231\n",
      "[Epoch 0/5] [Batch 47/360] [D loss: 0.025095] [G loss: 27.864460] time: 0:00:21.463846\n",
      "(30, 32, 32, 3)\n",
      "0.66880184\n",
      "[Epoch 0/5] [Batch 48/360] [D loss: 0.027510] [G loss: 27.831379] time: 0:00:21.568007\n",
      "(30, 32, 32, 3)\n",
      "0.6868572\n",
      "[Epoch 0/5] [Batch 49/360] [D loss: 0.017744] [G loss: 28.621477] time: 0:00:21.672375\n",
      "(30, 32, 32, 3)\n",
      "0.71570605\n",
      "[Epoch 0/5] [Batch 50/360] [D loss: 0.035204] [G loss: 28.414099] time: 0:00:21.780219\n",
      "(30, 32, 32, 3)\n",
      "0.6812844\n",
      "[Epoch 0/5] [Batch 51/360] [D loss: 0.015892] [G loss: 27.729759] time: 0:00:21.883528\n",
      "(30, 32, 32, 3)\n",
      "0.56616116\n",
      "[Epoch 0/5] [Batch 52/360] [D loss: 0.040340] [G loss: 27.133305] time: 0:00:21.987717\n",
      "(30, 32, 32, 3)\n",
      "0.62427443\n",
      "[Epoch 0/5] [Batch 53/360] [D loss: 0.015267] [G loss: 27.236406] time: 0:00:22.092199\n",
      "(30, 32, 32, 3)\n",
      "0.6732243\n",
      "[Epoch 0/5] [Batch 54/360] [D loss: 0.029184] [G loss: 26.634003] time: 0:00:22.198808\n",
      "(30, 32, 32, 3)\n",
      "0.6988921\n",
      "[Epoch 0/5] [Batch 55/360] [D loss: 0.012405] [G loss: 27.540241] time: 0:00:22.308752\n",
      "(30, 32, 32, 3)\n",
      "0.59985906\n",
      "[Epoch 0/5] [Batch 56/360] [D loss: 0.033042] [G loss: 26.882538] time: 0:00:22.417758\n",
      "(30, 32, 32, 3)\n",
      "0.6673638\n",
      "[Epoch 0/5] [Batch 57/360] [D loss: 0.013777] [G loss: 27.965006] time: 0:00:22.521129\n",
      "(30, 32, 32, 3)\n",
      "0.7067519\n",
      "[Epoch 0/5] [Batch 58/360] [D loss: 0.042417] [G loss: 27.398064] time: 0:00:22.625514\n",
      "(30, 32, 32, 3)\n",
      "0.7255245\n",
      "[Epoch 0/5] [Batch 59/360] [D loss: 0.013616] [G loss: 27.624176] time: 0:00:22.734215\n",
      "(30, 32, 32, 3)\n",
      "0.7559425\n",
      "[Epoch 0/5] [Batch 60/360] [D loss: 0.022841] [G loss: 27.041945] time: 0:00:22.841576\n",
      "(30, 32, 32, 3)\n",
      "0.7242286\n",
      "[Epoch 0/5] [Batch 61/360] [D loss: 0.010376] [G loss: 27.193689] time: 0:00:22.949913\n",
      "(30, 32, 32, 3)\n",
      "0.68294245\n",
      "[Epoch 0/5] [Batch 62/360] [D loss: 0.018058] [G loss: 26.530764] time: 0:00:23.053327\n",
      "(30, 32, 32, 3)\n",
      "0.65113014\n",
      "[Epoch 0/5] [Batch 63/360] [D loss: 0.011332] [G loss: 26.775389] time: 0:00:23.157665\n",
      "(30, 32, 32, 3)\n",
      "0.6978639\n",
      "[Epoch 0/5] [Batch 64/360] [D loss: 0.027662] [G loss: 26.188251] time: 0:00:23.270475\n",
      "(30, 32, 32, 3)\n",
      "0.72482497\n",
      "[Epoch 0/5] [Batch 65/360] [D loss: 0.009253] [G loss: 26.240917] time: 0:00:23.375187\n",
      "(30, 32, 32, 3)\n",
      "0.6912899\n",
      "[Epoch 0/5] [Batch 66/360] [D loss: 0.016580] [G loss: 26.178284] time: 0:00:23.479651\n",
      "(30, 32, 32, 3)\n",
      "0.75569296\n",
      "[Epoch 0/5] [Batch 67/360] [D loss: 0.011034] [G loss: 26.254263] time: 0:00:23.587011\n",
      "(30, 32, 32, 3)\n",
      "0.67300814\n",
      "[Epoch 0/5] [Batch 68/360] [D loss: 0.019061] [G loss: 26.141361] time: 0:00:23.693531\n",
      "(30, 32, 32, 3)\n",
      "0.65458685\n",
      "[Epoch 0/5] [Batch 69/360] [D loss: 0.011926] [G loss: 26.092569] time: 0:00:23.797649\n",
      "(30, 32, 32, 3)\n",
      "0.7657525\n",
      "[Epoch 0/5] [Batch 70/360] [D loss: 0.022256] [G loss: 25.585665] time: 0:00:23.901324\n",
      "(30, 32, 32, 3)\n",
      "0.67516166\n",
      "[Epoch 0/5] [Batch 71/360] [D loss: 0.007303] [G loss: 25.614132] time: 0:00:24.005111\n",
      "(30, 32, 32, 3)\n",
      "0.6706293\n",
      "[Epoch 0/5] [Batch 72/360] [D loss: 0.016877] [G loss: 25.816584] time: 0:00:24.109131\n",
      "(30, 32, 32, 3)\n",
      "0.6288673\n",
      "[Epoch 0/5] [Batch 73/360] [D loss: 0.008629] [G loss: 26.774509] time: 0:00:24.220869\n",
      "(30, 32, 32, 3)\n",
      "0.654401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 74/360] [D loss: 0.015724] [G loss: 25.648184] time: 0:00:24.326813\n",
      "(30, 32, 32, 3)\n",
      "0.7491582\n",
      "[Epoch 0/5] [Batch 75/360] [D loss: 0.008585] [G loss: 25.473957] time: 0:00:24.434185\n",
      "(30, 32, 32, 3)\n",
      "0.6888735\n",
      "[Epoch 0/5] [Batch 76/360] [D loss: 0.013013] [G loss: 26.318098] time: 0:00:24.542146\n",
      "(30, 32, 32, 3)\n",
      "0.7386187\n",
      "[Epoch 0/5] [Batch 77/360] [D loss: 0.007275] [G loss: 25.979515] time: 0:00:24.655078\n",
      "(30, 32, 32, 3)\n",
      "0.6743868\n",
      "[Epoch 0/5] [Batch 78/360] [D loss: 0.013529] [G loss: 25.403847] time: 0:00:24.761054\n",
      "(30, 32, 32, 3)\n",
      "0.7180272\n",
      "[Epoch 0/5] [Batch 79/360] [D loss: 0.005862] [G loss: 25.808718] time: 0:00:24.866029\n",
      "(30, 32, 32, 3)\n",
      "0.59154725\n",
      "[Epoch 0/5] [Batch 80/360] [D loss: 0.013414] [G loss: 25.000858] time: 0:00:24.974711\n",
      "(30, 32, 32, 3)\n",
      "0.6319538\n",
      "[Epoch 0/5] [Batch 81/360] [D loss: 0.007359] [G loss: 24.996452] time: 0:00:25.082316\n",
      "(30, 32, 32, 3)\n",
      "0.7460494\n",
      "[Epoch 0/5] [Batch 82/360] [D loss: 0.014791] [G loss: 24.566753] time: 0:00:25.201139\n",
      "(30, 32, 32, 3)\n",
      "0.6744098\n",
      "[Epoch 0/5] [Batch 83/360] [D loss: 0.008624] [G loss: 24.202663] time: 0:00:25.308232\n",
      "(30, 32, 32, 3)\n",
      "0.73741657\n",
      "[Epoch 0/5] [Batch 84/360] [D loss: 0.008094] [G loss: 24.451578] time: 0:00:25.415534\n",
      "(30, 32, 32, 3)\n",
      "0.6985078\n",
      "[Epoch 0/5] [Batch 85/360] [D loss: 0.008931] [G loss: 24.649761] time: 0:00:25.518998\n",
      "(30, 32, 32, 3)\n",
      "0.6665113\n",
      "[Epoch 0/5] [Batch 86/360] [D loss: 0.006119] [G loss: 25.579050] time: 0:00:25.626211\n",
      "(30, 32, 32, 3)\n",
      "0.69980526\n",
      "[Epoch 0/5] [Batch 87/360] [D loss: 0.018620] [G loss: 25.659674] time: 0:00:25.734060\n",
      "(30, 32, 32, 3)\n",
      "0.65525824\n",
      "[Epoch 0/5] [Batch 88/360] [D loss: 0.008443] [G loss: 24.554808] time: 0:00:25.841190\n",
      "(30, 32, 32, 3)\n",
      "0.7234203\n",
      "[Epoch 0/5] [Batch 89/360] [D loss: 0.009522] [G loss: 24.984900] time: 0:00:25.945758\n",
      "(30, 32, 32, 3)\n",
      "0.71616393\n",
      "[Epoch 0/5] [Batch 90/360] [D loss: 0.017565] [G loss: 25.254713] time: 0:00:26.051847\n",
      "(30, 32, 32, 3)\n",
      "0.6662559\n",
      "[Epoch 0/5] [Batch 91/360] [D loss: 0.007163] [G loss: 23.863464] time: 0:00:26.165193\n",
      "(30, 32, 32, 3)\n",
      "0.74574655\n",
      "[Epoch 0/5] [Batch 92/360] [D loss: 0.006464] [G loss: 24.501566] time: 0:00:26.274584\n",
      "(30, 32, 32, 3)\n",
      "0.8014347\n",
      "[Epoch 0/5] [Batch 93/360] [D loss: 0.007888] [G loss: 25.566603] time: 0:00:26.379310\n",
      "(30, 32, 32, 3)\n",
      "0.57246673\n",
      "[Epoch 0/5] [Batch 94/360] [D loss: 0.008404] [G loss: 25.050486] time: 0:00:26.492688\n",
      "(30, 32, 32, 3)\n",
      "0.6737165\n",
      "[Epoch 0/5] [Batch 95/360] [D loss: 0.005996] [G loss: 24.492018] time: 0:00:26.598092\n",
      "(30, 32, 32, 3)\n",
      "0.65238243\n",
      "[Epoch 0/5] [Batch 96/360] [D loss: 0.011618] [G loss: 24.518606] time: 0:00:26.701349\n",
      "(30, 32, 32, 3)\n",
      "0.63166744\n",
      "[Epoch 0/5] [Batch 97/360] [D loss: 0.006882] [G loss: 24.163702] time: 0:00:26.809558\n",
      "(30, 32, 32, 3)\n",
      "0.76959544\n",
      "[Epoch 0/5] [Batch 98/360] [D loss: 0.011158] [G loss: 24.116055] time: 0:00:26.918271\n",
      "(30, 32, 32, 3)\n",
      "0.72547096\n",
      "[Epoch 0/5] [Batch 99/360] [D loss: 0.008347] [G loss: 23.910238] time: 0:00:27.023524\n",
      "(30, 32, 32, 3)\n",
      "0.727526\n",
      "[Epoch 0/5] [Batch 100/360] [D loss: 0.005431] [G loss: 24.489832] time: 0:00:27.130098\n",
      "(30, 32, 32, 3)\n",
      "0.668112\n",
      "[Epoch 0/5] [Batch 101/360] [D loss: 0.004958] [G loss: 24.086658] time: 0:00:27.243050\n",
      "(30, 32, 32, 3)\n",
      "0.62537676\n",
      "[Epoch 0/5] [Batch 102/360] [D loss: 0.007228] [G loss: 24.179401] time: 0:00:27.354172\n",
      "(30, 32, 32, 3)\n",
      "0.682453\n",
      "[Epoch 0/5] [Batch 103/360] [D loss: 0.005602] [G loss: 23.904682] time: 0:00:27.458638\n",
      "(30, 32, 32, 3)\n",
      "0.76552695\n",
      "[Epoch 0/5] [Batch 104/360] [D loss: 0.006827] [G loss: 23.357454] time: 0:00:27.565950\n",
      "(30, 32, 32, 3)\n",
      "0.65703446\n",
      "[Epoch 0/5] [Batch 105/360] [D loss: 0.004817] [G loss: 23.988556] time: 0:00:27.674992\n",
      "(30, 32, 32, 3)\n",
      "0.6292897\n",
      "[Epoch 0/5] [Batch 106/360] [D loss: 0.009022] [G loss: 24.585863] time: 0:00:27.783240\n",
      "(30, 32, 32, 3)\n",
      "0.7171932\n",
      "[Epoch 0/5] [Batch 107/360] [D loss: 0.006946] [G loss: 23.271891] time: 0:00:27.886983\n",
      "(30, 32, 32, 3)\n",
      "0.63209826\n",
      "[Epoch 0/5] [Batch 108/360] [D loss: 0.011299] [G loss: 23.760044] time: 0:00:27.990151\n",
      "(30, 32, 32, 3)\n",
      "0.6940999\n",
      "[Epoch 0/5] [Batch 109/360] [D loss: 0.034846] [G loss: 23.345646] time: 0:00:28.096424\n",
      "(30, 32, 32, 3)\n",
      "0.7943687\n",
      "[Epoch 0/5] [Batch 110/360] [D loss: 0.009348] [G loss: 23.570601] time: 0:00:28.200919\n",
      "(30, 32, 32, 3)\n",
      "0.66784734\n",
      "[Epoch 0/5] [Batch 111/360] [D loss: 0.004933] [G loss: 23.414045] time: 0:00:28.304532\n",
      "(30, 32, 32, 3)\n",
      "0.7802549\n",
      "[Epoch 0/5] [Batch 112/360] [D loss: 0.007547] [G loss: 23.884766] time: 0:00:28.408604\n",
      "(30, 32, 32, 3)\n",
      "0.7937765\n",
      "[Epoch 0/5] [Batch 113/360] [D loss: 0.008415] [G loss: 23.749063] time: 0:00:28.515475\n",
      "(30, 32, 32, 3)\n",
      "0.7749868\n",
      "[Epoch 0/5] [Batch 114/360] [D loss: 0.006530] [G loss: 23.218067] time: 0:00:28.618264\n",
      "(30, 32, 32, 3)\n",
      "0.6846462\n",
      "[Epoch 0/5] [Batch 115/360] [D loss: 0.005869] [G loss: 22.891180] time: 0:00:28.728711\n",
      "(30, 32, 32, 3)\n",
      "0.6099176\n",
      "[Epoch 0/5] [Batch 116/360] [D loss: 0.007278] [G loss: 22.487675] time: 0:00:28.833171\n",
      "(30, 32, 32, 3)\n",
      "0.7573088\n",
      "[Epoch 0/5] [Batch 117/360] [D loss: 0.008441] [G loss: 23.453526] time: 0:00:28.937428\n",
      "(30, 32, 32, 3)\n",
      "0.69551253\n",
      "[Epoch 0/5] [Batch 118/360] [D loss: 0.006847] [G loss: 22.969425] time: 0:00:29.045825\n",
      "(30, 32, 32, 3)\n",
      "0.62843114\n",
      "[Epoch 0/5] [Batch 119/360] [D loss: 0.005325] [G loss: 22.411236] time: 0:00:29.158819\n",
      "(30, 32, 32, 3)\n",
      "0.74737006\n",
      "[Epoch 0/5] [Batch 120/360] [D loss: 0.009005] [G loss: 23.590660] time: 0:00:29.265772\n",
      "(30, 32, 32, 3)\n",
      "0.7028089\n",
      "[Epoch 0/5] [Batch 121/360] [D loss: 0.023971] [G loss: 23.309488] time: 0:00:29.367797\n",
      "(30, 32, 32, 3)\n",
      "0.6480747\n",
      "[Epoch 0/5] [Batch 122/360] [D loss: 0.012344] [G loss: 22.542400] time: 0:00:29.474736\n",
      "(30, 32, 32, 3)\n",
      "0.6940918\n",
      "[Epoch 0/5] [Batch 123/360] [D loss: 0.019087] [G loss: 22.937328] time: 0:00:29.578894\n",
      "(30, 32, 32, 3)\n",
      "0.64722896\n",
      "[Epoch 0/5] [Batch 124/360] [D loss: 0.025802] [G loss: 22.396772] time: 0:00:29.682192\n",
      "(30, 32, 32, 3)\n",
      "0.7298791\n",
      "[Epoch 0/5] [Batch 125/360] [D loss: 0.007412] [G loss: 22.777849] time: 0:00:29.785492\n",
      "(30, 32, 32, 3)\n",
      "0.7025604\n",
      "[Epoch 0/5] [Batch 126/360] [D loss: 0.004684] [G loss: 22.823967] time: 0:00:29.892423\n",
      "(30, 32, 32, 3)\n",
      "0.7072325\n",
      "[Epoch 0/5] [Batch 127/360] [D loss: 0.005475] [G loss: 22.481745] time: 0:00:29.999263\n",
      "(30, 32, 32, 3)\n",
      "0.6882088\n",
      "[Epoch 0/5] [Batch 128/360] [D loss: 0.004764] [G loss: 23.305548] time: 0:00:30.102682\n",
      "(30, 32, 32, 3)\n",
      "0.76518327\n",
      "[Epoch 0/5] [Batch 129/360] [D loss: 0.003863] [G loss: 22.516966] time: 0:00:30.205715\n",
      "(30, 32, 32, 3)\n",
      "0.7866562\n",
      "[Epoch 0/5] [Batch 130/360] [D loss: 0.005509] [G loss: 23.133884] time: 0:00:30.308738\n",
      "(30, 32, 32, 3)\n",
      "0.793432\n",
      "[Epoch 0/5] [Batch 131/360] [D loss: 0.004962] [G loss: 23.086964] time: 0:00:30.415858\n",
      "(30, 32, 32, 3)\n",
      "0.60291535\n",
      "[Epoch 0/5] [Batch 132/360] [D loss: 0.010917] [G loss: 22.070848] time: 0:00:30.519633\n",
      "(30, 32, 32, 3)\n",
      "0.6745555\n",
      "[Epoch 0/5] [Batch 133/360] [D loss: 0.004326] [G loss: 22.562248] time: 0:00:30.622677\n",
      "(30, 32, 32, 3)\n",
      "0.63647395\n",
      "[Epoch 0/5] [Batch 134/360] [D loss: 0.008888] [G loss: 22.591017] time: 0:00:30.732974\n",
      "(30, 32, 32, 3)\n",
      "0.7821417\n",
      "[Epoch 0/5] [Batch 135/360] [D loss: 0.017566] [G loss: 21.960165] time: 0:00:30.838681\n",
      "(30, 32, 32, 3)\n",
      "0.7811739\n",
      "[Epoch 0/5] [Batch 136/360] [D loss: 0.012156] [G loss: 22.869617] time: 0:00:30.945562\n",
      "(30, 32, 32, 3)\n",
      "0.73259765\n",
      "[Epoch 0/5] [Batch 137/360] [D loss: 0.006875] [G loss: 22.265266] time: 0:00:31.049057\n",
      "(30, 32, 32, 3)\n",
      "0.6692602\n",
      "[Epoch 0/5] [Batch 138/360] [D loss: 0.006607] [G loss: 22.078524] time: 0:00:31.151125\n",
      "(30, 32, 32, 3)\n",
      "0.6542558\n",
      "[Epoch 0/5] [Batch 139/360] [D loss: 0.011073] [G loss: 21.935509] time: 0:00:31.254224\n",
      "(30, 32, 32, 3)\n",
      "0.6806018\n",
      "[Epoch 0/5] [Batch 140/360] [D loss: 0.005497] [G loss: 22.420908] time: 0:00:31.359338\n",
      "(30, 32, 32, 3)\n",
      "0.7427097\n",
      "[Epoch 0/5] [Batch 141/360] [D loss: 0.004718] [G loss: 20.777504] time: 0:00:31.463186\n",
      "(30, 32, 32, 3)\n",
      "0.71814317\n",
      "[Epoch 0/5] [Batch 142/360] [D loss: 0.004241] [G loss: 21.537519] time: 0:00:31.566232\n",
      "(30, 32, 32, 3)\n",
      "0.713062\n",
      "[Epoch 0/5] [Batch 143/360] [D loss: 0.004969] [G loss: 21.952614] time: 0:00:31.670711\n",
      "(30, 32, 32, 3)\n",
      "0.6806362\n",
      "[Epoch 0/5] [Batch 144/360] [D loss: 0.005008] [G loss: 22.357847] time: 0:00:31.775044\n",
      "(30, 32, 32, 3)\n",
      "0.7924211\n",
      "[Epoch 0/5] [Batch 145/360] [D loss: 0.013053] [G loss: 21.757998] time: 0:00:31.884856\n",
      "(30, 32, 32, 3)\n",
      "0.66349965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 146/360] [D loss: 0.010696] [G loss: 21.697216] time: 0:00:31.993315\n",
      "(30, 32, 32, 3)\n",
      "0.782044\n",
      "[Epoch 0/5] [Batch 147/360] [D loss: 0.010820] [G loss: 21.727226] time: 0:00:32.098073\n",
      "(30, 32, 32, 3)\n",
      "0.7109005\n",
      "[Epoch 0/5] [Batch 148/360] [D loss: 0.016685] [G loss: 21.685747] time: 0:00:32.202757\n",
      "(30, 32, 32, 3)\n",
      "0.7283683\n",
      "[Epoch 0/5] [Batch 149/360] [D loss: 0.007694] [G loss: 21.817930] time: 0:00:32.313877\n",
      "(30, 32, 32, 3)\n",
      "0.66107106\n",
      "[Epoch 0/5] [Batch 150/360] [D loss: 0.013005] [G loss: 21.938232] time: 0:00:32.421069\n",
      "(30, 32, 32, 3)\n",
      "0.7579053\n",
      "[Epoch 0/5] [Batch 151/360] [D loss: 0.008154] [G loss: 21.196672] time: 0:00:32.525473\n",
      "(30, 32, 32, 3)\n",
      "0.7095763\n",
      "[Epoch 0/5] [Batch 152/360] [D loss: 0.014576] [G loss: 21.297903] time: 0:00:32.647318\n",
      "(30, 32, 32, 3)\n",
      "0.763552\n",
      "[Epoch 0/5] [Batch 153/360] [D loss: 0.019348] [G loss: 21.537327] time: 0:00:32.756373\n",
      "(30, 32, 32, 3)\n",
      "0.8203067\n",
      "[Epoch 0/5] [Batch 154/360] [D loss: 0.018223] [G loss: 21.428310] time: 0:00:32.866871\n",
      "(30, 32, 32, 3)\n",
      "0.71007246\n",
      "[Epoch 0/5] [Batch 155/360] [D loss: 0.011086] [G loss: 21.088961] time: 0:00:32.974337\n",
      "(30, 32, 32, 3)\n",
      "0.72805053\n",
      "[Epoch 0/5] [Batch 156/360] [D loss: 0.007500] [G loss: 20.722189] time: 0:00:33.082208\n",
      "(30, 32, 32, 3)\n",
      "0.78998655\n",
      "[Epoch 0/5] [Batch 157/360] [D loss: 0.004237] [G loss: 21.533590] time: 0:00:33.184580\n",
      "(30, 32, 32, 3)\n",
      "0.8206865\n",
      "[Epoch 0/5] [Batch 158/360] [D loss: 0.005944] [G loss: 20.961433] time: 0:00:33.295018\n",
      "(30, 32, 32, 3)\n",
      "0.70061636\n",
      "[Epoch 0/5] [Batch 159/360] [D loss: 0.005174] [G loss: 20.799826] time: 0:00:33.402873\n",
      "(30, 32, 32, 3)\n",
      "0.6728994\n",
      "[Epoch 0/5] [Batch 160/360] [D loss: 0.010663] [G loss: 21.552889] time: 0:00:33.507568\n",
      "(30, 32, 32, 3)\n",
      "0.8058974\n",
      "[Epoch 0/5] [Batch 161/360] [D loss: 0.005461] [G loss: 21.099480] time: 0:00:33.613749\n",
      "(30, 32, 32, 3)\n",
      "0.7122726\n",
      "[Epoch 0/5] [Batch 162/360] [D loss: 0.005349] [G loss: 21.461447] time: 0:00:33.723021\n",
      "(30, 32, 32, 3)\n",
      "0.7555909\n",
      "[Epoch 0/5] [Batch 163/360] [D loss: 0.006026] [G loss: 20.371284] time: 0:00:33.831437\n",
      "(30, 32, 32, 3)\n",
      "0.7682316\n",
      "[Epoch 0/5] [Batch 164/360] [D loss: 0.004416] [G loss: 20.807558] time: 0:00:33.938453\n",
      "(30, 32, 32, 3)\n",
      "0.78140813\n",
      "[Epoch 0/5] [Batch 165/360] [D loss: 0.006051] [G loss: 20.538725] time: 0:00:34.048285\n",
      "(30, 32, 32, 3)\n",
      "0.7709625\n",
      "[Epoch 0/5] [Batch 166/360] [D loss: 0.003749] [G loss: 20.784451] time: 0:00:34.157035\n",
      "(30, 32, 32, 3)\n",
      "0.67623025\n",
      "[Epoch 0/5] [Batch 167/360] [D loss: 0.006126] [G loss: 20.451252] time: 0:00:34.277842\n",
      "(30, 32, 32, 3)\n",
      "0.81801933\n",
      "[Epoch 0/5] [Batch 168/360] [D loss: 0.004951] [G loss: 20.829687] time: 0:00:34.386388\n",
      "(30, 32, 32, 3)\n",
      "0.74367505\n",
      "[Epoch 0/5] [Batch 169/360] [D loss: 0.011302] [G loss: 21.179668] time: 0:00:34.490723\n",
      "(30, 32, 32, 3)\n",
      "0.78796816\n",
      "[Epoch 0/5] [Batch 170/360] [D loss: 0.005855] [G loss: 20.776094] time: 0:00:34.603970\n",
      "(30, 32, 32, 3)\n",
      "0.8144811\n",
      "[Epoch 0/5] [Batch 171/360] [D loss: 0.004922] [G loss: 20.016815] time: 0:00:34.713268\n",
      "(30, 32, 32, 3)\n",
      "0.7595682\n",
      "[Epoch 0/5] [Batch 172/360] [D loss: 0.006182] [G loss: 20.657434] time: 0:00:34.821044\n",
      "(30, 32, 32, 3)\n",
      "0.67514855\n",
      "[Epoch 0/5] [Batch 173/360] [D loss: 0.004013] [G loss: 21.106253] time: 0:00:34.926153\n",
      "(30, 32, 32, 3)\n",
      "0.7367504\n",
      "[Epoch 0/5] [Batch 174/360] [D loss: 0.004041] [G loss: 20.292204] time: 0:00:35.047669\n",
      "(30, 32, 32, 3)\n",
      "0.79690367\n",
      "[Epoch 0/5] [Batch 175/360] [D loss: 0.003855] [G loss: 20.154507] time: 0:00:35.155106\n",
      "(30, 32, 32, 3)\n",
      "0.67683154\n",
      "[Epoch 0/5] [Batch 176/360] [D loss: 0.003714] [G loss: 20.309519] time: 0:00:35.262508\n",
      "(30, 32, 32, 3)\n",
      "0.7887072\n",
      "[Epoch 0/5] [Batch 177/360] [D loss: 0.005671] [G loss: 20.051229] time: 0:00:35.367199\n",
      "(30, 32, 32, 3)\n",
      "0.7379299\n",
      "[Epoch 0/5] [Batch 178/360] [D loss: 0.005699] [G loss: 20.999243] time: 0:00:35.470677\n",
      "(30, 32, 32, 3)\n",
      "0.786365\n",
      "[Epoch 0/5] [Batch 179/360] [D loss: 0.004444] [G loss: 20.600079] time: 0:00:35.577457\n",
      "(30, 32, 32, 3)\n",
      "0.7637735\n",
      "[Epoch 0/5] [Batch 180/360] [D loss: 0.004243] [G loss: 20.478920] time: 0:00:35.683894\n",
      "(30, 32, 32, 3)\n",
      "0.75939816\n",
      "[Epoch 0/5] [Batch 181/360] [D loss: 0.005105] [G loss: 19.611830] time: 0:00:35.798258\n",
      "(30, 32, 32, 3)\n",
      "0.7196372\n",
      "[Epoch 0/5] [Batch 182/360] [D loss: 0.004363] [G loss: 20.284092] time: 0:00:35.902935\n",
      "(30, 32, 32, 3)\n",
      "0.7163531\n",
      "[Epoch 0/5] [Batch 183/360] [D loss: 0.006005] [G loss: 20.568417] time: 0:00:36.008935\n",
      "(30, 32, 32, 3)\n",
      "0.70019597\n",
      "[Epoch 0/5] [Batch 184/360] [D loss: 0.006546] [G loss: 19.576860] time: 0:00:36.116882\n",
      "(30, 32, 32, 3)\n",
      "0.69173574\n",
      "[Epoch 0/5] [Batch 185/360] [D loss: 0.004802] [G loss: 20.252333] time: 0:00:36.225835\n",
      "(30, 32, 32, 3)\n",
      "0.707053\n",
      "[Epoch 0/5] [Batch 186/360] [D loss: 0.004100] [G loss: 19.550329] time: 0:00:36.331289\n",
      "(30, 32, 32, 3)\n",
      "0.7655441\n",
      "[Epoch 0/5] [Batch 187/360] [D loss: 0.005058] [G loss: 19.795708] time: 0:00:36.444367\n",
      "(30, 32, 32, 3)\n",
      "0.7207878\n",
      "[Epoch 0/5] [Batch 188/360] [D loss: 0.005959] [G loss: 19.907480] time: 0:00:36.557074\n",
      "(30, 32, 32, 3)\n",
      "0.7714111\n",
      "[Epoch 0/5] [Batch 189/360] [D loss: 0.006942] [G loss: 20.441031] time: 0:00:36.666916\n",
      "(30, 32, 32, 3)\n",
      "0.7678089\n",
      "[Epoch 0/5] [Batch 190/360] [D loss: 0.010509] [G loss: 19.476418] time: 0:00:36.783479\n",
      "(30, 32, 32, 3)\n",
      "0.78065354\n",
      "[Epoch 0/5] [Batch 191/360] [D loss: 0.007697] [G loss: 19.894709] time: 0:00:36.897368\n",
      "(30, 32, 32, 3)\n",
      "0.78928727\n",
      "[Epoch 0/5] [Batch 192/360] [D loss: 0.007301] [G loss: 19.393093] time: 0:00:37.009860\n",
      "(30, 32, 32, 3)\n",
      "0.79865074\n",
      "[Epoch 0/5] [Batch 193/360] [D loss: 0.005933] [G loss: 19.235201] time: 0:00:37.117315\n",
      "(30, 32, 32, 3)\n",
      "0.7757159\n",
      "[Epoch 0/5] [Batch 194/360] [D loss: 0.004624] [G loss: 19.881430] time: 0:00:37.231291\n",
      "(30, 32, 32, 3)\n",
      "0.80585057\n",
      "[Epoch 0/5] [Batch 195/360] [D loss: 0.004884] [G loss: 20.252350] time: 0:00:37.343697\n",
      "(30, 32, 32, 3)\n",
      "0.6819239\n",
      "[Epoch 0/5] [Batch 196/360] [D loss: 0.005283] [G loss: 19.445852] time: 0:00:37.455440\n",
      "(30, 32, 32, 3)\n",
      "0.7598217\n",
      "[Epoch 0/5] [Batch 197/360] [D loss: 0.004013] [G loss: 19.154900] time: 0:00:37.560280\n",
      "(30, 32, 32, 3)\n",
      "0.7111676\n",
      "[Epoch 0/5] [Batch 198/360] [D loss: 0.004618] [G loss: 19.671125] time: 0:00:37.666938\n",
      "(30, 32, 32, 3)\n",
      "0.7231062\n",
      "[Epoch 0/5] [Batch 199/360] [D loss: 0.004396] [G loss: 19.464579] time: 0:00:37.773189\n",
      "(30, 32, 32, 3)\n",
      "0.7603137\n",
      "[Epoch 0/5] [Batch 200/360] [D loss: 0.004079] [G loss: 19.549738] time: 0:00:37.877669\n",
      "(30, 32, 32, 3)\n",
      "0.8233182\n",
      "[Epoch 0/5] [Batch 201/360] [D loss: 0.003792] [G loss: 19.790865] time: 0:00:37.980568\n",
      "(30, 32, 32, 3)\n",
      "0.7380142\n",
      "[Epoch 0/5] [Batch 202/360] [D loss: 0.003996] [G loss: 19.453035] time: 0:00:38.090770\n",
      "(30, 32, 32, 3)\n",
      "0.75411946\n",
      "[Epoch 0/5] [Batch 203/360] [D loss: 0.003273] [G loss: 19.610962] time: 0:00:38.208719\n",
      "(30, 32, 32, 3)\n",
      "0.78961706\n",
      "[Epoch 0/5] [Batch 204/360] [D loss: 0.004522] [G loss: 19.398951] time: 0:00:38.318074\n",
      "(30, 32, 32, 3)\n",
      "0.8407542\n",
      "[Epoch 0/5] [Batch 205/360] [D loss: 0.004531] [G loss: 19.737913] time: 0:00:38.421333\n",
      "(30, 32, 32, 3)\n",
      "0.78492594\n",
      "[Epoch 0/5] [Batch 206/360] [D loss: 0.009927] [G loss: 20.206553] time: 0:00:38.540018\n",
      "(30, 32, 32, 3)\n",
      "0.8049841\n",
      "[Epoch 0/5] [Batch 207/360] [D loss: 0.008427] [G loss: 19.538862] time: 0:00:38.644281\n",
      "(30, 32, 32, 3)\n",
      "0.7708213\n",
      "[Epoch 0/5] [Batch 208/360] [D loss: 0.011011] [G loss: 19.300215] time: 0:00:38.751079\n",
      "(30, 32, 32, 3)\n",
      "0.77154446\n",
      "[Epoch 0/5] [Batch 209/360] [D loss: 0.049922] [G loss: 19.333639] time: 0:00:38.854619\n",
      "(30, 32, 32, 3)\n",
      "0.7857601\n",
      "[Epoch 0/5] [Batch 210/360] [D loss: 0.353178] [G loss: 19.643820] time: 0:00:38.957769\n",
      "(30, 32, 32, 3)\n",
      "0.79776955\n",
      "[Epoch 0/5] [Batch 211/360] [D loss: 0.063471] [G loss: 19.553703] time: 0:00:39.061988\n",
      "(30, 32, 32, 3)\n",
      "0.76250345\n",
      "[Epoch 0/5] [Batch 212/360] [D loss: 0.026868] [G loss: 19.184698] time: 0:00:39.168953\n",
      "(30, 32, 32, 3)\n",
      "0.81842875\n",
      "[Epoch 0/5] [Batch 213/360] [D loss: 0.025272] [G loss: 18.281940] time: 0:00:39.272054\n",
      "(30, 32, 32, 3)\n",
      "0.77222663\n",
      "[Epoch 0/5] [Batch 214/360] [D loss: 0.009859] [G loss: 19.311176] time: 0:00:39.375157\n",
      "(30, 32, 32, 3)\n",
      "0.7083273\n",
      "[Epoch 0/5] [Batch 215/360] [D loss: 0.008843] [G loss: 18.175035] time: 0:00:39.479746\n",
      "(30, 32, 32, 3)\n",
      "0.65115714\n",
      "[Epoch 0/5] [Batch 216/360] [D loss: 0.004028] [G loss: 18.760870] time: 0:00:39.583837\n",
      "(30, 32, 32, 3)\n",
      "0.8192118\n",
      "[Epoch 0/5] [Batch 217/360] [D loss: 0.003852] [G loss: 18.972561] time: 0:00:39.689858\n",
      "(30, 32, 32, 3)\n",
      "0.73744565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 218/360] [D loss: 0.005054] [G loss: 19.138628] time: 0:00:39.793351\n",
      "(30, 32, 32, 3)\n",
      "0.73541576\n",
      "[Epoch 0/5] [Batch 219/360] [D loss: 0.005148] [G loss: 18.357672] time: 0:00:39.895590\n",
      "(30, 32, 32, 3)\n",
      "0.7983858\n",
      "[Epoch 0/5] [Batch 220/360] [D loss: 0.003981] [G loss: 18.443735] time: 0:00:39.999079\n",
      "(30, 32, 32, 3)\n",
      "0.8193615\n",
      "[Epoch 0/5] [Batch 221/360] [D loss: 0.003389] [G loss: 18.867500] time: 0:00:40.105741\n",
      "(30, 32, 32, 3)\n",
      "0.7596182\n",
      "[Epoch 0/5] [Batch 222/360] [D loss: 0.004641] [G loss: 19.098959] time: 0:00:40.209316\n",
      "(30, 32, 32, 3)\n",
      "0.7314741\n",
      "[Epoch 0/5] [Batch 223/360] [D loss: 0.004533] [G loss: 17.694567] time: 0:00:40.313534\n",
      "(30, 32, 32, 3)\n",
      "0.8008949\n",
      "[Epoch 0/5] [Batch 224/360] [D loss: 0.002381] [G loss: 18.934179] time: 0:00:40.417529\n",
      "(30, 32, 32, 3)\n",
      "0.77569574\n",
      "[Epoch 0/5] [Batch 225/360] [D loss: 0.003825] [G loss: 18.926849] time: 0:00:40.525688\n",
      "(30, 32, 32, 3)\n",
      "0.6735309\n",
      "[Epoch 0/5] [Batch 226/360] [D loss: 0.003943] [G loss: 17.388067] time: 0:00:40.632469\n",
      "(30, 32, 32, 3)\n",
      "0.81995183\n",
      "[Epoch 0/5] [Batch 227/360] [D loss: 0.003800] [G loss: 19.009710] time: 0:00:40.735451\n",
      "(30, 32, 32, 3)\n",
      "0.84684116\n",
      "[Epoch 0/5] [Batch 228/360] [D loss: 0.005881] [G loss: 18.383907] time: 0:00:40.839337\n",
      "(30, 32, 32, 3)\n",
      "0.710512\n",
      "[Epoch 0/5] [Batch 229/360] [D loss: 0.003809] [G loss: 17.928606] time: 0:00:40.942553\n",
      "(30, 32, 32, 3)\n",
      "0.71871847\n",
      "[Epoch 0/5] [Batch 230/360] [D loss: 0.004054] [G loss: 18.438929] time: 0:00:41.060204\n",
      "(30, 32, 32, 3)\n",
      "0.7400095\n",
      "[Epoch 0/5] [Batch 231/360] [D loss: 0.003823] [G loss: 18.475708] time: 0:00:41.171838\n",
      "(30, 32, 32, 3)\n",
      "0.73636204\n",
      "[Epoch 0/5] [Batch 232/360] [D loss: 0.004465] [G loss: 18.311907] time: 0:00:41.276889\n",
      "(30, 32, 32, 3)\n",
      "0.7167854\n",
      "[Epoch 0/5] [Batch 233/360] [D loss: 0.005463] [G loss: 18.306435] time: 0:00:41.380410\n",
      "(30, 32, 32, 3)\n",
      "0.7934374\n",
      "[Epoch 0/5] [Batch 234/360] [D loss: 0.005216] [G loss: 18.407433] time: 0:00:41.484389\n",
      "(30, 32, 32, 3)\n",
      "0.7415522\n",
      "[Epoch 0/5] [Batch 235/360] [D loss: 0.004512] [G loss: 17.895971] time: 0:00:41.589552\n",
      "(30, 32, 32, 3)\n",
      "0.74150866\n",
      "[Epoch 0/5] [Batch 236/360] [D loss: 0.004890] [G loss: 18.486231] time: 0:00:41.699557\n",
      "(30, 32, 32, 3)\n",
      "0.793389\n",
      "[Epoch 0/5] [Batch 237/360] [D loss: 0.003193] [G loss: 18.025547] time: 0:00:41.801561\n",
      "(30, 32, 32, 3)\n",
      "0.81372005\n",
      "[Epoch 0/5] [Batch 238/360] [D loss: 0.003403] [G loss: 19.093897] time: 0:00:41.906010\n",
      "(30, 32, 32, 3)\n",
      "0.78771335\n",
      "[Epoch 0/5] [Batch 239/360] [D loss: 0.003449] [G loss: 17.753748] time: 0:00:42.014486\n",
      "(30, 32, 32, 3)\n",
      "0.7483987\n",
      "[Epoch 0/5] [Batch 240/360] [D loss: 0.007556] [G loss: 18.515316] time: 0:00:42.119871\n",
      "(30, 32, 32, 3)\n",
      "0.7733039\n",
      "[Epoch 0/5] [Batch 241/360] [D loss: 0.012305] [G loss: 17.780848] time: 0:00:42.227061\n",
      "(30, 32, 32, 3)\n",
      "0.5958758\n",
      "[Epoch 0/5] [Batch 242/360] [D loss: 0.007389] [G loss: 17.990971] time: 0:00:42.331035\n",
      "(30, 32, 32, 3)\n",
      "0.70449704\n",
      "[Epoch 0/5] [Batch 243/360] [D loss: 0.004542] [G loss: 17.673422] time: 0:00:42.434311\n",
      "(30, 32, 32, 3)\n",
      "0.8490548\n",
      "[Epoch 0/5] [Batch 244/360] [D loss: 0.008040] [G loss: 18.079723] time: 0:00:42.542130\n",
      "(30, 32, 32, 3)\n",
      "0.7673187\n",
      "[Epoch 0/5] [Batch 245/360] [D loss: 0.005484] [G loss: 18.539629] time: 0:00:42.647435\n",
      "(30, 32, 32, 3)\n",
      "0.7799869\n",
      "[Epoch 0/5] [Batch 246/360] [D loss: 0.005727] [G loss: 17.848743] time: 0:00:42.760241\n",
      "(30, 32, 32, 3)\n",
      "0.69255114\n",
      "[Epoch 0/5] [Batch 247/360] [D loss: 0.004354] [G loss: 17.637829] time: 0:00:42.872412\n",
      "(30, 32, 32, 3)\n",
      "0.79456407\n",
      "[Epoch 0/5] [Batch 248/360] [D loss: 0.005220] [G loss: 18.886091] time: 0:00:42.984427\n",
      "(30, 32, 32, 3)\n",
      "0.69499964\n",
      "[Epoch 0/5] [Batch 249/360] [D loss: 0.008047] [G loss: 17.990477] time: 0:00:43.096226\n",
      "(30, 32, 32, 3)\n",
      "0.7350991\n",
      "[Epoch 0/5] [Batch 250/360] [D loss: 0.011382] [G loss: 17.814398] time: 0:00:43.201932\n",
      "(30, 32, 32, 3)\n",
      "0.7433775\n",
      "[Epoch 0/5] [Batch 251/360] [D loss: 0.011552] [G loss: 18.026348] time: 0:00:43.306350\n",
      "(30, 32, 32, 3)\n",
      "0.72236013\n",
      "[Epoch 0/5] [Batch 252/360] [D loss: 0.014711] [G loss: 17.459333] time: 0:00:43.418720\n",
      "(30, 32, 32, 3)\n",
      "0.758169\n",
      "[Epoch 0/5] [Batch 253/360] [D loss: 0.006303] [G loss: 18.737394] time: 0:00:43.534221\n",
      "(30, 32, 32, 3)\n",
      "0.6686282\n",
      "[Epoch 0/5] [Batch 254/360] [D loss: 0.005805] [G loss: 17.488111] time: 0:00:43.647832\n",
      "(30, 32, 32, 3)\n",
      "0.7494734\n",
      "[Epoch 0/5] [Batch 255/360] [D loss: 0.003923] [G loss: 17.956472] time: 0:00:43.759672\n",
      "(30, 32, 32, 3)\n",
      "0.78874\n",
      "[Epoch 0/5] [Batch 256/360] [D loss: 0.015042] [G loss: 17.145880] time: 0:00:43.869826\n",
      "(30, 32, 32, 3)\n",
      "0.80816174\n",
      "[Epoch 0/5] [Batch 257/360] [D loss: 0.005515] [G loss: 17.636282] time: 0:00:43.984171\n",
      "(30, 32, 32, 3)\n",
      "0.8439494\n",
      "[Epoch 0/5] [Batch 258/360] [D loss: 0.008462] [G loss: 17.302967] time: 0:00:44.093949\n",
      "(30, 32, 32, 3)\n",
      "0.7605497\n",
      "[Epoch 0/5] [Batch 259/360] [D loss: 0.004351] [G loss: 17.596169] time: 0:00:44.199215\n",
      "(30, 32, 32, 3)\n",
      "0.7561471\n",
      "[Epoch 0/5] [Batch 260/360] [D loss: 0.003621] [G loss: 17.552498] time: 0:00:44.303525\n",
      "(30, 32, 32, 3)\n",
      "0.65144837\n",
      "[Epoch 0/5] [Batch 261/360] [D loss: 0.009712] [G loss: 16.887863] time: 0:00:44.413995\n",
      "(30, 32, 32, 3)\n",
      "0.7040393\n",
      "[Epoch 0/5] [Batch 262/360] [D loss: 0.008727] [G loss: 17.158312] time: 0:00:44.523237\n",
      "(30, 32, 32, 3)\n",
      "0.7882721\n",
      "[Epoch 0/5] [Batch 263/360] [D loss: 0.008553] [G loss: 17.387674] time: 0:00:44.628951\n",
      "(30, 32, 32, 3)\n",
      "0.6584327\n",
      "[Epoch 0/5] [Batch 264/360] [D loss: 0.007528] [G loss: 17.769911] time: 0:00:44.732579\n",
      "(30, 32, 32, 3)\n",
      "0.7442716\n",
      "[Epoch 0/5] [Batch 265/360] [D loss: 0.003252] [G loss: 17.724260] time: 0:00:44.846160\n",
      "(30, 32, 32, 3)\n",
      "0.73390436\n",
      "[Epoch 0/5] [Batch 266/360] [D loss: 0.007117] [G loss: 17.379322] time: 0:00:44.955144\n",
      "(30, 32, 32, 3)\n",
      "0.8082151\n",
      "[Epoch 0/5] [Batch 267/360] [D loss: 0.005235] [G loss: 16.754547] time: 0:00:45.061843\n",
      "(30, 32, 32, 3)\n",
      "0.8254569\n",
      "[Epoch 0/5] [Batch 268/360] [D loss: 0.003839] [G loss: 16.884003] time: 0:00:45.172250\n",
      "(30, 32, 32, 3)\n",
      "0.69714564\n",
      "[Epoch 0/5] [Batch 269/360] [D loss: 0.009437] [G loss: 16.851784] time: 0:00:45.292029\n",
      "(30, 32, 32, 3)\n",
      "0.84625226\n",
      "[Epoch 0/5] [Batch 270/360] [D loss: 0.003694] [G loss: 18.037315] time: 0:00:45.404035\n",
      "(30, 32, 32, 3)\n",
      "0.66668504\n",
      "[Epoch 0/5] [Batch 271/360] [D loss: 0.008271] [G loss: 16.760780] time: 0:00:45.516545\n",
      "(30, 32, 32, 3)\n",
      "0.7953145\n",
      "[Epoch 0/5] [Batch 272/360] [D loss: 0.005404] [G loss: 17.212475] time: 0:00:45.620944\n",
      "(30, 32, 32, 3)\n",
      "0.7226834\n",
      "[Epoch 0/5] [Batch 273/360] [D loss: 0.029522] [G loss: 16.877106] time: 0:00:45.726143\n",
      "(30, 32, 32, 3)\n",
      "0.7111067\n",
      "[Epoch 0/5] [Batch 274/360] [D loss: 0.115283] [G loss: 18.039227] time: 0:00:45.830488\n",
      "(30, 32, 32, 3)\n",
      "0.75453264\n",
      "[Epoch 0/5] [Batch 275/360] [D loss: 0.521002] [G loss: 16.353004] time: 0:00:45.937989\n",
      "(30, 32, 32, 3)\n",
      "0.8121121\n",
      "[Epoch 0/5] [Batch 276/360] [D loss: 0.017594] [G loss: 16.634630] time: 0:00:46.041069\n",
      "(30, 32, 32, 3)\n",
      "0.6929178\n",
      "[Epoch 0/5] [Batch 277/360] [D loss: 0.075010] [G loss: 17.081697] time: 0:00:46.146314\n",
      "(30, 32, 32, 3)\n",
      "0.7977459\n",
      "[Epoch 0/5] [Batch 278/360] [D loss: 0.011847] [G loss: 17.136417] time: 0:00:46.250480\n",
      "(30, 32, 32, 3)\n",
      "0.7938302\n",
      "[Epoch 0/5] [Batch 279/360] [D loss: 0.012895] [G loss: 16.584011] time: 0:00:46.362209\n",
      "(30, 32, 32, 3)\n",
      "0.8317266\n",
      "[Epoch 0/5] [Batch 280/360] [D loss: 0.008044] [G loss: 18.521557] time: 0:00:46.469309\n",
      "(30, 32, 32, 3)\n",
      "0.7154344\n",
      "[Epoch 0/5] [Batch 281/360] [D loss: 0.010980] [G loss: 16.499640] time: 0:00:46.573963\n",
      "(30, 32, 32, 3)\n",
      "0.8086162\n",
      "[Epoch 0/5] [Batch 282/360] [D loss: 0.007068] [G loss: 16.324118] time: 0:00:46.678130\n",
      "(30, 32, 32, 3)\n",
      "0.736135\n",
      "[Epoch 0/5] [Batch 283/360] [D loss: 0.004876] [G loss: 17.121906] time: 0:00:46.783489\n",
      "(30, 32, 32, 3)\n",
      "0.79259396\n",
      "[Epoch 0/5] [Batch 284/360] [D loss: 0.005557] [G loss: 17.045815] time: 0:00:46.895340\n",
      "(30, 32, 32, 3)\n",
      "0.7309372\n",
      "[Epoch 0/5] [Batch 285/360] [D loss: 0.004076] [G loss: 16.249083] time: 0:00:46.999838\n",
      "(30, 32, 32, 3)\n",
      "0.8140242\n",
      "[Epoch 0/5] [Batch 286/360] [D loss: 0.004809] [G loss: 16.003443] time: 0:00:47.104523\n",
      "(30, 32, 32, 3)\n",
      "0.7608269\n",
      "[Epoch 0/5] [Batch 287/360] [D loss: 0.004098] [G loss: 17.161274] time: 0:00:47.210523\n",
      "(30, 32, 32, 3)\n",
      "0.78859717\n",
      "[Epoch 0/5] [Batch 288/360] [D loss: 0.005827] [G loss: 16.125982] time: 0:00:47.314968\n",
      "(30, 32, 32, 3)\n",
      "0.86046284\n",
      "[Epoch 0/5] [Batch 289/360] [D loss: 0.004352] [G loss: 16.495552] time: 0:00:47.423226\n",
      "(30, 32, 32, 3)\n",
      "0.84066516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 290/360] [D loss: 0.003741] [G loss: 17.422935] time: 0:00:47.527655\n",
      "(30, 32, 32, 3)\n",
      "0.7646219\n",
      "[Epoch 0/5] [Batch 291/360] [D loss: 0.003920] [G loss: 16.434914] time: 0:00:47.631799\n",
      "(30, 32, 32, 3)\n",
      "0.78645474\n",
      "[Epoch 0/5] [Batch 292/360] [D loss: 0.004485] [G loss: 16.521666] time: 0:00:47.735569\n",
      "(30, 32, 32, 3)\n",
      "0.80130816\n",
      "[Epoch 0/5] [Batch 293/360] [D loss: 0.005245] [G loss: 16.924332] time: 0:00:47.844054\n",
      "(30, 32, 32, 3)\n",
      "0.66360575\n",
      "[Epoch 0/5] [Batch 294/360] [D loss: 0.004134] [G loss: 16.376417] time: 0:00:47.949435\n",
      "(30, 32, 32, 3)\n",
      "0.7714712\n",
      "[Epoch 0/5] [Batch 295/360] [D loss: 0.004032] [G loss: 15.805937] time: 0:00:48.055187\n",
      "(30, 32, 32, 3)\n",
      "0.78967077\n",
      "[Epoch 0/5] [Batch 296/360] [D loss: 0.003741] [G loss: 16.168100] time: 0:00:48.159210\n",
      "(30, 32, 32, 3)\n",
      "0.7457784\n",
      "[Epoch 0/5] [Batch 297/360] [D loss: 0.004400] [G loss: 16.249660] time: 0:00:48.263094\n",
      "(30, 32, 32, 3)\n",
      "0.74679035\n",
      "[Epoch 0/5] [Batch 298/360] [D loss: 0.003853] [G loss: 16.285028] time: 0:00:48.372318\n",
      "(30, 32, 32, 3)\n",
      "0.7910307\n",
      "[Epoch 0/5] [Batch 299/360] [D loss: 0.003624] [G loss: 16.411751] time: 0:00:48.476298\n",
      "(30, 32, 32, 3)\n",
      "0.79690164\n",
      "[Epoch 0/5] [Batch 300/360] [D loss: 0.003729] [G loss: 15.859714] time: 0:00:48.579438\n",
      "(30, 32, 32, 3)\n",
      "0.7801161\n",
      "[Epoch 0/5] [Batch 301/360] [D loss: 0.002389] [G loss: 16.151653] time: 0:00:48.686666\n",
      "(30, 32, 32, 3)\n",
      "0.74587655\n",
      "[Epoch 0/5] [Batch 302/360] [D loss: 0.004303] [G loss: 16.175396] time: 0:00:48.792930\n",
      "(30, 32, 32, 3)\n",
      "0.7813433\n",
      "[Epoch 0/5] [Batch 303/360] [D loss: 0.003661] [G loss: 15.636312] time: 0:00:48.896910\n",
      "(30, 32, 32, 3)\n",
      "0.80119306\n",
      "[Epoch 0/5] [Batch 304/360] [D loss: 0.004372] [G loss: 15.976782] time: 0:00:49.000227\n",
      "(30, 32, 32, 3)\n",
      "0.8161194\n",
      "[Epoch 0/5] [Batch 305/360] [D loss: 0.004055] [G loss: 16.495581] time: 0:00:49.106519\n",
      "(30, 32, 32, 3)\n",
      "0.7871159\n",
      "[Epoch 0/5] [Batch 306/360] [D loss: 0.003313] [G loss: 15.648263] time: 0:00:49.218408\n",
      "(30, 32, 32, 3)\n",
      "0.7747183\n",
      "[Epoch 0/5] [Batch 307/360] [D loss: 0.003641] [G loss: 16.131804] time: 0:00:49.326001\n",
      "(30, 32, 32, 3)\n",
      "0.8007717\n",
      "[Epoch 0/5] [Batch 308/360] [D loss: 0.005526] [G loss: 15.787028] time: 0:00:49.429942\n",
      "(30, 32, 32, 3)\n",
      "0.84286815\n",
      "[Epoch 0/5] [Batch 309/360] [D loss: 0.003131] [G loss: 15.734859] time: 0:00:49.533478\n",
      "(30, 32, 32, 3)\n",
      "0.8154823\n",
      "[Epoch 0/5] [Batch 310/360] [D loss: 0.003174] [G loss: 16.640522] time: 0:00:49.636567\n",
      "(30, 32, 32, 3)\n",
      "0.86264634\n",
      "[Epoch 0/5] [Batch 311/360] [D loss: 0.004114] [G loss: 15.509735] time: 0:00:49.746958\n",
      "(30, 32, 32, 3)\n",
      "0.71943945\n",
      "[Epoch 0/5] [Batch 312/360] [D loss: 0.003458] [G loss: 15.384305] time: 0:00:49.919137\n",
      "(30, 32, 32, 3)\n",
      "0.72423095\n",
      "[Epoch 0/5] [Batch 313/360] [D loss: 0.004896] [G loss: 15.987979] time: 0:00:50.024333\n",
      "(30, 32, 32, 3)\n",
      "0.664563\n",
      "[Epoch 0/5] [Batch 314/360] [D loss: 0.003338] [G loss: 15.125048] time: 0:00:50.129690\n",
      "(30, 32, 32, 3)\n",
      "0.78951526\n",
      "[Epoch 0/5] [Batch 315/360] [D loss: 0.003642] [G loss: 15.944300] time: 0:00:50.233590\n",
      "(30, 32, 32, 3)\n",
      "0.7918648\n",
      "[Epoch 0/5] [Batch 316/360] [D loss: 0.004826] [G loss: 14.745993] time: 0:00:50.344248\n",
      "(30, 32, 32, 3)\n",
      "0.84300226\n",
      "[Epoch 0/5] [Batch 317/360] [D loss: 0.002908] [G loss: 16.478312] time: 0:00:50.448693\n",
      "(30, 32, 32, 3)\n",
      "0.8175834\n",
      "[Epoch 0/5] [Batch 318/360] [D loss: 0.004080] [G loss: 16.092331] time: 0:00:50.561190\n",
      "(30, 32, 32, 3)\n",
      "0.779795\n",
      "[Epoch 0/5] [Batch 319/360] [D loss: 0.002649] [G loss: 15.567335] time: 0:00:50.671961\n",
      "(30, 32, 32, 3)\n",
      "0.7297805\n",
      "[Epoch 0/5] [Batch 320/360] [D loss: 0.004262] [G loss: 15.372991] time: 0:00:50.778280\n",
      "(30, 32, 32, 3)\n",
      "0.7636695\n",
      "[Epoch 0/5] [Batch 321/360] [D loss: 0.001901] [G loss: 15.585334] time: 0:00:50.890763\n",
      "(30, 32, 32, 3)\n",
      "0.82747537\n",
      "[Epoch 0/5] [Batch 322/360] [D loss: 0.003480] [G loss: 15.175095] time: 0:00:50.999732\n",
      "(30, 32, 32, 3)\n",
      "0.803273\n",
      "[Epoch 0/5] [Batch 323/360] [D loss: 0.002251] [G loss: 15.932622] time: 0:00:51.101401\n",
      "(30, 32, 32, 3)\n",
      "0.8555527\n",
      "[Epoch 0/5] [Batch 324/360] [D loss: 0.003837] [G loss: 15.236757] time: 0:00:51.207570\n",
      "(30, 32, 32, 3)\n",
      "0.84662503\n",
      "[Epoch 0/5] [Batch 325/360] [D loss: 0.004460] [G loss: 15.677756] time: 0:00:51.315338\n",
      "(30, 32, 32, 3)\n",
      "0.7599042\n",
      "[Epoch 0/5] [Batch 326/360] [D loss: 0.003782] [G loss: 15.253011] time: 0:00:51.419675\n",
      "(30, 32, 32, 3)\n",
      "0.83756226\n",
      "[Epoch 0/5] [Batch 327/360] [D loss: 0.004753] [G loss: 15.123214] time: 0:00:51.523042\n",
      "(30, 32, 32, 3)\n",
      "0.80953115\n",
      "[Epoch 0/5] [Batch 328/360] [D loss: 0.013652] [G loss: 15.754844] time: 0:00:51.628684\n",
      "(30, 32, 32, 3)\n",
      "0.74501544\n",
      "[Epoch 0/5] [Batch 329/360] [D loss: 0.004089] [G loss: 14.937943] time: 0:00:51.739067\n",
      "(30, 32, 32, 3)\n",
      "0.8373403\n",
      "[Epoch 0/5] [Batch 330/360] [D loss: 0.003501] [G loss: 15.611367] time: 0:00:51.847586\n",
      "(30, 32, 32, 3)\n",
      "0.7373207\n",
      "[Epoch 0/5] [Batch 331/360] [D loss: 0.003845] [G loss: 14.969477] time: 0:00:51.951536\n",
      "(30, 32, 32, 3)\n",
      "0.76835805\n",
      "[Epoch 0/5] [Batch 332/360] [D loss: 0.003674] [G loss: 15.747062] time: 0:00:52.056763\n",
      "(30, 32, 32, 3)\n",
      "0.79950076\n",
      "[Epoch 0/5] [Batch 333/360] [D loss: 0.004272] [G loss: 16.102858] time: 0:00:52.165740\n",
      "(30, 32, 32, 3)\n",
      "0.71514577\n",
      "[Epoch 0/5] [Batch 334/360] [D loss: 0.005393] [G loss: 14.609008] time: 0:00:52.274089\n",
      "(30, 32, 32, 3)\n",
      "0.772044\n",
      "[Epoch 0/5] [Batch 335/360] [D loss: 0.006399] [G loss: 15.766488] time: 0:00:52.377125\n",
      "(30, 32, 32, 3)\n",
      "0.7653615\n",
      "[Epoch 0/5] [Batch 336/360] [D loss: 0.005411] [G loss: 14.355442] time: 0:00:52.482199\n",
      "(30, 32, 32, 3)\n",
      "0.7866661\n",
      "[Epoch 0/5] [Batch 337/360] [D loss: 0.002814] [G loss: 14.615151] time: 0:00:52.583698\n",
      "(30, 32, 32, 3)\n",
      "0.7852208\n",
      "[Epoch 0/5] [Batch 338/360] [D loss: 0.004670] [G loss: 15.427367] time: 0:00:52.688974\n",
      "(30, 32, 32, 3)\n",
      "0.6953462\n",
      "[Epoch 0/5] [Batch 339/360] [D loss: 0.004715] [G loss: 14.560832] time: 0:00:52.797838\n",
      "(30, 32, 32, 3)\n",
      "0.8187733\n",
      "[Epoch 0/5] [Batch 340/360] [D loss: 0.003617] [G loss: 15.351327] time: 0:00:52.906069\n",
      "(30, 32, 32, 3)\n",
      "0.71661377\n",
      "[Epoch 0/5] [Batch 341/360] [D loss: 0.003625] [G loss: 15.488951] time: 0:00:53.010394\n",
      "(30, 32, 32, 3)\n",
      "0.84552574\n",
      "[Epoch 0/5] [Batch 342/360] [D loss: 0.003692] [G loss: 14.448828] time: 0:00:53.117101\n",
      "(30, 32, 32, 3)\n",
      "0.852465\n",
      "[Epoch 0/5] [Batch 343/360] [D loss: 0.003411] [G loss: 15.078051] time: 0:00:53.223392\n",
      "(30, 32, 32, 3)\n",
      "0.7898645\n",
      "[Epoch 0/5] [Batch 344/360] [D loss: 0.004346] [G loss: 14.693490] time: 0:00:53.329419\n",
      "(30, 32, 32, 3)\n",
      "0.848465\n",
      "[Epoch 0/5] [Batch 345/360] [D loss: 0.002221] [G loss: 15.648244] time: 0:00:53.433848\n",
      "(30, 32, 32, 3)\n",
      "0.7013785\n",
      "[Epoch 0/5] [Batch 346/360] [D loss: 0.005458] [G loss: 15.584040] time: 0:00:53.538921\n",
      "(30, 32, 32, 3)\n",
      "0.78901815\n",
      "[Epoch 0/5] [Batch 347/360] [D loss: 0.003510] [G loss: 15.547812] time: 0:00:53.645372\n",
      "(30, 32, 32, 3)\n",
      "0.77369386\n",
      "[Epoch 0/5] [Batch 348/360] [D loss: 0.004902] [G loss: 14.923917] time: 0:00:53.752651\n",
      "(30, 32, 32, 3)\n",
      "0.7845852\n",
      "[Epoch 0/5] [Batch 349/360] [D loss: 0.015474] [G loss: 14.847212] time: 0:00:53.855296\n",
      "(30, 32, 32, 3)\n",
      "0.8808356\n",
      "[Epoch 0/5] [Batch 350/360] [D loss: 0.013233] [G loss: 14.672382] time: 0:00:53.959299\n",
      "(30, 32, 32, 3)\n",
      "0.8242771\n",
      "[Epoch 0/5] [Batch 351/360] [D loss: 0.011375] [G loss: 15.134727] time: 0:00:54.062687\n",
      "(30, 32, 32, 3)\n",
      "0.77347153\n",
      "[Epoch 0/5] [Batch 352/360] [D loss: 0.006442] [G loss: 14.409909] time: 0:00:54.175874\n",
      "(30, 32, 32, 3)\n",
      "0.740688\n",
      "[Epoch 0/5] [Batch 353/360] [D loss: 0.002803] [G loss: 15.736124] time: 0:00:54.279683\n",
      "(30, 32, 32, 3)\n",
      "0.79994184\n",
      "[Epoch 0/5] [Batch 354/360] [D loss: 0.004838] [G loss: 14.780199] time: 0:00:54.386339\n",
      "(30, 32, 32, 3)\n",
      "0.7123642\n",
      "[Epoch 0/5] [Batch 355/360] [D loss: 0.006799] [G loss: 15.772320] time: 0:00:54.489182\n",
      "(30, 32, 32, 3)\n",
      "0.8026569\n",
      "[Epoch 0/5] [Batch 356/360] [D loss: 0.033079] [G loss: 14.571169] time: 0:00:54.592893\n",
      "(30, 32, 32, 3)\n",
      "0.773179\n",
      "[Epoch 0/5] [Batch 357/360] [D loss: 0.224955] [G loss: 16.284386] time: 0:00:54.700621\n",
      "(30, 32, 32, 3)\n",
      "0.77806216\n",
      "[Epoch 0/5] [Batch 359/360] [D loss: 0.493677] [G loss: 14.780263] time: 0:00:54.815961\n",
      "(30, 32, 32, 3)\n",
      "0.77000123\n",
      "[Epoch 1/5] [Batch 0/360] [D loss: 0.005775] [G loss: 14.757565] time: 0:00:54.919484\n",
      "(30, 32, 32, 3)\n",
      "0.7968604\n",
      "[Epoch 1/5] [Batch 1/360] [D loss: 0.080312] [G loss: 15.058362] time: 0:00:55.024044\n",
      "(30, 32, 32, 3)\n",
      "0.8297674\n",
      "[Epoch 1/5] [Batch 2/360] [D loss: 0.007736] [G loss: 14.409855] time: 0:00:55.131183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.8011551\n",
      "[Epoch 1/5] [Batch 3/360] [D loss: 0.017882] [G loss: 14.180889] time: 0:00:55.243348\n",
      "(30, 32, 32, 3)\n",
      "0.7722257\n",
      "[Epoch 1/5] [Batch 4/360] [D loss: 0.006206] [G loss: 14.152914] time: 0:00:55.348071\n",
      "(30, 32, 32, 3)\n",
      "0.7806441\n",
      "[Epoch 1/5] [Batch 5/360] [D loss: 0.006791] [G loss: 14.263772] time: 0:00:55.454821\n",
      "(30, 32, 32, 3)\n",
      "0.8306709\n",
      "[Epoch 1/5] [Batch 6/360] [D loss: 0.007438] [G loss: 14.800731] time: 0:00:55.560806\n",
      "(30, 32, 32, 3)\n",
      "0.78818184\n",
      "[Epoch 1/5] [Batch 7/360] [D loss: 0.003960] [G loss: 14.741030] time: 0:00:55.675961\n",
      "(30, 32, 32, 3)\n",
      "0.77201205\n",
      "[Epoch 1/5] [Batch 8/360] [D loss: 0.005457] [G loss: 15.012420] time: 0:00:55.784815\n",
      "(30, 32, 32, 3)\n",
      "0.80900866\n",
      "[Epoch 1/5] [Batch 9/360] [D loss: 0.004731] [G loss: 14.729000] time: 0:00:55.891143\n",
      "(30, 32, 32, 3)\n",
      "0.83898216\n",
      "[Epoch 1/5] [Batch 10/360] [D loss: 0.005066] [G loss: 14.286434] time: 0:00:55.993166\n",
      "(30, 32, 32, 3)\n",
      "0.76164883\n",
      "[Epoch 1/5] [Batch 11/360] [D loss: 0.004346] [G loss: 15.111106] time: 0:00:56.110723\n",
      "(30, 32, 32, 3)\n",
      "0.8471467\n",
      "[Epoch 1/5] [Batch 12/360] [D loss: 0.003457] [G loss: 14.926486] time: 0:00:56.215149\n",
      "(30, 32, 32, 3)\n",
      "0.78722066\n",
      "[Epoch 1/5] [Batch 13/360] [D loss: 0.005079] [G loss: 14.258487] time: 0:00:56.321361\n",
      "(30, 32, 32, 3)\n",
      "0.7902095\n",
      "[Epoch 1/5] [Batch 14/360] [D loss: 0.004232] [G loss: 13.787900] time: 0:00:56.427501\n",
      "(30, 32, 32, 3)\n",
      "0.8275215\n",
      "[Epoch 1/5] [Batch 15/360] [D loss: 0.004407] [G loss: 13.944089] time: 0:00:56.533540\n",
      "(30, 32, 32, 3)\n",
      "0.83899146\n",
      "[Epoch 1/5] [Batch 16/360] [D loss: 0.003349] [G loss: 14.411242] time: 0:00:56.643292\n",
      "(30, 32, 32, 3)\n",
      "0.70534784\n",
      "[Epoch 1/5] [Batch 17/360] [D loss: 0.006622] [G loss: 13.933606] time: 0:00:56.760598\n",
      "(30, 32, 32, 3)\n",
      "0.7566325\n",
      "[Epoch 1/5] [Batch 18/360] [D loss: 0.003859] [G loss: 13.220005] time: 0:00:56.865241\n",
      "(30, 32, 32, 3)\n",
      "0.7790875\n",
      "[Epoch 1/5] [Batch 19/360] [D loss: 0.005349] [G loss: 13.658038] time: 0:00:56.970722\n",
      "(30, 32, 32, 3)\n",
      "0.7489453\n",
      "[Epoch 1/5] [Batch 20/360] [D loss: 0.004327] [G loss: 14.416655] time: 0:00:57.079177\n",
      "(30, 32, 32, 3)\n",
      "0.8531192\n",
      "[Epoch 1/5] [Batch 21/360] [D loss: 0.004147] [G loss: 14.178458] time: 0:00:57.186321\n",
      "(30, 32, 32, 3)\n",
      "0.7719078\n",
      "[Epoch 1/5] [Batch 22/360] [D loss: 0.003517] [G loss: 13.658539] time: 0:00:57.290714\n",
      "(30, 32, 32, 3)\n",
      "0.792646\n",
      "[Epoch 1/5] [Batch 23/360] [D loss: 0.002415] [G loss: 13.830806] time: 0:00:57.397170\n",
      "(30, 32, 32, 3)\n",
      "0.7500673\n",
      "[Epoch 1/5] [Batch 24/360] [D loss: 0.003551] [G loss: 14.537155] time: 0:00:57.501184\n",
      "(30, 32, 32, 3)\n",
      "0.8450482\n",
      "[Epoch 1/5] [Batch 25/360] [D loss: 0.004219] [G loss: 14.618937] time: 0:00:57.621183\n",
      "(30, 32, 32, 3)\n",
      "0.7117488\n",
      "[Epoch 1/5] [Batch 26/360] [D loss: 0.003740] [G loss: 14.546250] time: 0:00:57.731066\n",
      "(30, 32, 32, 3)\n",
      "0.7550257\n",
      "[Epoch 1/5] [Batch 27/360] [D loss: 0.002662] [G loss: 14.146887] time: 0:00:57.837636\n",
      "(30, 32, 32, 3)\n",
      "0.81295437\n",
      "[Epoch 1/5] [Batch 28/360] [D loss: 0.003306] [G loss: 14.300442] time: 0:00:57.941543\n",
      "(30, 32, 32, 3)\n",
      "0.8295302\n",
      "[Epoch 1/5] [Batch 29/360] [D loss: 0.004504] [G loss: 14.307886] time: 0:00:58.049903\n",
      "(30, 32, 32, 3)\n",
      "0.729368\n",
      "[Epoch 1/5] [Batch 30/360] [D loss: 0.004965] [G loss: 13.979052] time: 0:00:58.153984\n",
      "(30, 32, 32, 3)\n",
      "0.79887027\n",
      "[Epoch 1/5] [Batch 31/360] [D loss: 0.005275] [G loss: 14.010607] time: 0:00:58.260189\n",
      "(30, 32, 32, 3)\n",
      "0.7719707\n",
      "[Epoch 1/5] [Batch 32/360] [D loss: 0.003906] [G loss: 13.905973] time: 0:00:58.364621\n",
      "(30, 32, 32, 3)\n",
      "0.83045083\n",
      "[Epoch 1/5] [Batch 33/360] [D loss: 0.006394] [G loss: 13.337670] time: 0:00:58.475708\n",
      "(30, 32, 32, 3)\n",
      "0.7954879\n",
      "[Epoch 1/5] [Batch 34/360] [D loss: 0.003362] [G loss: 13.976943] time: 0:00:58.582213\n",
      "(30, 32, 32, 3)\n",
      "0.8343436\n",
      "[Epoch 1/5] [Batch 35/360] [D loss: 0.006092] [G loss: 13.374959] time: 0:00:58.691804\n",
      "(30, 32, 32, 3)\n",
      "0.8639217\n",
      "[Epoch 1/5] [Batch 36/360] [D loss: 0.004941] [G loss: 14.658834] time: 0:00:58.795129\n",
      "(30, 32, 32, 3)\n",
      "0.77846885\n",
      "[Epoch 1/5] [Batch 37/360] [D loss: 0.014839] [G loss: 13.285381] time: 0:00:58.905295\n",
      "(30, 32, 32, 3)\n",
      "0.7971991\n",
      "[Epoch 1/5] [Batch 38/360] [D loss: 0.006186] [G loss: 13.008487] time: 0:00:59.012479\n",
      "(30, 32, 32, 3)\n",
      "0.8398816\n",
      "[Epoch 1/5] [Batch 39/360] [D loss: 0.017273] [G loss: 13.939396] time: 0:00:59.118812\n",
      "(30, 32, 32, 3)\n",
      "0.74530905\n",
      "[Epoch 1/5] [Batch 40/360] [D loss: 0.005087] [G loss: 14.393869] time: 0:00:59.229765\n",
      "(30, 32, 32, 3)\n",
      "0.81714153\n",
      "[Epoch 1/5] [Batch 41/360] [D loss: 0.007727] [G loss: 13.976002] time: 0:00:59.339457\n",
      "(30, 32, 32, 3)\n",
      "0.7847474\n",
      "[Epoch 1/5] [Batch 42/360] [D loss: 0.005741] [G loss: 14.500656] time: 0:00:59.441668\n",
      "(30, 32, 32, 3)\n",
      "0.6837883\n",
      "[Epoch 1/5] [Batch 43/360] [D loss: 0.004971] [G loss: 13.217946] time: 0:00:59.550677\n",
      "(30, 32, 32, 3)\n",
      "0.718247\n",
      "[Epoch 1/5] [Batch 44/360] [D loss: 0.003619] [G loss: 13.648190] time: 0:00:59.654420\n",
      "(30, 32, 32, 3)\n",
      "0.75095695\n",
      "[Epoch 1/5] [Batch 45/360] [D loss: 0.007150] [G loss: 13.620210] time: 0:00:59.761000\n",
      "(30, 32, 32, 3)\n",
      "0.7786278\n",
      "[Epoch 1/5] [Batch 46/360] [D loss: 0.003481] [G loss: 13.426788] time: 0:00:59.868233\n",
      "(30, 32, 32, 3)\n",
      "0.83054286\n",
      "[Epoch 1/5] [Batch 47/360] [D loss: 0.004675] [G loss: 13.633456] time: 0:00:59.978146\n",
      "(30, 32, 32, 3)\n",
      "0.7951226\n",
      "[Epoch 1/5] [Batch 48/360] [D loss: 0.003323] [G loss: 13.510070] time: 0:01:00.081507\n",
      "(30, 32, 32, 3)\n",
      "0.7472295\n",
      "[Epoch 1/5] [Batch 49/360] [D loss: 0.002742] [G loss: 13.505909] time: 0:01:00.188345\n",
      "(30, 32, 32, 3)\n",
      "0.8286369\n",
      "[Epoch 1/5] [Batch 50/360] [D loss: 0.004215] [G loss: 13.323779] time: 0:01:00.292003\n",
      "(30, 32, 32, 3)\n",
      "0.8138752\n",
      "[Epoch 1/5] [Batch 51/360] [D loss: 0.007936] [G loss: 12.858191] time: 0:01:00.398476\n",
      "(30, 32, 32, 3)\n",
      "0.8327798\n",
      "[Epoch 1/5] [Batch 52/360] [D loss: 0.003001] [G loss: 13.747053] time: 0:01:00.504280\n",
      "(30, 32, 32, 3)\n",
      "0.9029023\n",
      "[Epoch 1/5] [Batch 53/360] [D loss: 0.005185] [G loss: 13.398221] time: 0:01:00.609458\n",
      "(30, 32, 32, 3)\n",
      "0.79045993\n",
      "[Epoch 1/5] [Batch 54/360] [D loss: 0.003665] [G loss: 12.914819] time: 0:01:00.720007\n",
      "(30, 32, 32, 3)\n",
      "0.79068327\n",
      "[Epoch 1/5] [Batch 55/360] [D loss: 0.004836] [G loss: 12.678384] time: 0:01:00.825531\n",
      "(30, 32, 32, 3)\n",
      "0.79103845\n",
      "[Epoch 1/5] [Batch 56/360] [D loss: 0.002775] [G loss: 13.295206] time: 0:01:00.933219\n",
      "(30, 32, 32, 3)\n",
      "0.79158896\n",
      "[Epoch 1/5] [Batch 57/360] [D loss: 0.003448] [G loss: 13.150204] time: 0:01:01.039945\n",
      "(30, 32, 32, 3)\n",
      "0.7720413\n",
      "[Epoch 1/5] [Batch 58/360] [D loss: 0.002500] [G loss: 13.704197] time: 0:01:01.144164\n",
      "(30, 32, 32, 3)\n",
      "0.75876456\n",
      "[Epoch 1/5] [Batch 59/360] [D loss: 0.004126] [G loss: 13.147836] time: 0:01:01.248938\n",
      "(30, 32, 32, 3)\n",
      "0.81888205\n",
      "[Epoch 1/5] [Batch 60/360] [D loss: 0.002900] [G loss: 12.765613] time: 0:01:01.354036\n",
      "(30, 32, 32, 3)\n",
      "0.75088024\n",
      "[Epoch 1/5] [Batch 61/360] [D loss: 0.005862] [G loss: 13.147582] time: 0:01:01.462455\n",
      "(30, 32, 32, 3)\n",
      "0.7619422\n",
      "[Epoch 1/5] [Batch 62/360] [D loss: 0.003637] [G loss: 13.038440] time: 0:01:01.569781\n",
      "(30, 32, 32, 3)\n",
      "0.82848626\n",
      "[Epoch 1/5] [Batch 63/360] [D loss: 0.004304] [G loss: 13.487970] time: 0:01:01.673736\n",
      "(30, 32, 32, 3)\n",
      "0.84566194\n",
      "[Epoch 1/5] [Batch 64/360] [D loss: 0.006449] [G loss: 13.382302] time: 0:01:01.779313\n",
      "(30, 32, 32, 3)\n",
      "0.82820064\n",
      "[Epoch 1/5] [Batch 65/360] [D loss: 0.005023] [G loss: 13.462769] time: 0:01:01.886096\n",
      "(30, 32, 32, 3)\n",
      "0.80394953\n",
      "[Epoch 1/5] [Batch 66/360] [D loss: 0.005016] [G loss: 13.106702] time: 0:01:01.991713\n",
      "(30, 32, 32, 3)\n",
      "0.8137514\n",
      "[Epoch 1/5] [Batch 67/360] [D loss: 0.003838] [G loss: 12.980523] time: 0:01:02.098154\n",
      "(30, 32, 32, 3)\n",
      "0.83423996\n",
      "[Epoch 1/5] [Batch 68/360] [D loss: 0.010333] [G loss: 13.346784] time: 0:01:02.203020\n",
      "(30, 32, 32, 3)\n",
      "0.7819934\n",
      "[Epoch 1/5] [Batch 69/360] [D loss: 0.008194] [G loss: 13.015695] time: 0:01:02.309746\n",
      "(30, 32, 32, 3)\n",
      "0.7851941\n",
      "[Epoch 1/5] [Batch 70/360] [D loss: 0.006434] [G loss: 13.103045] time: 0:01:02.415371\n",
      "(30, 32, 32, 3)\n",
      "0.73739105\n",
      "[Epoch 1/5] [Batch 71/360] [D loss: 0.008735] [G loss: 12.573323] time: 0:01:02.523059\n",
      "(30, 32, 32, 3)\n",
      "0.8204677\n",
      "[Epoch 1/5] [Batch 72/360] [D loss: 0.005373] [G loss: 13.330057] time: 0:01:02.627242\n",
      "(30, 32, 32, 3)\n",
      "0.7810931\n",
      "[Epoch 1/5] [Batch 73/360] [D loss: 0.004641] [G loss: 12.628104] time: 0:01:02.733707\n",
      "(30, 32, 32, 3)\n",
      "0.7242448\n",
      "[Epoch 1/5] [Batch 74/360] [D loss: 0.002998] [G loss: 12.782101] time: 0:01:02.849452\n",
      "(30, 32, 32, 3)\n",
      "0.76494557\n",
      "[Epoch 1/5] [Batch 75/360] [D loss: 0.002878] [G loss: 12.080563] time: 0:01:02.965041\n",
      "(30, 32, 32, 3)\n",
      "0.7718478\n",
      "[Epoch 1/5] [Batch 76/360] [D loss: 0.004332] [G loss: 12.739701] time: 0:01:03.072036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.79014826\n",
      "[Epoch 1/5] [Batch 77/360] [D loss: 0.016461] [G loss: 13.009763] time: 0:01:03.181069\n",
      "(30, 32, 32, 3)\n",
      "0.78503436\n",
      "[Epoch 1/5] [Batch 78/360] [D loss: 0.007654] [G loss: 14.137577] time: 0:01:03.284801\n",
      "(30, 32, 32, 3)\n",
      "0.82621485\n",
      "[Epoch 1/5] [Batch 79/360] [D loss: 0.004755] [G loss: 13.184717] time: 0:01:03.399597\n",
      "(30, 32, 32, 3)\n",
      "0.7236435\n",
      "[Epoch 1/5] [Batch 80/360] [D loss: 0.005824] [G loss: 12.962258] time: 0:01:03.502836\n",
      "(30, 32, 32, 3)\n",
      "0.8335829\n",
      "[Epoch 1/5] [Batch 81/360] [D loss: 0.009581] [G loss: 13.247913] time: 0:01:03.609510\n",
      "(30, 32, 32, 3)\n",
      "0.8091677\n",
      "[Epoch 1/5] [Batch 82/360] [D loss: 0.003403] [G loss: 12.727732] time: 0:01:03.714031\n",
      "(30, 32, 32, 3)\n",
      "0.76102614\n",
      "[Epoch 1/5] [Batch 83/360] [D loss: 0.002866] [G loss: 12.951138] time: 0:01:03.823843\n",
      "(30, 32, 32, 3)\n",
      "0.7578945\n",
      "[Epoch 1/5] [Batch 84/360] [D loss: 0.003399] [G loss: 12.514223] time: 0:01:03.929493\n",
      "(30, 32, 32, 3)\n",
      "0.8136743\n",
      "[Epoch 1/5] [Batch 85/360] [D loss: 0.006668] [G loss: 13.037284] time: 0:01:04.036631\n",
      "(30, 32, 32, 3)\n",
      "0.78289956\n",
      "[Epoch 1/5] [Batch 86/360] [D loss: 0.003726] [G loss: 12.052064] time: 0:01:04.141746\n",
      "(30, 32, 32, 3)\n",
      "0.7685528\n",
      "[Epoch 1/5] [Batch 87/360] [D loss: 0.006086] [G loss: 12.293097] time: 0:01:04.249992\n",
      "(30, 32, 32, 3)\n",
      "0.86084753\n",
      "[Epoch 1/5] [Batch 88/360] [D loss: 0.003025] [G loss: 12.923303] time: 0:01:04.357042\n",
      "(30, 32, 32, 3)\n",
      "0.81667596\n",
      "[Epoch 1/5] [Batch 89/360] [D loss: 0.018265] [G loss: 12.618057] time: 0:01:04.463599\n",
      "(30, 32, 32, 3)\n",
      "0.80300254\n",
      "[Epoch 1/5] [Batch 90/360] [D loss: 0.009156] [G loss: 12.686708] time: 0:01:04.567804\n",
      "(30, 32, 32, 3)\n",
      "0.7462263\n",
      "[Epoch 1/5] [Batch 91/360] [D loss: 0.017111] [G loss: 12.957125] time: 0:01:04.678237\n",
      "(30, 32, 32, 3)\n",
      "0.7697003\n",
      "[Epoch 1/5] [Batch 92/360] [D loss: 0.050290] [G loss: 12.748715] time: 0:01:04.784357\n",
      "(30, 32, 32, 3)\n",
      "0.8229497\n",
      "[Epoch 1/5] [Batch 93/360] [D loss: 0.315532] [G loss: 13.912884] time: 0:01:04.896734\n",
      "(30, 32, 32, 3)\n",
      "0.8478174\n",
      "[Epoch 1/5] [Batch 94/360] [D loss: 0.450061] [G loss: 12.060608] time: 0:01:05.000264\n",
      "(30, 32, 32, 3)\n",
      "0.80041903\n",
      "[Epoch 1/5] [Batch 95/360] [D loss: 0.098921] [G loss: 12.291918] time: 0:01:05.107686\n",
      "(30, 32, 32, 3)\n",
      "0.78854084\n",
      "[Epoch 1/5] [Batch 96/360] [D loss: 0.059886] [G loss: 12.282314] time: 0:01:05.211286\n",
      "(30, 32, 32, 3)\n",
      "0.86076856\n",
      "[Epoch 1/5] [Batch 97/360] [D loss: 0.051792] [G loss: 12.959158] time: 0:01:05.319670\n",
      "(30, 32, 32, 3)\n",
      "0.7894988\n",
      "[Epoch 1/5] [Batch 98/360] [D loss: 0.012091] [G loss: 13.251238] time: 0:01:05.423299\n",
      "(30, 32, 32, 3)\n",
      "0.8158534\n",
      "[Epoch 1/5] [Batch 99/360] [D loss: 0.017967] [G loss: 12.751056] time: 0:01:05.530024\n",
      "(30, 32, 32, 3)\n",
      "0.7894189\n",
      "[Epoch 1/5] [Batch 100/360] [D loss: 0.007133] [G loss: 12.397991] time: 0:01:05.633588\n",
      "(30, 32, 32, 3)\n",
      "0.78487104\n",
      "[Epoch 1/5] [Batch 101/360] [D loss: 0.012283] [G loss: 12.210410] time: 0:01:05.742582\n",
      "(30, 32, 32, 3)\n",
      "0.81133765\n",
      "[Epoch 1/5] [Batch 102/360] [D loss: 0.006647] [G loss: 12.179279] time: 0:01:05.846743\n",
      "(30, 32, 32, 3)\n",
      "0.8308796\n",
      "[Epoch 1/5] [Batch 103/360] [D loss: 0.008801] [G loss: 13.099855] time: 0:01:05.955702\n",
      "(30, 32, 32, 3)\n",
      "0.74290687\n",
      "[Epoch 1/5] [Batch 104/360] [D loss: 0.004636] [G loss: 12.540028] time: 0:01:06.061751\n",
      "(30, 32, 32, 3)\n",
      "0.8357256\n",
      "[Epoch 1/5] [Batch 105/360] [D loss: 0.005055] [G loss: 11.779684] time: 0:01:06.166766\n",
      "(30, 32, 32, 3)\n",
      "0.85098153\n",
      "[Epoch 1/5] [Batch 106/360] [D loss: 0.006155] [G loss: 12.142392] time: 0:01:06.274835\n",
      "(30, 32, 32, 3)\n",
      "0.7897425\n",
      "[Epoch 1/5] [Batch 107/360] [D loss: 0.013606] [G loss: 11.878470] time: 0:01:06.382194\n",
      "(30, 32, 32, 3)\n",
      "0.74876404\n",
      "[Epoch 1/5] [Batch 108/360] [D loss: 0.005701] [G loss: 12.377858] time: 0:01:06.495327\n",
      "(30, 32, 32, 3)\n",
      "0.83123034\n",
      "[Epoch 1/5] [Batch 109/360] [D loss: 0.007791] [G loss: 11.671496] time: 0:01:06.603024\n",
      "(30, 32, 32, 3)\n",
      "0.7426017\n",
      "[Epoch 1/5] [Batch 110/360] [D loss: 0.005194] [G loss: 11.806158] time: 0:01:06.717012\n",
      "(30, 32, 32, 3)\n",
      "0.83429223\n",
      "[Epoch 1/5] [Batch 111/360] [D loss: 0.008458] [G loss: 12.092576] time: 0:01:06.824578\n",
      "(30, 32, 32, 3)\n",
      "0.7657477\n",
      "[Epoch 1/5] [Batch 112/360] [D loss: 0.005051] [G loss: 11.741796] time: 0:01:06.928703\n",
      "(30, 32, 32, 3)\n",
      "0.85025555\n",
      "[Epoch 1/5] [Batch 113/360] [D loss: 0.004311] [G loss: 12.929034] time: 0:01:07.034906\n",
      "(30, 32, 32, 3)\n",
      "0.8009896\n",
      "[Epoch 1/5] [Batch 114/360] [D loss: 0.010215] [G loss: 12.477879] time: 0:01:07.138091\n",
      "(30, 32, 32, 3)\n",
      "0.79070395\n",
      "[Epoch 1/5] [Batch 115/360] [D loss: 0.004991] [G loss: 12.191088] time: 0:01:07.247401\n",
      "(30, 32, 32, 3)\n",
      "0.83588266\n",
      "[Epoch 1/5] [Batch 116/360] [D loss: 0.013140] [G loss: 11.967603] time: 0:01:07.350376\n",
      "(30, 32, 32, 3)\n",
      "0.78783756\n",
      "[Epoch 1/5] [Batch 117/360] [D loss: 0.003521] [G loss: 12.193979] time: 0:01:07.455067\n",
      "(30, 32, 32, 3)\n",
      "0.88077253\n",
      "[Epoch 1/5] [Batch 118/360] [D loss: 0.006743] [G loss: 11.949936] time: 0:01:07.559394\n",
      "(30, 32, 32, 3)\n",
      "0.83182883\n",
      "[Epoch 1/5] [Batch 119/360] [D loss: 0.003306] [G loss: 12.712283] time: 0:01:07.667430\n",
      "(30, 32, 32, 3)\n",
      "0.8226385\n",
      "[Epoch 1/5] [Batch 120/360] [D loss: 0.005672] [G loss: 11.994466] time: 0:01:07.774346\n",
      "(30, 32, 32, 3)\n",
      "0.86851853\n",
      "[Epoch 1/5] [Batch 121/360] [D loss: 0.004852] [G loss: 12.214201] time: 0:01:07.886595\n",
      "(30, 32, 32, 3)\n",
      "0.81738067\n",
      "[Epoch 1/5] [Batch 122/360] [D loss: 0.019109] [G loss: 12.325876] time: 0:01:07.990509\n",
      "(30, 32, 32, 3)\n",
      "0.7347885\n",
      "[Epoch 1/5] [Batch 123/360] [D loss: 0.006152] [G loss: 11.569782] time: 0:01:08.097249\n",
      "(30, 32, 32, 3)\n",
      "0.90120125\n",
      "[Epoch 1/5] [Batch 124/360] [D loss: 0.004527] [G loss: 12.296032] time: 0:01:08.205176\n",
      "(30, 32, 32, 3)\n",
      "0.79982215\n",
      "[Epoch 1/5] [Batch 125/360] [D loss: 0.022686] [G loss: 12.146174] time: 0:01:08.311745\n",
      "(30, 32, 32, 3)\n",
      "0.7733629\n",
      "[Epoch 1/5] [Batch 126/360] [D loss: 0.010188] [G loss: 12.135016] time: 0:01:08.418298\n",
      "(30, 32, 32, 3)\n",
      "0.7703969\n",
      "[Epoch 1/5] [Batch 127/360] [D loss: 0.008998] [G loss: 11.789034] time: 0:01:08.526717\n",
      "(30, 32, 32, 3)\n",
      "0.76234865\n",
      "[Epoch 1/5] [Batch 128/360] [D loss: 0.019227] [G loss: 11.594325] time: 0:01:08.635858\n",
      "(30, 32, 32, 3)\n",
      "0.8885765\n",
      "[Epoch 1/5] [Batch 129/360] [D loss: 0.011362] [G loss: 12.682900] time: 0:01:08.743074\n",
      "(30, 32, 32, 3)\n",
      "0.8613704\n",
      "[Epoch 1/5] [Batch 130/360] [D loss: 0.006757] [G loss: 11.974409] time: 0:01:08.851162\n",
      "(30, 32, 32, 3)\n",
      "0.82611626\n",
      "[Epoch 1/5] [Batch 131/360] [D loss: 0.011360] [G loss: 11.809926] time: 0:01:08.957620\n",
      "(30, 32, 32, 3)\n",
      "0.87307686\n",
      "[Epoch 1/5] [Batch 132/360] [D loss: 0.003413] [G loss: 10.943305] time: 0:01:09.067151\n",
      "(30, 32, 32, 3)\n",
      "0.83381206\n",
      "[Epoch 1/5] [Batch 133/360] [D loss: 0.007397] [G loss: 11.499004] time: 0:01:09.178480\n",
      "(30, 32, 32, 3)\n",
      "0.83776945\n",
      "[Epoch 1/5] [Batch 134/360] [D loss: 0.005289] [G loss: 11.442614] time: 0:01:09.286449\n",
      "(30, 32, 32, 3)\n",
      "0.7858307\n",
      "[Epoch 1/5] [Batch 135/360] [D loss: 0.005410] [G loss: 11.625028] time: 0:01:09.398605\n",
      "(30, 32, 32, 3)\n",
      "0.86911297\n",
      "[Epoch 1/5] [Batch 136/360] [D loss: 0.003579] [G loss: 11.626166] time: 0:01:09.504663\n",
      "(30, 32, 32, 3)\n",
      "0.90840644\n",
      "[Epoch 1/5] [Batch 137/360] [D loss: 0.005680] [G loss: 11.609122] time: 0:01:09.615564\n",
      "(30, 32, 32, 3)\n",
      "0.8442731\n",
      "[Epoch 1/5] [Batch 138/360] [D loss: 0.030983] [G loss: 12.675518] time: 0:01:09.724680\n",
      "(30, 32, 32, 3)\n",
      "0.84433985\n",
      "[Epoch 1/5] [Batch 139/360] [D loss: 0.065574] [G loss: 12.371365] time: 0:01:09.833792\n",
      "(30, 32, 32, 3)\n",
      "0.849637\n",
      "[Epoch 1/5] [Batch 140/360] [D loss: 0.025875] [G loss: 11.366556] time: 0:01:09.939987\n",
      "(30, 32, 32, 3)\n",
      "0.79551214\n",
      "[Epoch 1/5] [Batch 141/360] [D loss: 0.027275] [G loss: 12.023182] time: 0:01:10.043751\n",
      "(30, 32, 32, 3)\n",
      "0.8146389\n",
      "[Epoch 1/5] [Batch 142/360] [D loss: 0.071641] [G loss: 12.630598] time: 0:01:10.153924\n",
      "(30, 32, 32, 3)\n",
      "0.81553644\n",
      "[Epoch 1/5] [Batch 143/360] [D loss: 0.279881] [G loss: 12.043058] time: 0:01:10.261401\n",
      "(30, 32, 32, 3)\n",
      "0.8334872\n",
      "[Epoch 1/5] [Batch 144/360] [D loss: 0.455096] [G loss: 11.348046] time: 0:01:10.367114\n",
      "(30, 32, 32, 3)\n",
      "0.8028836\n",
      "[Epoch 1/5] [Batch 145/360] [D loss: 0.009660] [G loss: 11.257007] time: 0:01:10.476111\n",
      "(30, 32, 32, 3)\n",
      "0.83643484\n",
      "[Epoch 1/5] [Batch 146/360] [D loss: 0.068294] [G loss: 12.001958] time: 0:01:10.589163\n",
      "(30, 32, 32, 3)\n",
      "0.852266\n",
      "[Epoch 1/5] [Batch 147/360] [D loss: 0.013703] [G loss: 12.054899] time: 0:01:10.694354\n",
      "(30, 32, 32, 3)\n",
      "0.7960746\n",
      "[Epoch 1/5] [Batch 148/360] [D loss: 0.009401] [G loss: 11.563870] time: 0:01:10.799944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.6979921\n",
      "[Epoch 1/5] [Batch 149/360] [D loss: 0.006242] [G loss: 10.819955] time: 0:01:10.912720\n",
      "(30, 32, 32, 3)\n",
      "0.8017946\n",
      "[Epoch 1/5] [Batch 150/360] [D loss: 0.008139] [G loss: 11.160152] time: 0:01:11.021050\n",
      "(30, 32, 32, 3)\n",
      "0.7939016\n",
      "[Epoch 1/5] [Batch 151/360] [D loss: 0.007217] [G loss: 12.038553] time: 0:01:11.131182\n",
      "(30, 32, 32, 3)\n",
      "0.7634589\n",
      "[Epoch 1/5] [Batch 152/360] [D loss: 0.006893] [G loss: 11.422212] time: 0:01:11.242767\n",
      "(30, 32, 32, 3)\n",
      "0.81891793\n",
      "[Epoch 1/5] [Batch 153/360] [D loss: 0.006089] [G loss: 11.113523] time: 0:01:11.353851\n",
      "(30, 32, 32, 3)\n",
      "0.82870275\n",
      "[Epoch 1/5] [Batch 154/360] [D loss: 0.005585] [G loss: 11.515401] time: 0:01:11.459707\n",
      "(30, 32, 32, 3)\n",
      "0.827223\n",
      "[Epoch 1/5] [Batch 155/360] [D loss: 0.004252] [G loss: 11.363622] time: 0:01:11.578889\n",
      "(30, 32, 32, 3)\n",
      "0.84596854\n",
      "[Epoch 1/5] [Batch 156/360] [D loss: 0.004953] [G loss: 11.993193] time: 0:01:11.687017\n",
      "(30, 32, 32, 3)\n",
      "0.84321016\n",
      "[Epoch 1/5] [Batch 157/360] [D loss: 0.004288] [G loss: 11.175705] time: 0:01:11.793164\n",
      "(30, 32, 32, 3)\n",
      "0.8101206\n",
      "[Epoch 1/5] [Batch 158/360] [D loss: 0.003442] [G loss: 10.465422] time: 0:01:11.896886\n",
      "(30, 32, 32, 3)\n",
      "0.7871342\n",
      "[Epoch 1/5] [Batch 159/360] [D loss: 0.005994] [G loss: 11.703084] time: 0:01:12.005723\n",
      "(30, 32, 32, 3)\n",
      "0.7626361\n",
      "[Epoch 1/5] [Batch 160/360] [D loss: 0.006007] [G loss: 11.262214] time: 0:01:12.115459\n",
      "(30, 32, 32, 3)\n",
      "0.7827573\n",
      "[Epoch 1/5] [Batch 161/360] [D loss: 0.005683] [G loss: 10.560312] time: 0:01:12.222259\n",
      "(30, 32, 32, 3)\n",
      "0.8186655\n",
      "[Epoch 1/5] [Batch 162/360] [D loss: 0.003708] [G loss: 11.456183] time: 0:01:12.332261\n",
      "(30, 32, 32, 3)\n",
      "0.7571719\n",
      "[Epoch 1/5] [Batch 163/360] [D loss: 0.005056] [G loss: 10.934387] time: 0:01:12.441809\n",
      "(30, 32, 32, 3)\n",
      "0.83972836\n",
      "[Epoch 1/5] [Batch 164/360] [D loss: 0.004627] [G loss: 11.341507] time: 0:01:12.549264\n",
      "(30, 32, 32, 3)\n",
      "0.8237357\n",
      "[Epoch 1/5] [Batch 165/360] [D loss: 0.010796] [G loss: 11.683139] time: 0:01:12.661668\n",
      "(30, 32, 32, 3)\n",
      "0.8256089\n",
      "[Epoch 1/5] [Batch 166/360] [D loss: 0.004910] [G loss: 10.882056] time: 0:01:12.776671\n",
      "(30, 32, 32, 3)\n",
      "0.8700292\n",
      "[Epoch 1/5] [Batch 167/360] [D loss: 0.004062] [G loss: 11.188918] time: 0:01:12.886716\n",
      "(30, 32, 32, 3)\n",
      "0.81940466\n",
      "[Epoch 1/5] [Batch 168/360] [D loss: 0.003211] [G loss: 11.169544] time: 0:01:12.997653\n",
      "(30, 32, 32, 3)\n",
      "0.80104065\n",
      "[Epoch 1/5] [Batch 169/360] [D loss: 0.002456] [G loss: 11.795668] time: 0:01:13.116551\n",
      "(30, 32, 32, 3)\n",
      "0.809089\n",
      "[Epoch 1/5] [Batch 170/360] [D loss: 0.003369] [G loss: 10.740355] time: 0:01:13.226142\n",
      "(30, 32, 32, 3)\n",
      "0.81614023\n",
      "[Epoch 1/5] [Batch 171/360] [D loss: 0.003981] [G loss: 11.039089] time: 0:01:13.337187\n",
      "(30, 32, 32, 3)\n",
      "0.8151224\n",
      "[Epoch 1/5] [Batch 172/360] [D loss: 0.003577] [G loss: 11.185812] time: 0:01:13.443478\n",
      "(30, 32, 32, 3)\n",
      "0.91832995\n",
      "[Epoch 1/5] [Batch 173/360] [D loss: 0.004480] [G loss: 11.194486] time: 0:01:13.550833\n",
      "(30, 32, 32, 3)\n",
      "0.8199334\n",
      "[Epoch 1/5] [Batch 174/360] [D loss: 0.003927] [G loss: 11.352312] time: 0:01:13.654019\n",
      "(30, 32, 32, 3)\n",
      "0.84196573\n",
      "[Epoch 1/5] [Batch 175/360] [D loss: 0.005864] [G loss: 11.596706] time: 0:01:13.759598\n",
      "(30, 32, 32, 3)\n",
      "0.8523981\n",
      "[Epoch 1/5] [Batch 176/360] [D loss: 0.005518] [G loss: 10.912709] time: 0:01:13.864025\n",
      "(30, 32, 32, 3)\n",
      "0.81270105\n",
      "[Epoch 1/5] [Batch 177/360] [D loss: 0.002149] [G loss: 11.194674] time: 0:01:13.968288\n",
      "(30, 32, 32, 3)\n",
      "0.85459536\n",
      "[Epoch 1/5] [Batch 178/360] [D loss: 0.002751] [G loss: 11.521551] time: 0:01:14.076912\n",
      "(30, 32, 32, 3)\n",
      "0.839382\n",
      "[Epoch 1/5] [Batch 179/360] [D loss: 0.003891] [G loss: 11.050668] time: 0:01:14.182809\n",
      "(30, 32, 32, 3)\n",
      "0.8067007\n",
      "[Epoch 1/5] [Batch 180/360] [D loss: 0.004477] [G loss: 10.992666] time: 0:01:14.285672\n",
      "(30, 32, 32, 3)\n",
      "0.77029175\n",
      "[Epoch 1/5] [Batch 181/360] [D loss: 0.004415] [G loss: 10.352311] time: 0:01:14.391464\n",
      "(30, 32, 32, 3)\n",
      "0.8153171\n",
      "[Epoch 1/5] [Batch 182/360] [D loss: 0.003928] [G loss: 11.294647] time: 0:01:14.498982\n",
      "(30, 32, 32, 3)\n",
      "0.78390867\n",
      "[Epoch 1/5] [Batch 183/360] [D loss: 0.003351] [G loss: 10.396780] time: 0:01:14.604074\n",
      "(30, 32, 32, 3)\n",
      "0.81424856\n",
      "[Epoch 1/5] [Batch 184/360] [D loss: 0.003449] [G loss: 10.805820] time: 0:01:14.716538\n",
      "(30, 32, 32, 3)\n",
      "0.8462003\n",
      "[Epoch 1/5] [Batch 185/360] [D loss: 0.004931] [G loss: 10.575245] time: 0:01:14.830582\n",
      "(30, 32, 32, 3)\n",
      "0.86364526\n",
      "[Epoch 1/5] [Batch 186/360] [D loss: 0.003841] [G loss: 10.976933] time: 0:01:14.939121\n",
      "(30, 32, 32, 3)\n",
      "0.844951\n",
      "[Epoch 1/5] [Batch 187/360] [D loss: 0.006592] [G loss: 11.695458] time: 0:01:15.050763\n",
      "(30, 32, 32, 3)\n",
      "0.9148051\n",
      "[Epoch 1/5] [Batch 188/360] [D loss: 0.004931] [G loss: 10.270721] time: 0:01:15.163587\n",
      "(30, 32, 32, 3)\n",
      "0.78133273\n",
      "[Epoch 1/5] [Batch 189/360] [D loss: 0.003947] [G loss: 10.618944] time: 0:01:15.277551\n",
      "(30, 32, 32, 3)\n",
      "0.8381839\n",
      "[Epoch 1/5] [Batch 190/360] [D loss: 0.005280] [G loss: 11.250366] time: 0:01:15.388642\n",
      "(30, 32, 32, 3)\n",
      "0.86591196\n",
      "[Epoch 1/5] [Batch 191/360] [D loss: 0.003676] [G loss: 10.401276] time: 0:01:15.498371\n",
      "(30, 32, 32, 3)\n",
      "0.8663257\n",
      "[Epoch 1/5] [Batch 192/360] [D loss: 0.003724] [G loss: 10.616518] time: 0:01:15.608477\n",
      "(30, 32, 32, 3)\n",
      "0.7773779\n",
      "[Epoch 1/5] [Batch 193/360] [D loss: 0.002885] [G loss: 10.502835] time: 0:01:15.723305\n",
      "(30, 32, 32, 3)\n",
      "0.88325876\n",
      "[Epoch 1/5] [Batch 194/360] [D loss: 0.005076] [G loss: 10.695109] time: 0:01:15.835010\n",
      "(30, 32, 32, 3)\n",
      "0.8625869\n",
      "[Epoch 1/5] [Batch 195/360] [D loss: 0.004473] [G loss: 10.641528] time: 0:01:15.949830\n",
      "(30, 32, 32, 3)\n",
      "0.793793\n",
      "[Epoch 1/5] [Batch 196/360] [D loss: 0.004081] [G loss: 9.865968] time: 0:01:16.063494\n",
      "(30, 32, 32, 3)\n",
      "0.76274854\n",
      "[Epoch 1/5] [Batch 197/360] [D loss: 0.009853] [G loss: 10.360388] time: 0:01:16.175416\n",
      "(30, 32, 32, 3)\n",
      "0.88402206\n",
      "[Epoch 1/5] [Batch 198/360] [D loss: 0.006185] [G loss: 12.443826] time: 0:01:16.284837\n",
      "(30, 32, 32, 3)\n",
      "0.8175803\n",
      "[Epoch 1/5] [Batch 199/360] [D loss: 0.003881] [G loss: 10.756206] time: 0:01:16.396893\n",
      "(30, 32, 32, 3)\n",
      "0.7294093\n",
      "[Epoch 1/5] [Batch 200/360] [D loss: 0.003341] [G loss: 10.956804] time: 0:01:16.511002\n",
      "(30, 32, 32, 3)\n",
      "0.7911418\n",
      "[Epoch 1/5] [Batch 201/360] [D loss: 0.004375] [G loss: 10.266891] time: 0:01:16.625354\n",
      "(30, 32, 32, 3)\n",
      "0.8813348\n",
      "[Epoch 1/5] [Batch 202/360] [D loss: 0.004810] [G loss: 10.621161] time: 0:01:16.736604\n",
      "(30, 32, 32, 3)\n",
      "0.86608\n",
      "[Epoch 1/5] [Batch 203/360] [D loss: 0.004465] [G loss: 10.261407] time: 0:01:16.843345\n",
      "(30, 32, 32, 3)\n",
      "0.7949991\n",
      "[Epoch 1/5] [Batch 204/360] [D loss: 0.005604] [G loss: 10.201072] time: 0:01:16.946671\n",
      "(30, 32, 32, 3)\n",
      "0.78425294\n",
      "[Epoch 1/5] [Batch 205/360] [D loss: 0.003737] [G loss: 10.390962] time: 0:01:17.055161\n",
      "(30, 32, 32, 3)\n",
      "0.89599013\n",
      "[Epoch 1/5] [Batch 206/360] [D loss: 0.004359] [G loss: 10.795210] time: 0:01:17.159633\n",
      "(30, 32, 32, 3)\n",
      "0.7507434\n",
      "[Epoch 1/5] [Batch 207/360] [D loss: 0.013633] [G loss: 10.643703] time: 0:01:17.266087\n",
      "(30, 32, 32, 3)\n",
      "0.80256015\n",
      "[Epoch 1/5] [Batch 208/360] [D loss: 0.006146] [G loss: 10.204286] time: 0:01:17.376983\n",
      "(30, 32, 32, 3)\n",
      "0.89546347\n",
      "[Epoch 1/5] [Batch 209/360] [D loss: 0.004203] [G loss: 10.906620] time: 0:01:17.490754\n",
      "(30, 32, 32, 3)\n",
      "0.8524496\n",
      "[Epoch 1/5] [Batch 210/360] [D loss: 0.006419] [G loss: 10.980666] time: 0:01:17.595329\n",
      "(30, 32, 32, 3)\n",
      "0.79134035\n",
      "[Epoch 1/5] [Batch 211/360] [D loss: 0.004329] [G loss: 10.134044] time: 0:01:17.701179\n",
      "(30, 32, 32, 3)\n",
      "0.84090465\n",
      "[Epoch 1/5] [Batch 212/360] [D loss: 0.006508] [G loss: 10.701216] time: 0:01:17.808602\n",
      "(30, 32, 32, 3)\n",
      "0.8487158\n",
      "[Epoch 1/5] [Batch 213/360] [D loss: 0.005612] [G loss: 10.255262] time: 0:01:17.920539\n",
      "(30, 32, 32, 3)\n",
      "0.7648663\n",
      "[Epoch 1/5] [Batch 214/360] [D loss: 0.003629] [G loss: 10.864440] time: 0:01:18.033880\n",
      "(30, 32, 32, 3)\n",
      "0.83849984\n",
      "[Epoch 1/5] [Batch 215/360] [D loss: 0.003807] [G loss: 10.161250] time: 0:01:18.140984\n",
      "(30, 32, 32, 3)\n",
      "0.84636235\n",
      "[Epoch 1/5] [Batch 216/360] [D loss: 0.003457] [G loss: 11.040369] time: 0:01:18.246290\n",
      "(30, 32, 32, 3)\n",
      "0.84382397\n",
      "[Epoch 1/5] [Batch 217/360] [D loss: 0.003134] [G loss: 10.926397] time: 0:01:18.351395\n",
      "(30, 32, 32, 3)\n",
      "0.8784116\n",
      "[Epoch 1/5] [Batch 218/360] [D loss: 0.005386] [G loss: 10.392093] time: 0:01:18.457569\n",
      "(30, 32, 32, 3)\n",
      "0.8273961\n",
      "[Epoch 1/5] [Batch 219/360] [D loss: 0.013430] [G loss: 10.442043] time: 0:01:18.561732\n",
      "(30, 32, 32, 3)\n",
      "0.8427747\n",
      "[Epoch 1/5] [Batch 220/360] [D loss: 0.009783] [G loss: 10.250663] time: 0:01:18.671871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.7827797\n",
      "[Epoch 1/5] [Batch 221/360] [D loss: 0.006536] [G loss: 10.257089] time: 0:01:18.782495\n",
      "(30, 32, 32, 3)\n",
      "0.82711935\n",
      "[Epoch 1/5] [Batch 222/360] [D loss: 0.028837] [G loss: 10.196095] time: 0:01:18.886994\n",
      "(30, 32, 32, 3)\n",
      "0.81995994\n",
      "[Epoch 1/5] [Batch 223/360] [D loss: 0.124235] [G loss: 10.855564] time: 0:01:18.993791\n",
      "(30, 32, 32, 3)\n",
      "0.79634625\n",
      "[Epoch 1/5] [Batch 224/360] [D loss: 0.753604] [G loss: 10.446626] time: 0:01:19.098841\n",
      "(30, 32, 32, 3)\n",
      "0.8408365\n",
      "[Epoch 1/5] [Batch 225/360] [D loss: 0.128233] [G loss: 9.589205] time: 0:01:19.209663\n",
      "(30, 32, 32, 3)\n",
      "0.84006715\n",
      "[Epoch 1/5] [Batch 226/360] [D loss: 0.073314] [G loss: 9.793876] time: 0:01:19.322926\n",
      "(30, 32, 32, 3)\n",
      "0.85518306\n",
      "[Epoch 1/5] [Batch 227/360] [D loss: 0.055719] [G loss: 10.298013] time: 0:01:19.439030\n",
      "(30, 32, 32, 3)\n",
      "0.8031266\n",
      "[Epoch 1/5] [Batch 228/360] [D loss: 0.021395] [G loss: 11.449308] time: 0:01:19.550981\n",
      "(30, 32, 32, 3)\n",
      "0.82907003\n",
      "[Epoch 1/5] [Batch 229/360] [D loss: 0.022573] [G loss: 10.728559] time: 0:01:19.661053\n",
      "(30, 32, 32, 3)\n",
      "0.84814423\n",
      "[Epoch 1/5] [Batch 230/360] [D loss: 0.010929] [G loss: 10.476996] time: 0:01:19.765448\n",
      "(30, 32, 32, 3)\n",
      "0.86320275\n",
      "[Epoch 1/5] [Batch 231/360] [D loss: 0.012250] [G loss: 9.792645] time: 0:01:19.878211\n",
      "(30, 32, 32, 3)\n",
      "0.8563371\n",
      "[Epoch 1/5] [Batch 232/360] [D loss: 0.007365] [G loss: 10.512613] time: 0:01:19.991691\n",
      "(30, 32, 32, 3)\n",
      "0.87057465\n",
      "[Epoch 1/5] [Batch 233/360] [D loss: 0.009747] [G loss: 10.017548] time: 0:01:20.097247\n",
      "(30, 32, 32, 3)\n",
      "0.8785855\n",
      "[Epoch 1/5] [Batch 234/360] [D loss: 0.008027] [G loss: 10.468009] time: 0:01:20.200885\n",
      "(30, 32, 32, 3)\n",
      "0.80073375\n",
      "[Epoch 1/5] [Batch 235/360] [D loss: 0.006879] [G loss: 9.979278] time: 0:01:20.304336\n",
      "(30, 32, 32, 3)\n",
      "0.8229902\n",
      "[Epoch 1/5] [Batch 236/360] [D loss: 0.006315] [G loss: 9.911757] time: 0:01:20.410602\n",
      "(30, 32, 32, 3)\n",
      "0.7864649\n",
      "[Epoch 1/5] [Batch 237/360] [D loss: 0.007285] [G loss: 9.785592] time: 0:01:20.514436\n",
      "(30, 32, 32, 3)\n",
      "0.85817456\n",
      "[Epoch 1/5] [Batch 238/360] [D loss: 0.007078] [G loss: 10.347311] time: 0:01:20.618241\n",
      "(30, 32, 32, 3)\n",
      "0.77612066\n",
      "[Epoch 1/5] [Batch 239/360] [D loss: 0.004622] [G loss: 9.923557] time: 0:01:20.723460\n",
      "(30, 32, 32, 3)\n",
      "0.8299084\n",
      "[Epoch 1/5] [Batch 240/360] [D loss: 0.007436] [G loss: 9.873842] time: 0:01:20.837145\n",
      "(30, 32, 32, 3)\n",
      "0.8602291\n",
      "[Epoch 1/5] [Batch 241/360] [D loss: 0.005781] [G loss: 10.690228] time: 0:01:20.947920\n",
      "(30, 32, 32, 3)\n",
      "0.8646836\n",
      "[Epoch 1/5] [Batch 242/360] [D loss: 0.005597] [G loss: 10.199807] time: 0:01:21.051902\n",
      "(30, 32, 32, 3)\n",
      "0.9004307\n",
      "[Epoch 1/5] [Batch 243/360] [D loss: 0.005593] [G loss: 10.089069] time: 0:01:21.155390\n",
      "(30, 32, 32, 3)\n",
      "0.8309061\n",
      "[Epoch 1/5] [Batch 244/360] [D loss: 0.006259] [G loss: 10.477738] time: 0:01:21.260625\n",
      "(30, 32, 32, 3)\n",
      "0.7972452\n",
      "[Epoch 1/5] [Batch 245/360] [D loss: 0.009131] [G loss: 10.197598] time: 0:01:21.367186\n",
      "(30, 32, 32, 3)\n",
      "0.81960005\n",
      "[Epoch 1/5] [Batch 246/360] [D loss: 0.004651] [G loss: 9.867227] time: 0:01:21.469797\n",
      "(30, 32, 32, 3)\n",
      "0.83092666\n",
      "[Epoch 1/5] [Batch 247/360] [D loss: 0.003432] [G loss: 9.790636] time: 0:01:21.573073\n",
      "(30, 32, 32, 3)\n",
      "0.84227353\n",
      "[Epoch 1/5] [Batch 248/360] [D loss: 0.004802] [G loss: 9.581818] time: 0:01:21.676963\n",
      "(30, 32, 32, 3)\n",
      "0.83946705\n",
      "[Epoch 1/5] [Batch 249/360] [D loss: 0.008695] [G loss: 9.118313] time: 0:01:21.782420\n",
      "(30, 32, 32, 3)\n",
      "0.79371166\n",
      "[Epoch 1/5] [Batch 250/360] [D loss: 0.003683] [G loss: 9.409879] time: 0:01:21.887956\n",
      "(30, 32, 32, 3)\n",
      "0.7955103\n",
      "[Epoch 1/5] [Batch 251/360] [D loss: 0.011596] [G loss: 9.531967] time: 0:01:21.998860\n",
      "(30, 32, 32, 3)\n",
      "0.8581398\n",
      "[Epoch 1/5] [Batch 252/360] [D loss: 0.004680] [G loss: 9.654481] time: 0:01:22.101014\n",
      "(30, 32, 32, 3)\n",
      "0.8392616\n",
      "[Epoch 1/5] [Batch 253/360] [D loss: 0.007581] [G loss: 10.208130] time: 0:01:22.209025\n",
      "(30, 32, 32, 3)\n",
      "0.8200653\n",
      "[Epoch 1/5] [Batch 254/360] [D loss: 0.005400] [G loss: 9.476115] time: 0:01:22.319164\n",
      "(30, 32, 32, 3)\n",
      "0.75032014\n",
      "[Epoch 1/5] [Batch 255/360] [D loss: 0.010997] [G loss: 9.976803] time: 0:01:22.426350\n",
      "(30, 32, 32, 3)\n",
      "0.8381682\n",
      "[Epoch 1/5] [Batch 256/360] [D loss: 0.022466] [G loss: 9.978528] time: 0:01:22.532377\n",
      "(30, 32, 32, 3)\n",
      "0.8148791\n",
      "[Epoch 1/5] [Batch 257/360] [D loss: 0.035594] [G loss: 9.976591] time: 0:01:22.646791\n",
      "(30, 32, 32, 3)\n",
      "0.77072674\n",
      "[Epoch 1/5] [Batch 258/360] [D loss: 0.012214] [G loss: 9.708518] time: 0:01:22.756910\n",
      "(30, 32, 32, 3)\n",
      "0.8977873\n",
      "[Epoch 1/5] [Batch 259/360] [D loss: 0.103341] [G loss: 9.961330] time: 0:01:22.867524\n",
      "(30, 32, 32, 3)\n",
      "0.8764677\n",
      "[Epoch 1/5] [Batch 260/360] [D loss: 0.490661] [G loss: 9.649038] time: 0:01:22.973325\n",
      "(30, 32, 32, 3)\n",
      "0.8578127\n",
      "[Epoch 1/5] [Batch 261/360] [D loss: 0.127190] [G loss: 9.597401] time: 0:01:23.079860\n",
      "(30, 32, 32, 3)\n",
      "0.8456029\n",
      "[Epoch 1/5] [Batch 262/360] [D loss: 0.016208] [G loss: 9.838863] time: 0:01:23.189420\n",
      "(30, 32, 32, 3)\n",
      "0.8502755\n",
      "[Epoch 1/5] [Batch 263/360] [D loss: 0.040711] [G loss: 10.223516] time: 0:01:23.304368\n",
      "(30, 32, 32, 3)\n",
      "0.81218505\n",
      "[Epoch 1/5] [Batch 264/360] [D loss: 0.010371] [G loss: 10.027548] time: 0:01:23.418175\n",
      "(30, 32, 32, 3)\n",
      "0.8704575\n",
      "[Epoch 1/5] [Batch 265/360] [D loss: 0.011571] [G loss: 9.361235] time: 0:01:23.529293\n",
      "(30, 32, 32, 3)\n",
      "0.7961447\n",
      "[Epoch 1/5] [Batch 266/360] [D loss: 0.008224] [G loss: 9.420264] time: 0:01:23.640638\n",
      "(30, 32, 32, 3)\n",
      "0.8227353\n",
      "[Epoch 1/5] [Batch 267/360] [D loss: 0.011260] [G loss: 9.660601] time: 0:01:23.749041\n",
      "(30, 32, 32, 3)\n",
      "0.8272845\n",
      "[Epoch 1/5] [Batch 268/360] [D loss: 0.006427] [G loss: 9.486342] time: 0:01:23.860837\n",
      "(30, 32, 32, 3)\n",
      "0.81699497\n",
      "[Epoch 1/5] [Batch 269/360] [D loss: 0.009676] [G loss: 9.653522] time: 0:01:23.971251\n",
      "(30, 32, 32, 3)\n",
      "0.82732874\n",
      "[Epoch 1/5] [Batch 270/360] [D loss: 0.005556] [G loss: 10.906043] time: 0:01:24.073830\n",
      "(30, 32, 32, 3)\n",
      "0.78707415\n",
      "[Epoch 1/5] [Batch 271/360] [D loss: 0.010303] [G loss: 10.408456] time: 0:01:24.178911\n",
      "(30, 32, 32, 3)\n",
      "0.84146214\n",
      "[Epoch 1/5] [Batch 272/360] [D loss: 0.005179] [G loss: 9.840916] time: 0:01:24.288932\n",
      "(30, 32, 32, 3)\n",
      "0.86415285\n",
      "[Epoch 1/5] [Batch 273/360] [D loss: 0.004799] [G loss: 9.827830] time: 0:01:24.397290\n",
      "(30, 32, 32, 3)\n",
      "0.88644797\n",
      "[Epoch 1/5] [Batch 274/360] [D loss: 0.004887] [G loss: 9.551581] time: 0:01:24.502268\n",
      "(30, 32, 32, 3)\n",
      "0.8134845\n",
      "[Epoch 1/5] [Batch 275/360] [D loss: 0.007041] [G loss: 9.472300] time: 0:01:24.607542\n",
      "(30, 32, 32, 3)\n",
      "0.82967615\n",
      "[Epoch 1/5] [Batch 276/360] [D loss: 0.004300] [G loss: 9.775904] time: 0:01:24.712007\n",
      "(30, 32, 32, 3)\n",
      "0.7952962\n",
      "[Epoch 1/5] [Batch 277/360] [D loss: 0.005505] [G loss: 9.386683] time: 0:01:24.820722\n",
      "(30, 32, 32, 3)\n",
      "0.9136316\n",
      "[Epoch 1/5] [Batch 278/360] [D loss: 0.004757] [G loss: 9.956761] time: 0:01:24.925002\n",
      "(30, 32, 32, 3)\n",
      "0.87879974\n",
      "[Epoch 1/5] [Batch 279/360] [D loss: 0.007255] [G loss: 9.076695] time: 0:01:25.035323\n",
      "(30, 32, 32, 3)\n",
      "0.8625334\n",
      "[Epoch 1/5] [Batch 280/360] [D loss: 0.004142] [G loss: 9.162522] time: 0:01:25.146513\n",
      "(30, 32, 32, 3)\n",
      "0.83203554\n",
      "[Epoch 1/5] [Batch 281/360] [D loss: 0.009955] [G loss: 9.286808] time: 0:01:25.343369\n",
      "(30, 32, 32, 3)\n",
      "0.8293686\n",
      "[Epoch 1/5] [Batch 282/360] [D loss: 0.004480] [G loss: 9.455721] time: 0:01:25.452142\n",
      "(30, 32, 32, 3)\n",
      "0.82837343\n",
      "[Epoch 1/5] [Batch 283/360] [D loss: 0.011537] [G loss: 8.503279] time: 0:01:25.556887\n",
      "(30, 32, 32, 3)\n",
      "0.88437515\n",
      "[Epoch 1/5] [Batch 284/360] [D loss: 0.004365] [G loss: 9.050020] time: 0:01:25.662092\n",
      "(30, 32, 32, 3)\n",
      "0.82403296\n",
      "[Epoch 1/5] [Batch 285/360] [D loss: 0.017979] [G loss: 10.432257] time: 0:01:25.766338\n",
      "(30, 32, 32, 3)\n",
      "0.8330064\n",
      "[Epoch 1/5] [Batch 286/360] [D loss: 0.013537] [G loss: 9.104724] time: 0:01:25.874020\n",
      "(30, 32, 32, 3)\n",
      "0.8650239\n",
      "[Epoch 1/5] [Batch 287/360] [D loss: 0.005498] [G loss: 9.111331] time: 0:01:25.978711\n",
      "(30, 32, 32, 3)\n",
      "0.885825\n",
      "[Epoch 1/5] [Batch 288/360] [D loss: 0.015115] [G loss: 9.649809] time: 0:01:26.092384\n",
      "(30, 32, 32, 3)\n",
      "0.9040179\n",
      "[Epoch 1/5] [Batch 289/360] [D loss: 0.012915] [G loss: 9.691822] time: 0:01:26.201058\n",
      "(30, 32, 32, 3)\n",
      "0.87471265\n",
      "[Epoch 1/5] [Batch 290/360] [D loss: 0.004606] [G loss: 8.905369] time: 0:01:26.316658\n",
      "(30, 32, 32, 3)\n",
      "0.8588322\n",
      "[Epoch 1/5] [Batch 291/360] [D loss: 0.006101] [G loss: 9.326593] time: 0:01:26.421820\n",
      "(30, 32, 32, 3)\n",
      "0.78464395\n",
      "[Epoch 1/5] [Batch 292/360] [D loss: 0.015049] [G loss: 9.417239] time: 0:01:26.532867\n",
      "(30, 32, 32, 3)\n",
      "0.7716541\n",
      "[Epoch 1/5] [Batch 293/360] [D loss: 0.009079] [G loss: 9.310749] time: 0:01:26.643579\n",
      "(30, 32, 32, 3)\n",
      "0.81060314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] [Batch 294/360] [D loss: 0.005959] [G loss: 8.581060] time: 0:01:26.747643\n",
      "(30, 32, 32, 3)\n",
      "0.82424664\n",
      "[Epoch 1/5] [Batch 295/360] [D loss: 0.015477] [G loss: 9.362779] time: 0:01:26.855188\n",
      "(30, 32, 32, 3)\n",
      "0.898984\n",
      "[Epoch 1/5] [Batch 296/360] [D loss: 0.018375] [G loss: 8.866317] time: 0:01:26.963502\n",
      "(30, 32, 32, 3)\n",
      "0.80582136\n",
      "[Epoch 1/5] [Batch 297/360] [D loss: 0.006634] [G loss: 9.070831] time: 0:01:27.070448\n",
      "(30, 32, 32, 3)\n",
      "0.81428176\n",
      "[Epoch 1/5] [Batch 298/360] [D loss: 0.029558] [G loss: 8.668247] time: 0:01:27.178912\n",
      "(30, 32, 32, 3)\n",
      "0.8536639\n",
      "[Epoch 1/5] [Batch 299/360] [D loss: 0.025589] [G loss: 9.558660] time: 0:01:27.290973\n",
      "(30, 32, 32, 3)\n",
      "0.77694553\n",
      "[Epoch 1/5] [Batch 300/360] [D loss: 0.018287] [G loss: 9.971682] time: 0:01:27.396010\n",
      "(30, 32, 32, 3)\n",
      "0.8188904\n",
      "[Epoch 1/5] [Batch 301/360] [D loss: 0.018939] [G loss: 9.629959] time: 0:01:27.503152\n",
      "(30, 32, 32, 3)\n",
      "0.8626773\n",
      "[Epoch 1/5] [Batch 302/360] [D loss: 0.127011] [G loss: 9.913121] time: 0:01:27.608297\n",
      "(30, 32, 32, 3)\n",
      "0.85874444\n",
      "[Epoch 1/5] [Batch 303/360] [D loss: 0.720371] [G loss: 8.512591] time: 0:01:27.717069\n",
      "(30, 32, 32, 3)\n",
      "0.8382778\n",
      "[Epoch 1/5] [Batch 304/360] [D loss: 0.031941] [G loss: 8.788949] time: 0:01:27.829706\n",
      "(30, 32, 32, 3)\n",
      "0.8885143\n",
      "[Epoch 1/5] [Batch 305/360] [D loss: 0.138467] [G loss: 8.928941] time: 0:01:27.937064\n",
      "(30, 32, 32, 3)\n",
      "0.7757004\n",
      "[Epoch 1/5] [Batch 306/360] [D loss: 0.071291] [G loss: 9.014096] time: 0:01:28.048468\n",
      "(30, 32, 32, 3)\n",
      "0.75794315\n",
      "[Epoch 1/5] [Batch 307/360] [D loss: 0.050572] [G loss: 9.189876] time: 0:01:28.153950\n",
      "(30, 32, 32, 3)\n",
      "0.82815456\n",
      "[Epoch 1/5] [Batch 308/360] [D loss: 0.017937] [G loss: 9.004313] time: 0:01:28.263083\n",
      "(30, 32, 32, 3)\n",
      "0.8992931\n",
      "[Epoch 1/5] [Batch 309/360] [D loss: 0.016476] [G loss: 9.304974] time: 0:01:28.376698\n",
      "(30, 32, 32, 3)\n",
      "0.9073879\n",
      "[Epoch 1/5] [Batch 310/360] [D loss: 0.011947] [G loss: 8.801219] time: 0:01:28.482003\n",
      "(30, 32, 32, 3)\n",
      "0.804342\n",
      "[Epoch 1/5] [Batch 311/360] [D loss: 0.010419] [G loss: 8.848963] time: 0:01:28.586524\n",
      "(30, 32, 32, 3)\n",
      "0.7757873\n",
      "[Epoch 1/5] [Batch 312/360] [D loss: 0.006093] [G loss: 9.253819] time: 0:01:28.698292\n",
      "(30, 32, 32, 3)\n",
      "0.91638803\n",
      "[Epoch 1/5] [Batch 313/360] [D loss: 0.011252] [G loss: 9.348151] time: 0:01:28.816943\n",
      "(30, 32, 32, 3)\n",
      "0.88313633\n",
      "[Epoch 1/5] [Batch 314/360] [D loss: 0.009196] [G loss: 8.970483] time: 0:01:28.923659\n",
      "(30, 32, 32, 3)\n",
      "0.8604045\n",
      "[Epoch 1/5] [Batch 315/360] [D loss: 0.010019] [G loss: 9.329784] time: 0:01:29.037193\n",
      "(30, 32, 32, 3)\n",
      "0.9028495\n",
      "[Epoch 1/5] [Batch 316/360] [D loss: 0.015507] [G loss: 9.141474] time: 0:01:29.150630\n",
      "(30, 32, 32, 3)\n",
      "0.90676767\n",
      "[Epoch 1/5] [Batch 317/360] [D loss: 0.007105] [G loss: 9.481257] time: 0:01:29.276737\n",
      "(30, 32, 32, 3)\n",
      "0.87648064\n",
      "[Epoch 1/5] [Batch 318/360] [D loss: 0.015954] [G loss: 9.029611] time: 0:01:29.385927\n",
      "(30, 32, 32, 3)\n",
      "0.83805674\n",
      "[Epoch 1/5] [Batch 319/360] [D loss: 0.009981] [G loss: 8.716304] time: 0:01:29.497388\n",
      "(30, 32, 32, 3)\n",
      "0.92540336\n",
      "[Epoch 1/5] [Batch 320/360] [D loss: 0.010356] [G loss: 9.217841] time: 0:01:29.601466\n",
      "(30, 32, 32, 3)\n",
      "0.88581586\n",
      "[Epoch 1/5] [Batch 321/360] [D loss: 0.009540] [G loss: 8.618527] time: 0:01:29.704686\n",
      "(30, 32, 32, 3)\n",
      "0.83508253\n",
      "[Epoch 1/5] [Batch 322/360] [D loss: 0.005081] [G loss: 9.097053] time: 0:01:29.814038\n",
      "(30, 32, 32, 3)\n",
      "0.88620955\n",
      "[Epoch 1/5] [Batch 323/360] [D loss: 0.010804] [G loss: 9.488727] time: 0:01:29.919690\n",
      "(30, 32, 32, 3)\n",
      "0.89211327\n",
      "[Epoch 1/5] [Batch 324/360] [D loss: 0.018217] [G loss: 9.281164] time: 0:01:30.022644\n",
      "(30, 32, 32, 3)\n",
      "0.83782214\n",
      "[Epoch 1/5] [Batch 325/360] [D loss: 0.006228] [G loss: 9.023732] time: 0:01:30.126470\n",
      "(30, 32, 32, 3)\n",
      "0.8572355\n",
      "[Epoch 1/5] [Batch 326/360] [D loss: 0.019824] [G loss: 9.297229] time: 0:01:30.239058\n",
      "(30, 32, 32, 3)\n",
      "0.8046095\n",
      "[Epoch 1/5] [Batch 327/360] [D loss: 0.030353] [G loss: 8.350909] time: 0:01:30.344075\n",
      "(30, 32, 32, 3)\n",
      "0.8547938\n",
      "[Epoch 1/5] [Batch 328/360] [D loss: 0.011758] [G loss: 8.532123] time: 0:01:30.451157\n",
      "(30, 32, 32, 3)\n",
      "0.80332994\n",
      "[Epoch 1/5] [Batch 329/360] [D loss: 0.071084] [G loss: 9.276546] time: 0:01:30.556765\n",
      "(30, 32, 32, 3)\n",
      "0.85265404\n",
      "[Epoch 1/5] [Batch 330/360] [D loss: 0.257019] [G loss: 8.635319] time: 0:01:30.662488\n",
      "(30, 32, 32, 3)\n",
      "0.82137007\n",
      "[Epoch 1/5] [Batch 331/360] [D loss: 0.332967] [G loss: 8.901818] time: 0:01:30.777520\n",
      "(30, 32, 32, 3)\n",
      "0.8722568\n",
      "[Epoch 1/5] [Batch 332/360] [D loss: 0.106361] [G loss: 8.906730] time: 0:01:30.886990\n",
      "(30, 32, 32, 3)\n",
      "0.8311308\n",
      "[Epoch 1/5] [Batch 333/360] [D loss: 0.007620] [G loss: 8.675669] time: 0:01:31.000938\n",
      "(30, 32, 32, 3)\n",
      "0.808896\n",
      "[Epoch 1/5] [Batch 334/360] [D loss: 0.012337] [G loss: 8.775809] time: 0:01:31.111055\n",
      "(30, 32, 32, 3)\n",
      "0.9291752\n",
      "[Epoch 1/5] [Batch 335/360] [D loss: 0.012724] [G loss: 8.845428] time: 0:01:31.239390\n",
      "(30, 32, 32, 3)\n",
      "0.91706735\n",
      "[Epoch 1/5] [Batch 336/360] [D loss: 0.008670] [G loss: 8.614331] time: 0:01:31.350035\n",
      "(30, 32, 32, 3)\n",
      "0.84087133\n",
      "[Epoch 1/5] [Batch 337/360] [D loss: 0.006672] [G loss: 8.559332] time: 0:01:31.462082\n",
      "(30, 32, 32, 3)\n",
      "0.8644132\n",
      "[Epoch 1/5] [Batch 338/360] [D loss: 0.008596] [G loss: 8.790807] time: 0:01:31.570937\n",
      "(30, 32, 32, 3)\n",
      "0.8935582\n",
      "[Epoch 1/5] [Batch 339/360] [D loss: 0.006003] [G loss: 8.345680] time: 0:01:31.675056\n",
      "(30, 32, 32, 3)\n",
      "0.88713557\n",
      "[Epoch 1/5] [Batch 340/360] [D loss: 0.006638] [G loss: 8.519516] time: 0:01:31.786615\n",
      "(30, 32, 32, 3)\n",
      "0.84686327\n",
      "[Epoch 1/5] [Batch 341/360] [D loss: 0.010387] [G loss: 8.808430] time: 0:01:31.892337\n",
      "(30, 32, 32, 3)\n",
      "0.8523519\n",
      "[Epoch 1/5] [Batch 342/360] [D loss: 0.011014] [G loss: 9.100037] time: 0:01:32.003332\n",
      "(30, 32, 32, 3)\n",
      "0.8763342\n",
      "[Epoch 1/5] [Batch 343/360] [D loss: 0.006096] [G loss: 9.578535] time: 0:01:32.109175\n",
      "(30, 32, 32, 3)\n",
      "0.8273789\n",
      "[Epoch 1/5] [Batch 344/360] [D loss: 0.013021] [G loss: 9.196476] time: 0:01:32.219646\n",
      "(30, 32, 32, 3)\n",
      "0.82976073\n",
      "[Epoch 1/5] [Batch 345/360] [D loss: 0.008575] [G loss: 8.416709] time: 0:01:32.325368\n",
      "(30, 32, 32, 3)\n",
      "0.8113654\n",
      "[Epoch 1/5] [Batch 346/360] [D loss: 0.005184] [G loss: 8.796561] time: 0:01:32.429370\n",
      "(30, 32, 32, 3)\n",
      "0.90064055\n",
      "[Epoch 1/5] [Batch 347/360] [D loss: 0.017746] [G loss: 8.355490] time: 0:01:32.535282\n",
      "(30, 32, 32, 3)\n",
      "0.8725934\n",
      "[Epoch 1/5] [Batch 348/360] [D loss: 0.004698] [G loss: 8.338231] time: 0:01:32.644739\n",
      "(30, 32, 32, 3)\n",
      "0.81005955\n",
      "[Epoch 1/5] [Batch 349/360] [D loss: 0.026359] [G loss: 8.731617] time: 0:01:32.761277\n",
      "(30, 32, 32, 3)\n",
      "0.84894735\n",
      "[Epoch 1/5] [Batch 350/360] [D loss: 0.017894] [G loss: 7.993805] time: 0:01:32.876234\n",
      "(30, 32, 32, 3)\n",
      "0.81648844\n",
      "[Epoch 1/5] [Batch 351/360] [D loss: 0.009906] [G loss: 8.677658] time: 0:01:32.987789\n",
      "(30, 32, 32, 3)\n",
      "0.8633806\n",
      "[Epoch 1/5] [Batch 352/360] [D loss: 0.063213] [G loss: 8.417390] time: 0:01:33.103017\n",
      "(30, 32, 32, 3)\n",
      "0.8522348\n",
      "[Epoch 1/5] [Batch 353/360] [D loss: 0.100674] [G loss: 8.874544] time: 0:01:33.216095\n",
      "(30, 32, 32, 3)\n",
      "0.81518835\n",
      "[Epoch 1/5] [Batch 354/360] [D loss: 0.147086] [G loss: 9.341227] time: 0:01:33.326966\n",
      "(30, 32, 32, 3)\n",
      "0.9049854\n",
      "[Epoch 1/5] [Batch 355/360] [D loss: 0.176262] [G loss: 9.222171] time: 0:01:33.436086\n",
      "(30, 32, 32, 3)\n",
      "0.8888542\n",
      "[Epoch 1/5] [Batch 356/360] [D loss: 0.047288] [G loss: 8.279349] time: 0:01:33.544645\n",
      "(30, 32, 32, 3)\n",
      "0.821883\n",
      "[Epoch 1/5] [Batch 357/360] [D loss: 0.034535] [G loss: 8.969003] time: 0:01:33.661369\n",
      "(30, 32, 32, 3)\n",
      "0.88801366\n",
      "[Epoch 1/5] [Batch 358/360] [D loss: 0.039625] [G loss: 8.863087] time: 0:01:33.779899\n",
      "(30, 32, 32, 3)\n",
      "0.81999874\n",
      "[Epoch 2/5] [Batch 0/360] [D loss: 0.023288] [G loss: 8.304849] time: 0:01:33.901411\n",
      "(30, 32, 32, 3)\n",
      "0.8343792\n",
      "[Epoch 2/5] [Batch 1/360] [D loss: 0.011470] [G loss: 8.305126] time: 0:01:34.026764\n",
      "(30, 32, 32, 3)\n",
      "0.87512237\n",
      "[Epoch 2/5] [Batch 2/360] [D loss: 0.011047] [G loss: 8.859380] time: 0:01:34.141576\n",
      "(30, 32, 32, 3)\n",
      "0.84565306\n",
      "[Epoch 2/5] [Batch 3/360] [D loss: 0.006813] [G loss: 8.431743] time: 0:01:34.259155\n",
      "(30, 32, 32, 3)\n",
      "0.8919974\n",
      "[Epoch 2/5] [Batch 4/360] [D loss: 0.009756] [G loss: 8.812582] time: 0:01:34.372503\n",
      "(30, 32, 32, 3)\n",
      "0.88311005\n",
      "[Epoch 2/5] [Batch 5/360] [D loss: 0.006336] [G loss: 8.996163] time: 0:01:34.477838\n",
      "(30, 32, 32, 3)\n",
      "0.8501342\n",
      "[Epoch 2/5] [Batch 6/360] [D loss: 0.007480] [G loss: 8.663408] time: 0:01:34.587222\n",
      "(30, 32, 32, 3)\n",
      "0.8227213\n",
      "[Epoch 2/5] [Batch 7/360] [D loss: 0.006671] [G loss: 8.824694] time: 0:01:34.693966\n",
      "(30, 32, 32, 3)\n",
      "0.8676195\n",
      "[Epoch 2/5] [Batch 8/360] [D loss: 0.011120] [G loss: 7.929754] time: 0:01:34.801952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.83133197\n",
      "[Epoch 2/5] [Batch 9/360] [D loss: 0.006212] [G loss: 8.462986] time: 0:01:34.911050\n",
      "(30, 32, 32, 3)\n",
      "0.7782629\n",
      "[Epoch 2/5] [Batch 10/360] [D loss: 0.009612] [G loss: 8.573578] time: 0:01:35.021022\n",
      "(30, 32, 32, 3)\n",
      "0.9089946\n",
      "[Epoch 2/5] [Batch 11/360] [D loss: 0.005318] [G loss: 8.463611] time: 0:01:35.129669\n",
      "(30, 32, 32, 3)\n",
      "0.83692545\n",
      "[Epoch 2/5] [Batch 12/360] [D loss: 0.009167] [G loss: 8.189332] time: 0:01:35.246204\n",
      "(30, 32, 32, 3)\n",
      "0.8539869\n",
      "[Epoch 2/5] [Batch 13/360] [D loss: 0.006705] [G loss: 8.163039] time: 0:01:35.353298\n",
      "(30, 32, 32, 3)\n",
      "0.83874863\n",
      "[Epoch 2/5] [Batch 14/360] [D loss: 0.006390] [G loss: 8.592624] time: 0:01:35.459390\n",
      "(30, 32, 32, 3)\n",
      "0.7873209\n",
      "[Epoch 2/5] [Batch 15/360] [D loss: 0.006698] [G loss: 7.865068] time: 0:01:35.576280\n",
      "(30, 32, 32, 3)\n",
      "0.8649358\n",
      "[Epoch 2/5] [Batch 16/360] [D loss: 0.007073] [G loss: 8.021351] time: 0:01:35.683131\n",
      "(30, 32, 32, 3)\n",
      "0.7943576\n",
      "[Epoch 2/5] [Batch 17/360] [D loss: 0.010398] [G loss: 8.379616] time: 0:01:35.795003\n",
      "(30, 32, 32, 3)\n",
      "0.8743319\n",
      "[Epoch 2/5] [Batch 18/360] [D loss: 0.008877] [G loss: 8.598834] time: 0:01:35.906095\n",
      "(30, 32, 32, 3)\n",
      "0.81715363\n",
      "[Epoch 2/5] [Batch 19/360] [D loss: 0.026743] [G loss: 8.550549] time: 0:01:36.014277\n",
      "(30, 32, 32, 3)\n",
      "0.8477847\n",
      "[Epoch 2/5] [Batch 20/360] [D loss: 0.045876] [G loss: 7.885892] time: 0:01:36.118697\n",
      "(30, 32, 32, 3)\n",
      "0.8172548\n",
      "[Epoch 2/5] [Batch 21/360] [D loss: 0.008077] [G loss: 8.635595] time: 0:01:36.227307\n",
      "(30, 32, 32, 3)\n",
      "0.858543\n",
      "[Epoch 2/5] [Batch 22/360] [D loss: 0.032477] [G loss: 8.037539] time: 0:01:36.336642\n",
      "(30, 32, 32, 3)\n",
      "0.8581228\n",
      "[Epoch 2/5] [Batch 23/360] [D loss: 0.009176] [G loss: 8.207051] time: 0:01:36.442777\n",
      "(30, 32, 32, 3)\n",
      "0.89311284\n",
      "[Epoch 2/5] [Batch 24/360] [D loss: 0.008520] [G loss: 8.150213] time: 0:01:36.546783\n",
      "(30, 32, 32, 3)\n",
      "0.9106268\n",
      "[Epoch 2/5] [Batch 25/360] [D loss: 0.004486] [G loss: 8.295269] time: 0:01:36.652737\n",
      "(30, 32, 32, 3)\n",
      "0.86368877\n",
      "[Epoch 2/5] [Batch 26/360] [D loss: 0.022006] [G loss: 7.783191] time: 0:01:36.759278\n",
      "(30, 32, 32, 3)\n",
      "0.8444558\n",
      "[Epoch 2/5] [Batch 27/360] [D loss: 0.005437] [G loss: 8.916699] time: 0:01:36.865861\n",
      "(30, 32, 32, 3)\n",
      "0.86608344\n",
      "[Epoch 2/5] [Batch 28/360] [D loss: 0.033175] [G loss: 8.246832] time: 0:01:36.972966\n",
      "(30, 32, 32, 3)\n",
      "0.83674574\n",
      "[Epoch 2/5] [Batch 29/360] [D loss: 0.014764] [G loss: 7.937693] time: 0:01:37.081652\n",
      "(30, 32, 32, 3)\n",
      "0.87949324\n",
      "[Epoch 2/5] [Batch 30/360] [D loss: 0.006301] [G loss: 8.528243] time: 0:01:37.195083\n",
      "(30, 32, 32, 3)\n",
      "0.8578226\n",
      "[Epoch 2/5] [Batch 31/360] [D loss: 0.011016] [G loss: 8.631948] time: 0:01:37.311428\n",
      "(30, 32, 32, 3)\n",
      "0.8191039\n",
      "[Epoch 2/5] [Batch 32/360] [D loss: 0.006830] [G loss: 8.742745] time: 0:01:37.423123\n",
      "(30, 32, 32, 3)\n",
      "0.8447378\n",
      "[Epoch 2/5] [Batch 33/360] [D loss: 0.025487] [G loss: 7.870400] time: 0:01:37.537068\n",
      "(30, 32, 32, 3)\n",
      "0.9159195\n",
      "[Epoch 2/5] [Batch 34/360] [D loss: 0.004733] [G loss: 7.830510] time: 0:01:37.644179\n",
      "(30, 32, 32, 3)\n",
      "0.8697669\n",
      "[Epoch 2/5] [Batch 35/360] [D loss: 0.050612] [G loss: 7.942804] time: 0:01:37.754970\n",
      "(30, 32, 32, 3)\n",
      "0.90545344\n",
      "[Epoch 2/5] [Batch 36/360] [D loss: 0.116115] [G loss: 8.710685] time: 0:01:37.864203\n",
      "(30, 32, 32, 3)\n",
      "0.8612706\n",
      "[Epoch 2/5] [Batch 37/360] [D loss: 0.401275] [G loss: 7.856355] time: 0:01:37.973724\n",
      "(30, 32, 32, 3)\n",
      "0.82501143\n",
      "[Epoch 2/5] [Batch 38/360] [D loss: 0.498827] [G loss: 7.768319] time: 0:01:38.080893\n",
      "(30, 32, 32, 3)\n",
      "0.88271207\n",
      "[Epoch 2/5] [Batch 39/360] [D loss: 0.032571] [G loss: 8.007203] time: 0:01:38.189533\n",
      "(30, 32, 32, 3)\n",
      "0.87015074\n",
      "[Epoch 2/5] [Batch 40/360] [D loss: 0.091420] [G loss: 8.127978] time: 0:01:38.302057\n",
      "(30, 32, 32, 3)\n",
      "0.880362\n",
      "[Epoch 2/5] [Batch 41/360] [D loss: 0.023731] [G loss: 7.744246] time: 0:01:38.410994\n",
      "(30, 32, 32, 3)\n",
      "0.92710847\n",
      "[Epoch 2/5] [Batch 42/360] [D loss: 0.030118] [G loss: 8.103657] time: 0:01:38.516735\n",
      "(30, 32, 32, 3)\n",
      "0.89867115\n",
      "[Epoch 2/5] [Batch 43/360] [D loss: 0.032685] [G loss: 7.671528] time: 0:01:38.627535\n",
      "(30, 32, 32, 3)\n",
      "0.8134133\n",
      "[Epoch 2/5] [Batch 44/360] [D loss: 0.011514] [G loss: 7.476556] time: 0:01:38.735876\n",
      "(30, 32, 32, 3)\n",
      "0.8832014\n",
      "[Epoch 2/5] [Batch 45/360] [D loss: 0.019474] [G loss: 8.317478] time: 0:01:38.841936\n",
      "(30, 32, 32, 3)\n",
      "0.8976986\n",
      "[Epoch 2/5] [Batch 46/360] [D loss: 0.025036] [G loss: 7.771833] time: 0:01:38.946783\n",
      "(30, 32, 32, 3)\n",
      "0.8379431\n",
      "[Epoch 2/5] [Batch 47/360] [D loss: 0.011822] [G loss: 7.726656] time: 0:01:39.053032\n",
      "(30, 32, 32, 3)\n",
      "0.86898327\n",
      "[Epoch 2/5] [Batch 48/360] [D loss: 0.040028] [G loss: 8.011198] time: 0:01:39.158590\n",
      "(30, 32, 32, 3)\n",
      "0.88939697\n",
      "[Epoch 2/5] [Batch 49/360] [D loss: 0.031962] [G loss: 7.916972] time: 0:01:39.266499\n",
      "(30, 32, 32, 3)\n",
      "0.8467951\n",
      "[Epoch 2/5] [Batch 50/360] [D loss: 0.009952] [G loss: 8.418300] time: 0:01:39.370955\n",
      "(30, 32, 32, 3)\n",
      "0.89549166\n",
      "[Epoch 2/5] [Batch 51/360] [D loss: 0.022477] [G loss: 7.726949] time: 0:01:39.477875\n",
      "(30, 32, 32, 3)\n",
      "0.8696944\n",
      "[Epoch 2/5] [Batch 52/360] [D loss: 0.007819] [G loss: 7.695430] time: 0:01:39.583031\n",
      "(30, 32, 32, 3)\n",
      "0.82483715\n",
      "[Epoch 2/5] [Batch 53/360] [D loss: 0.020279] [G loss: 7.619454] time: 0:01:39.697724\n",
      "(30, 32, 32, 3)\n",
      "0.8696408\n",
      "[Epoch 2/5] [Batch 54/360] [D loss: 0.006755] [G loss: 7.951053] time: 0:01:39.815852\n",
      "(30, 32, 32, 3)\n",
      "0.8516049\n",
      "[Epoch 2/5] [Batch 55/360] [D loss: 0.010327] [G loss: 7.370871] time: 0:01:39.923814\n",
      "(30, 32, 32, 3)\n",
      "0.90645343\n",
      "[Epoch 2/5] [Batch 56/360] [D loss: 0.012149] [G loss: 7.771403] time: 0:01:40.027790\n",
      "(30, 32, 32, 3)\n",
      "0.8317456\n",
      "[Epoch 2/5] [Batch 57/360] [D loss: 0.057854] [G loss: 7.770582] time: 0:01:40.137621\n",
      "(30, 32, 32, 3)\n",
      "0.89965415\n",
      "[Epoch 2/5] [Batch 58/360] [D loss: 0.048254] [G loss: 7.509459] time: 0:01:40.241478\n",
      "(30, 32, 32, 3)\n",
      "0.81933314\n",
      "[Epoch 2/5] [Batch 59/360] [D loss: 0.007884] [G loss: 7.584310] time: 0:01:40.348691\n",
      "(30, 32, 32, 3)\n",
      "0.8413856\n",
      "[Epoch 2/5] [Batch 60/360] [D loss: 0.028382] [G loss: 7.355004] time: 0:01:40.453048\n",
      "(30, 32, 32, 3)\n",
      "0.8391407\n",
      "[Epoch 2/5] [Batch 61/360] [D loss: 0.008332] [G loss: 7.790477] time: 0:01:40.559407\n",
      "(30, 32, 32, 3)\n",
      "0.9225803\n",
      "[Epoch 2/5] [Batch 62/360] [D loss: 0.015872] [G loss: 8.098146] time: 0:01:40.668434\n",
      "(30, 32, 32, 3)\n",
      "0.84521\n",
      "[Epoch 2/5] [Batch 63/360] [D loss: 0.008770] [G loss: 7.038164] time: 0:01:40.774831\n",
      "(30, 32, 32, 3)\n",
      "0.8887251\n",
      "[Epoch 2/5] [Batch 64/360] [D loss: 0.015102] [G loss: 7.587784] time: 0:01:40.880086\n",
      "(30, 32, 32, 3)\n",
      "0.87453246\n",
      "[Epoch 2/5] [Batch 65/360] [D loss: 0.013209] [G loss: 7.871545] time: 0:01:40.987096\n",
      "(30, 32, 32, 3)\n",
      "0.8947485\n",
      "[Epoch 2/5] [Batch 66/360] [D loss: 0.007757] [G loss: 7.821170] time: 0:01:41.094828\n",
      "(30, 32, 32, 3)\n",
      "0.8590227\n",
      "[Epoch 2/5] [Batch 67/360] [D loss: 0.020626] [G loss: 7.773354] time: 0:01:41.201788\n",
      "(30, 32, 32, 3)\n",
      "0.86519766\n",
      "[Epoch 2/5] [Batch 68/360] [D loss: 0.007865] [G loss: 7.228131] time: 0:01:41.306698\n",
      "(30, 32, 32, 3)\n",
      "0.8208347\n",
      "[Epoch 2/5] [Batch 69/360] [D loss: 0.019394] [G loss: 7.940407] time: 0:01:41.418458\n",
      "(30, 32, 32, 3)\n",
      "0.7596004\n",
      "[Epoch 2/5] [Batch 70/360] [D loss: 0.013917] [G loss: 7.594362] time: 0:01:41.521007\n",
      "(30, 32, 32, 3)\n",
      "0.89686507\n",
      "[Epoch 2/5] [Batch 71/360] [D loss: 0.008439] [G loss: 7.640753] time: 0:01:41.630749\n",
      "(30, 32, 32, 3)\n",
      "0.906238\n",
      "[Epoch 2/5] [Batch 72/360] [D loss: 0.008154] [G loss: 7.533602] time: 0:01:41.735650\n",
      "(30, 32, 32, 3)\n",
      "0.87229997\n",
      "[Epoch 2/5] [Batch 73/360] [D loss: 0.005832] [G loss: 7.643349] time: 0:01:41.843156\n",
      "(30, 32, 32, 3)\n",
      "0.94201595\n",
      "[Epoch 2/5] [Batch 74/360] [D loss: 0.009255] [G loss: 7.499818] time: 0:01:41.949392\n",
      "(30, 32, 32, 3)\n",
      "0.8654435\n",
      "[Epoch 2/5] [Batch 75/360] [D loss: 0.020320] [G loss: 7.486545] time: 0:01:42.067045\n",
      "(30, 32, 32, 3)\n",
      "0.8643224\n",
      "[Epoch 2/5] [Batch 76/360] [D loss: 0.019239] [G loss: 7.563434] time: 0:01:42.171169\n",
      "(30, 32, 32, 3)\n",
      "0.87106544\n",
      "[Epoch 2/5] [Batch 77/360] [D loss: 0.019451] [G loss: 7.644750] time: 0:01:42.280528\n",
      "(30, 32, 32, 3)\n",
      "0.8631203\n",
      "[Epoch 2/5] [Batch 78/360] [D loss: 0.010858] [G loss: 7.249897] time: 0:01:42.385542\n",
      "(30, 32, 32, 3)\n",
      "0.85024995\n",
      "[Epoch 2/5] [Batch 79/360] [D loss: 0.009508] [G loss: 7.585745] time: 0:01:42.492048\n",
      "(30, 32, 32, 3)\n",
      "0.83891934\n",
      "[Epoch 2/5] [Batch 80/360] [D loss: 0.013756] [G loss: 6.822833] time: 0:01:42.599059\n",
      "(30, 32, 32, 3)\n",
      "0.8503542\n",
      "[Epoch 2/5] [Batch 81/360] [D loss: 0.009412] [G loss: 8.095510] time: 0:01:42.711040\n",
      "(30, 32, 32, 3)\n",
      "0.88073903\n",
      "[Epoch 2/5] [Batch 82/360] [D loss: 0.008580] [G loss: 7.546201] time: 0:01:42.815858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.90764683\n",
      "[Epoch 2/5] [Batch 83/360] [D loss: 0.005573] [G loss: 7.311393] time: 0:01:42.922923\n",
      "(30, 32, 32, 3)\n",
      "0.8523224\n",
      "[Epoch 2/5] [Batch 84/360] [D loss: 0.019091] [G loss: 8.011373] time: 0:01:43.032072\n",
      "(30, 32, 32, 3)\n",
      "0.8470699\n",
      "[Epoch 2/5] [Batch 85/360] [D loss: 0.083751] [G loss: 7.877299] time: 0:01:43.138037\n",
      "(30, 32, 32, 3)\n",
      "0.8505834\n",
      "[Epoch 2/5] [Batch 86/360] [D loss: 0.752612] [G loss: 6.709872] time: 0:01:43.242132\n",
      "(30, 32, 32, 3)\n",
      "0.8087036\n",
      "[Epoch 2/5] [Batch 87/360] [D loss: 0.020241] [G loss: 7.207570] time: 0:01:43.348613\n",
      "(30, 32, 32, 3)\n",
      "0.88755685\n",
      "[Epoch 2/5] [Batch 88/360] [D loss: 0.173666] [G loss: 7.332338] time: 0:01:43.453650\n",
      "(30, 32, 32, 3)\n",
      "0.82402164\n",
      "[Epoch 2/5] [Batch 89/360] [D loss: 0.069550] [G loss: 7.647099] time: 0:01:43.565772\n",
      "(30, 32, 32, 3)\n",
      "0.8768203\n",
      "[Epoch 2/5] [Batch 90/360] [D loss: 0.057657] [G loss: 6.756038] time: 0:01:43.670720\n",
      "(30, 32, 32, 3)\n",
      "0.8581801\n",
      "[Epoch 2/5] [Batch 91/360] [D loss: 0.019490] [G loss: 6.657396] time: 0:01:43.780776\n",
      "(30, 32, 32, 3)\n",
      "0.8952629\n",
      "[Epoch 2/5] [Batch 92/360] [D loss: 0.024438] [G loss: 7.683112] time: 0:01:43.885287\n",
      "(30, 32, 32, 3)\n",
      "0.8500661\n",
      "[Epoch 2/5] [Batch 93/360] [D loss: 0.246883] [G loss: 7.816326] time: 0:01:43.993966\n",
      "(30, 32, 32, 3)\n",
      "0.8826601\n",
      "[Epoch 2/5] [Batch 94/360] [D loss: 0.229820] [G loss: 7.970400] time: 0:01:44.102643\n",
      "(30, 32, 32, 3)\n",
      "0.8449383\n",
      "[Epoch 2/5] [Batch 95/360] [D loss: 0.016930] [G loss: 7.925049] time: 0:01:44.212462\n",
      "(30, 32, 32, 3)\n",
      "0.88223696\n",
      "[Epoch 2/5] [Batch 96/360] [D loss: 0.035673] [G loss: 7.558743] time: 0:01:44.316682\n",
      "(30, 32, 32, 3)\n",
      "0.8856063\n",
      "[Epoch 2/5] [Batch 97/360] [D loss: 0.016820] [G loss: 7.301619] time: 0:01:44.421937\n",
      "(30, 32, 32, 3)\n",
      "0.8911329\n",
      "[Epoch 2/5] [Batch 98/360] [D loss: 0.052773] [G loss: 7.350270] time: 0:01:44.530631\n",
      "(30, 32, 32, 3)\n",
      "0.8791676\n",
      "[Epoch 2/5] [Batch 99/360] [D loss: 0.097997] [G loss: 8.022813] time: 0:01:44.636653\n",
      "(30, 32, 32, 3)\n",
      "0.8549471\n",
      "[Epoch 2/5] [Batch 100/360] [D loss: 0.022876] [G loss: 7.086208] time: 0:01:44.740604\n",
      "(30, 32, 32, 3)\n",
      "0.79580253\n",
      "[Epoch 2/5] [Batch 101/360] [D loss: 0.023492] [G loss: 7.868934] time: 0:01:44.846204\n",
      "(30, 32, 32, 3)\n",
      "0.8155949\n",
      "[Epoch 2/5] [Batch 102/360] [D loss: 0.074947] [G loss: 7.065500] time: 0:01:44.952391\n",
      "(30, 32, 32, 3)\n",
      "0.82783526\n",
      "[Epoch 2/5] [Batch 103/360] [D loss: 0.015683] [G loss: 7.664301] time: 0:01:45.057371\n",
      "(30, 32, 32, 3)\n",
      "0.84588987\n",
      "[Epoch 2/5] [Batch 104/360] [D loss: 0.033382] [G loss: 7.332240] time: 0:01:45.161662\n",
      "(30, 32, 32, 3)\n",
      "0.8761546\n",
      "[Epoch 2/5] [Batch 105/360] [D loss: 0.062686] [G loss: 7.803826] time: 0:01:45.268805\n",
      "(30, 32, 32, 3)\n",
      "0.8769656\n",
      "[Epoch 2/5] [Batch 106/360] [D loss: 0.023709] [G loss: 6.849178] time: 0:01:45.372819\n",
      "(30, 32, 32, 3)\n",
      "0.8545203\n",
      "[Epoch 2/5] [Batch 107/360] [D loss: 0.013021] [G loss: 7.249755] time: 0:01:45.480630\n",
      "(30, 32, 32, 3)\n",
      "0.8866504\n",
      "[Epoch 2/5] [Batch 108/360] [D loss: 0.024738] [G loss: 7.011914] time: 0:01:45.584678\n",
      "(30, 32, 32, 3)\n",
      "0.8218407\n",
      "[Epoch 2/5] [Batch 109/360] [D loss: 0.013155] [G loss: 7.092958] time: 0:01:45.690328\n",
      "(30, 32, 32, 3)\n",
      "0.85660917\n",
      "[Epoch 2/5] [Batch 110/360] [D loss: 0.050335] [G loss: 7.754963] time: 0:01:45.796181\n",
      "(30, 32, 32, 3)\n",
      "0.86887234\n",
      "[Epoch 2/5] [Batch 111/360] [D loss: 0.182843] [G loss: 7.808195] time: 0:01:45.908726\n",
      "(30, 32, 32, 3)\n",
      "0.9155235\n",
      "[Epoch 2/5] [Batch 112/360] [D loss: 0.175460] [G loss: 7.563347] time: 0:01:46.015258\n",
      "(30, 32, 32, 3)\n",
      "0.90530396\n",
      "[Epoch 2/5] [Batch 113/360] [D loss: 0.192287] [G loss: 7.405272] time: 0:01:46.119359\n",
      "(30, 32, 32, 3)\n",
      "0.891651\n",
      "[Epoch 2/5] [Batch 114/360] [D loss: 0.153286] [G loss: 7.307109] time: 0:01:46.225001\n",
      "(30, 32, 32, 3)\n",
      "0.86943626\n",
      "[Epoch 2/5] [Batch 115/360] [D loss: 0.083185] [G loss: 7.058766] time: 0:01:46.329605\n",
      "(30, 32, 32, 3)\n",
      "0.8225444\n",
      "[Epoch 2/5] [Batch 116/360] [D loss: 0.018856] [G loss: 7.007863] time: 0:01:46.436366\n",
      "(30, 32, 32, 3)\n",
      "0.8970068\n",
      "[Epoch 2/5] [Batch 117/360] [D loss: 0.037692] [G loss: 7.372728] time: 0:01:46.551824\n",
      "(30, 32, 32, 3)\n",
      "0.8589392\n",
      "[Epoch 2/5] [Batch 118/360] [D loss: 0.138346] [G loss: 7.465490] time: 0:01:46.656053\n",
      "(30, 32, 32, 3)\n",
      "0.9381079\n",
      "[Epoch 2/5] [Batch 119/360] [D loss: 0.425577] [G loss: 7.694878] time: 0:01:46.760077\n",
      "(30, 32, 32, 3)\n",
      "0.83088756\n",
      "[Epoch 2/5] [Batch 120/360] [D loss: 0.172674] [G loss: 6.843654] time: 0:01:46.867336\n",
      "(30, 32, 32, 3)\n",
      "0.8551376\n",
      "[Epoch 2/5] [Batch 121/360] [D loss: 0.023775] [G loss: 7.318149] time: 0:01:46.971663\n",
      "(30, 32, 32, 3)\n",
      "0.82880276\n",
      "[Epoch 2/5] [Batch 122/360] [D loss: 0.031691] [G loss: 7.065855] time: 0:01:47.075607\n",
      "(30, 32, 32, 3)\n",
      "0.9205673\n",
      "[Epoch 2/5] [Batch 123/360] [D loss: 0.020713] [G loss: 7.583404] time: 0:01:47.181420\n",
      "(30, 32, 32, 3)\n",
      "0.8890546\n",
      "[Epoch 2/5] [Batch 124/360] [D loss: 0.036111] [G loss: 7.865542] time: 0:01:47.284515\n",
      "(30, 32, 32, 3)\n",
      "0.84008\n",
      "[Epoch 2/5] [Batch 125/360] [D loss: 0.017664] [G loss: 7.426983] time: 0:01:47.392965\n",
      "(30, 32, 32, 3)\n",
      "0.86214715\n",
      "[Epoch 2/5] [Batch 126/360] [D loss: 0.020258] [G loss: 7.079381] time: 0:01:47.497642\n",
      "(30, 32, 32, 3)\n",
      "0.8605559\n",
      "[Epoch 2/5] [Batch 127/360] [D loss: 0.020788] [G loss: 7.396285] time: 0:01:47.602408\n",
      "(30, 32, 32, 3)\n",
      "0.8472326\n",
      "[Epoch 2/5] [Batch 128/360] [D loss: 0.038964] [G loss: 6.700068] time: 0:01:47.707103\n",
      "(30, 32, 32, 3)\n",
      "0.9220794\n",
      "[Epoch 2/5] [Batch 129/360] [D loss: 0.018175] [G loss: 7.127841] time: 0:01:47.816904\n",
      "(30, 32, 32, 3)\n",
      "0.8506713\n",
      "[Epoch 2/5] [Batch 130/360] [D loss: 0.016614] [G loss: 7.163015] time: 0:01:47.921355\n",
      "(30, 32, 32, 3)\n",
      "0.8524546\n",
      "[Epoch 2/5] [Batch 131/360] [D loss: 0.012780] [G loss: 7.198034] time: 0:01:48.034422\n",
      "(30, 32, 32, 3)\n",
      "0.8560447\n",
      "[Epoch 2/5] [Batch 132/360] [D loss: 0.050251] [G loss: 7.115614] time: 0:01:48.140041\n",
      "(30, 32, 32, 3)\n",
      "0.87566215\n",
      "[Epoch 2/5] [Batch 133/360] [D loss: 0.068389] [G loss: 6.883575] time: 0:01:48.245808\n",
      "(30, 32, 32, 3)\n",
      "0.8543258\n",
      "[Epoch 2/5] [Batch 134/360] [D loss: 0.014295] [G loss: 6.886575] time: 0:01:48.356296\n",
      "(30, 32, 32, 3)\n",
      "0.90620345\n",
      "[Epoch 2/5] [Batch 135/360] [D loss: 0.011065] [G loss: 7.019967] time: 0:01:48.462212\n",
      "(30, 32, 32, 3)\n",
      "0.8606532\n",
      "[Epoch 2/5] [Batch 136/360] [D loss: 0.011870] [G loss: 6.993875] time: 0:01:48.566778\n",
      "(30, 32, 32, 3)\n",
      "0.88209206\n",
      "[Epoch 2/5] [Batch 137/360] [D loss: 0.012071] [G loss: 6.767159] time: 0:01:48.673164\n",
      "(30, 32, 32, 3)\n",
      "0.8869633\n",
      "[Epoch 2/5] [Batch 138/360] [D loss: 0.031880] [G loss: 7.089771] time: 0:01:48.779338\n",
      "(30, 32, 32, 3)\n",
      "0.9270007\n",
      "[Epoch 2/5] [Batch 139/360] [D loss: 0.102280] [G loss: 7.096290] time: 0:01:48.886348\n",
      "(30, 32, 32, 3)\n",
      "0.8832769\n",
      "[Epoch 2/5] [Batch 140/360] [D loss: 0.047826] [G loss: 6.434185] time: 0:01:48.991284\n",
      "(30, 32, 32, 3)\n",
      "0.8666158\n",
      "[Epoch 2/5] [Batch 141/360] [D loss: 0.010559] [G loss: 7.533278] time: 0:01:49.096199\n",
      "(30, 32, 32, 3)\n",
      "0.9273723\n",
      "[Epoch 2/5] [Batch 142/360] [D loss: 0.066770] [G loss: 7.075381] time: 0:01:49.199662\n",
      "(30, 32, 32, 3)\n",
      "0.89213175\n",
      "[Epoch 2/5] [Batch 143/360] [D loss: 0.267186] [G loss: 7.060357] time: 0:01:49.309946\n",
      "(30, 32, 32, 3)\n",
      "0.84252435\n",
      "[Epoch 2/5] [Batch 144/360] [D loss: 0.556333] [G loss: 6.596211] time: 0:01:49.415563\n",
      "(30, 32, 32, 3)\n",
      "0.9351956\n",
      "[Epoch 2/5] [Batch 145/360] [D loss: 0.021503] [G loss: 6.619587] time: 0:01:49.522940\n",
      "(30, 32, 32, 3)\n",
      "0.869438\n",
      "[Epoch 2/5] [Batch 146/360] [D loss: 0.152412] [G loss: 6.860247] time: 0:01:49.626300\n",
      "(30, 32, 32, 3)\n",
      "0.85642976\n",
      "[Epoch 2/5] [Batch 147/360] [D loss: 0.025370] [G loss: 6.539082] time: 0:01:49.734271\n",
      "(30, 32, 32, 3)\n",
      "0.85546607\n",
      "[Epoch 2/5] [Batch 148/360] [D loss: 0.045261] [G loss: 7.000373] time: 0:01:49.839209\n",
      "(30, 32, 32, 3)\n",
      "0.87267894\n",
      "[Epoch 2/5] [Batch 149/360] [D loss: 0.058012] [G loss: 7.333890] time: 0:01:49.943976\n",
      "(30, 32, 32, 3)\n",
      "0.83371735\n",
      "[Epoch 2/5] [Batch 150/360] [D loss: 0.022222] [G loss: 6.929965] time: 0:01:50.047123\n",
      "(30, 32, 32, 3)\n",
      "0.82780725\n",
      "[Epoch 2/5] [Batch 151/360] [D loss: 0.014472] [G loss: 6.784587] time: 0:01:50.152717\n",
      "(30, 32, 32, 3)\n",
      "0.8765413\n",
      "[Epoch 2/5] [Batch 152/360] [D loss: 0.018915] [G loss: 6.560504] time: 0:01:50.261640\n",
      "(30, 32, 32, 3)\n",
      "0.8507027\n",
      "[Epoch 2/5] [Batch 153/360] [D loss: 0.046052] [G loss: 7.189847] time: 0:01:50.366253\n",
      "(30, 32, 32, 3)\n",
      "0.8911467\n",
      "[Epoch 2/5] [Batch 154/360] [D loss: 0.063742] [G loss: 7.089050] time: 0:01:50.474784\n",
      "(30, 32, 32, 3)\n",
      "0.79591674\n",
      "[Epoch 2/5] [Batch 155/360] [D loss: 0.028396] [G loss: 6.798494] time: 0:01:50.579290\n",
      "(30, 32, 32, 3)\n",
      "0.9297648\n",
      "[Epoch 2/5] [Batch 156/360] [D loss: 0.021624] [G loss: 7.505524] time: 0:01:50.692438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.83361745\n",
      "[Epoch 2/5] [Batch 157/360] [D loss: 0.027132] [G loss: 6.309551] time: 0:01:50.798217\n",
      "(30, 32, 32, 3)\n",
      "0.8667638\n",
      "[Epoch 2/5] [Batch 158/360] [D loss: 0.012506] [G loss: 6.520146] time: 0:01:50.910469\n",
      "(30, 32, 32, 3)\n",
      "0.8678775\n",
      "[Epoch 2/5] [Batch 159/360] [D loss: 0.032839] [G loss: 7.029385] time: 0:01:51.015975\n",
      "(30, 32, 32, 3)\n",
      "0.80588037\n",
      "[Epoch 2/5] [Batch 160/360] [D loss: 0.022443] [G loss: 6.793741] time: 0:01:51.122202\n",
      "(30, 32, 32, 3)\n",
      "0.8536703\n",
      "[Epoch 2/5] [Batch 161/360] [D loss: 0.017075] [G loss: 7.246088] time: 0:01:51.232042\n",
      "(30, 32, 32, 3)\n",
      "0.8497576\n",
      "[Epoch 2/5] [Batch 162/360] [D loss: 0.063168] [G loss: 6.924631] time: 0:01:51.340277\n",
      "(30, 32, 32, 3)\n",
      "0.8642275\n",
      "[Epoch 2/5] [Batch 163/360] [D loss: 0.015199] [G loss: 7.245667] time: 0:01:51.445517\n",
      "(30, 32, 32, 3)\n",
      "0.8953884\n",
      "[Epoch 2/5] [Batch 164/360] [D loss: 0.031084] [G loss: 6.792900] time: 0:01:51.551274\n",
      "(30, 32, 32, 3)\n",
      "0.89791805\n",
      "[Epoch 2/5] [Batch 165/360] [D loss: 0.047737] [G loss: 6.591959] time: 0:01:51.668002\n",
      "(30, 32, 32, 3)\n",
      "0.88314337\n",
      "[Epoch 2/5] [Batch 166/360] [D loss: 0.021712] [G loss: 6.364677] time: 0:01:51.773929\n",
      "(30, 32, 32, 3)\n",
      "0.8695636\n",
      "[Epoch 2/5] [Batch 167/360] [D loss: 0.019498] [G loss: 7.286676] time: 0:01:51.881400\n",
      "(30, 32, 32, 3)\n",
      "0.8954919\n",
      "[Epoch 2/5] [Batch 168/360] [D loss: 0.052699] [G loss: 6.376843] time: 0:01:51.990421\n",
      "(30, 32, 32, 3)\n",
      "0.8951378\n",
      "[Epoch 2/5] [Batch 169/360] [D loss: 0.010834] [G loss: 6.955861] time: 0:01:52.099954\n",
      "(30, 32, 32, 3)\n",
      "0.8985011\n",
      "[Epoch 2/5] [Batch 170/360] [D loss: 0.144950] [G loss: 7.309647] time: 0:01:52.208034\n",
      "(30, 32, 32, 3)\n",
      "0.85304683\n",
      "[Epoch 2/5] [Batch 171/360] [D loss: 0.570378] [G loss: 6.030781] time: 0:01:52.321205\n",
      "(30, 32, 32, 3)\n",
      "0.8809862\n",
      "[Epoch 2/5] [Batch 172/360] [D loss: 0.012232] [G loss: 6.956175] time: 0:01:52.425896\n",
      "(30, 32, 32, 3)\n",
      "0.82327455\n",
      "[Epoch 2/5] [Batch 173/360] [D loss: 0.280045] [G loss: 6.797059] time: 0:01:52.532483\n",
      "(30, 32, 32, 3)\n",
      "0.9188903\n",
      "[Epoch 2/5] [Batch 174/360] [D loss: 0.024874] [G loss: 6.618436] time: 0:01:52.638180\n",
      "(30, 32, 32, 3)\n",
      "0.8305419\n",
      "[Epoch 2/5] [Batch 175/360] [D loss: 0.101723] [G loss: 6.792685] time: 0:01:52.744298\n",
      "(30, 32, 32, 3)\n",
      "0.89208126\n",
      "[Epoch 2/5] [Batch 176/360] [D loss: 0.036412] [G loss: 6.240466] time: 0:01:52.849559\n",
      "(30, 32, 32, 3)\n",
      "0.90281314\n",
      "[Epoch 2/5] [Batch 177/360] [D loss: 0.030432] [G loss: 6.602083] time: 0:01:52.954555\n",
      "(30, 32, 32, 3)\n",
      "0.8712985\n",
      "[Epoch 2/5] [Batch 178/360] [D loss: 0.050511] [G loss: 7.090477] time: 0:01:53.060105\n",
      "(30, 32, 32, 3)\n",
      "0.8599367\n",
      "[Epoch 2/5] [Batch 179/360] [D loss: 0.026081] [G loss: 6.289244] time: 0:01:53.170589\n",
      "(30, 32, 32, 3)\n",
      "0.8355248\n",
      "[Epoch 2/5] [Batch 180/360] [D loss: 0.019722] [G loss: 6.951406] time: 0:01:53.273471\n",
      "(30, 32, 32, 3)\n",
      "0.8927317\n",
      "[Epoch 2/5] [Batch 181/360] [D loss: 0.022829] [G loss: 6.569579] time: 0:01:53.392002\n",
      "(30, 32, 32, 3)\n",
      "0.8494794\n",
      "[Epoch 2/5] [Batch 182/360] [D loss: 0.015010] [G loss: 6.347266] time: 0:01:53.495806\n",
      "(30, 32, 32, 3)\n",
      "0.8656042\n",
      "[Epoch 2/5] [Batch 183/360] [D loss: 0.039438] [G loss: 6.088755] time: 0:01:53.604660\n",
      "(30, 32, 32, 3)\n",
      "0.90686494\n",
      "[Epoch 2/5] [Batch 184/360] [D loss: 0.066875] [G loss: 6.006682] time: 0:01:53.707762\n",
      "(30, 32, 32, 3)\n",
      "0.86177206\n",
      "[Epoch 2/5] [Batch 185/360] [D loss: 0.126373] [G loss: 6.595512] time: 0:01:53.815167\n",
      "(30, 32, 32, 3)\n",
      "0.90355325\n",
      "[Epoch 2/5] [Batch 186/360] [D loss: 0.238763] [G loss: 7.046063] time: 0:01:53.919370\n",
      "(30, 32, 32, 3)\n",
      "0.85522056\n",
      "[Epoch 2/5] [Batch 187/360] [D loss: 0.313776] [G loss: 6.722021] time: 0:01:54.023748\n",
      "(30, 32, 32, 3)\n",
      "0.8411141\n",
      "[Epoch 2/5] [Batch 188/360] [D loss: 0.070688] [G loss: 6.191376] time: 0:01:54.130270\n",
      "(30, 32, 32, 3)\n",
      "0.82477355\n",
      "[Epoch 2/5] [Batch 189/360] [D loss: 0.023931] [G loss: 6.741782] time: 0:01:54.234314\n",
      "(30, 32, 32, 3)\n",
      "0.87303066\n",
      "[Epoch 2/5] [Batch 190/360] [D loss: 0.105495] [G loss: 6.383919] time: 0:01:54.342119\n",
      "(30, 32, 32, 3)\n",
      "0.87504953\n",
      "[Epoch 2/5] [Batch 191/360] [D loss: 0.043088] [G loss: 6.601007] time: 0:01:54.456359\n",
      "(30, 32, 32, 3)\n",
      "0.8467417\n",
      "[Epoch 2/5] [Batch 192/360] [D loss: 0.031092] [G loss: 5.934212] time: 0:01:54.562565\n",
      "(30, 32, 32, 3)\n",
      "0.854122\n",
      "[Epoch 2/5] [Batch 193/360] [D loss: 0.041425] [G loss: 6.276330] time: 0:01:54.668116\n",
      "(30, 32, 32, 3)\n",
      "0.8326929\n",
      "[Epoch 2/5] [Batch 194/360] [D loss: 0.021906] [G loss: 6.361323] time: 0:01:54.771015\n",
      "(30, 32, 32, 3)\n",
      "0.8584482\n",
      "[Epoch 2/5] [Batch 195/360] [D loss: 0.058310] [G loss: 7.144797] time: 0:01:54.875702\n",
      "(30, 32, 32, 3)\n",
      "0.9261496\n",
      "[Epoch 2/5] [Batch 196/360] [D loss: 0.193394] [G loss: 7.020361] time: 0:01:54.978190\n",
      "(30, 32, 32, 3)\n",
      "0.9184654\n",
      "[Epoch 2/5] [Batch 197/360] [D loss: 0.156536] [G loss: 6.133081] time: 0:01:55.086502\n",
      "(30, 32, 32, 3)\n",
      "0.8346254\n",
      "[Epoch 2/5] [Batch 198/360] [D loss: 0.059922] [G loss: 6.164658] time: 0:01:55.189941\n",
      "(30, 32, 32, 3)\n",
      "0.8485825\n",
      "[Epoch 2/5] [Batch 199/360] [D loss: 0.029527] [G loss: 6.454853] time: 0:01:55.295628\n",
      "(30, 32, 32, 3)\n",
      "0.93661904\n",
      "[Epoch 2/5] [Batch 200/360] [D loss: 0.029432] [G loss: 6.525414] time: 0:01:55.399994\n",
      "(30, 32, 32, 3)\n",
      "0.8669741\n",
      "[Epoch 2/5] [Batch 201/360] [D loss: 0.026411] [G loss: 6.595201] time: 0:01:55.521642\n",
      "(30, 32, 32, 3)\n",
      "0.86949754\n",
      "[Epoch 2/5] [Batch 202/360] [D loss: 0.102336] [G loss: 6.333018] time: 0:01:55.627949\n",
      "(30, 32, 32, 3)\n",
      "0.8699844\n",
      "[Epoch 2/5] [Batch 203/360] [D loss: 0.125420] [G loss: 6.370212] time: 0:01:55.737440\n",
      "(30, 32, 32, 3)\n",
      "0.8759981\n",
      "[Epoch 2/5] [Batch 204/360] [D loss: 0.077529] [G loss: 6.440381] time: 0:01:55.841198\n",
      "(30, 32, 32, 3)\n",
      "0.91586477\n",
      "[Epoch 2/5] [Batch 205/360] [D loss: 0.034377] [G loss: 6.329594] time: 0:01:55.948612\n",
      "(30, 32, 32, 3)\n",
      "0.88146496\n",
      "[Epoch 2/5] [Batch 206/360] [D loss: 0.023201] [G loss: 6.478244] time: 0:01:56.059318\n",
      "(30, 32, 32, 3)\n",
      "0.88167626\n",
      "[Epoch 2/5] [Batch 207/360] [D loss: 0.033089] [G loss: 6.575193] time: 0:01:56.172602\n",
      "(30, 32, 32, 3)\n",
      "0.8846434\n",
      "[Epoch 2/5] [Batch 208/360] [D loss: 0.047631] [G loss: 6.432588] time: 0:01:56.294012\n",
      "(30, 32, 32, 3)\n",
      "0.8815651\n",
      "[Epoch 2/5] [Batch 209/360] [D loss: 0.026301] [G loss: 5.789664] time: 0:01:56.400041\n",
      "(30, 32, 32, 3)\n",
      "0.8497898\n",
      "[Epoch 2/5] [Batch 210/360] [D loss: 0.025500] [G loss: 6.463337] time: 0:01:56.507115\n",
      "(30, 32, 32, 3)\n",
      "0.84712523\n",
      "[Epoch 2/5] [Batch 211/360] [D loss: 0.110060] [G loss: 6.177746] time: 0:01:56.616949\n",
      "(30, 32, 32, 3)\n",
      "0.8612024\n",
      "[Epoch 2/5] [Batch 212/360] [D loss: 0.049532] [G loss: 5.827836] time: 0:01:56.723100\n",
      "(30, 32, 32, 3)\n",
      "0.91029364\n",
      "[Epoch 2/5] [Batch 213/360] [D loss: 0.033846] [G loss: 6.483558] time: 0:01:56.830324\n",
      "(30, 32, 32, 3)\n",
      "0.86911446\n",
      "[Epoch 2/5] [Batch 214/360] [D loss: 0.258756] [G loss: 6.800275] time: 0:01:56.933230\n",
      "(30, 32, 32, 3)\n",
      "0.909225\n",
      "[Epoch 2/5] [Batch 215/360] [D loss: 0.322098] [G loss: 6.259824] time: 0:01:57.042938\n",
      "(30, 32, 32, 3)\n",
      "0.88821983\n",
      "[Epoch 2/5] [Batch 216/360] [D loss: 0.045876] [G loss: 6.150688] time: 0:01:57.150923\n",
      "(30, 32, 32, 3)\n",
      "0.8665971\n",
      "[Epoch 2/5] [Batch 217/360] [D loss: 0.025373] [G loss: 6.503100] time: 0:01:57.264492\n",
      "(30, 32, 32, 3)\n",
      "0.88725996\n",
      "[Epoch 2/5] [Batch 218/360] [D loss: 0.052771] [G loss: 6.414458] time: 0:01:57.372472\n",
      "(30, 32, 32, 3)\n",
      "0.8668\n",
      "[Epoch 2/5] [Batch 219/360] [D loss: 0.026205] [G loss: 6.133698] time: 0:01:57.480448\n",
      "(30, 32, 32, 3)\n",
      "0.9098265\n",
      "[Epoch 2/5] [Batch 220/360] [D loss: 0.266977] [G loss: 7.026923] time: 0:01:57.583707\n",
      "(30, 32, 32, 3)\n",
      "0.879621\n",
      "[Epoch 2/5] [Batch 221/360] [D loss: 0.523084] [G loss: 5.991623] time: 0:01:57.688751\n",
      "(30, 32, 32, 3)\n",
      "0.8632223\n",
      "[Epoch 2/5] [Batch 222/360] [D loss: 0.104469] [G loss: 5.792939] time: 0:01:57.794381\n",
      "(30, 32, 32, 3)\n",
      "0.8507063\n",
      "[Epoch 2/5] [Batch 223/360] [D loss: 0.089434] [G loss: 5.640065] time: 0:01:57.900546\n",
      "(30, 32, 32, 3)\n",
      "0.8986654\n",
      "[Epoch 2/5] [Batch 224/360] [D loss: 0.050776] [G loss: 5.824768] time: 0:01:58.007155\n",
      "(30, 32, 32, 3)\n",
      "0.87542653\n",
      "[Epoch 2/5] [Batch 225/360] [D loss: 0.057686] [G loss: 6.725199] time: 0:01:58.117606\n",
      "(30, 32, 32, 3)\n",
      "0.88118935\n",
      "[Epoch 2/5] [Batch 226/360] [D loss: 0.034090] [G loss: 6.072138] time: 0:01:58.220063\n",
      "(30, 32, 32, 3)\n",
      "0.8618848\n",
      "[Epoch 2/5] [Batch 227/360] [D loss: 0.018634] [G loss: 5.895115] time: 0:01:58.327651\n",
      "(30, 32, 32, 3)\n",
      "0.86546534\n",
      "[Epoch 2/5] [Batch 228/360] [D loss: 0.018281] [G loss: 6.178064] time: 0:01:58.435669\n",
      "(30, 32, 32, 3)\n",
      "0.8828352\n",
      "[Epoch 2/5] [Batch 229/360] [D loss: 0.053088] [G loss: 5.519410] time: 0:01:58.545867\n",
      "(30, 32, 32, 3)\n",
      "0.87950677\n",
      "[Epoch 2/5] [Batch 230/360] [D loss: 0.035329] [G loss: 6.340031] time: 0:01:58.650270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9037719\n",
      "[Epoch 2/5] [Batch 231/360] [D loss: 0.040660] [G loss: 5.929643] time: 0:01:58.757805\n",
      "(30, 32, 32, 3)\n",
      "0.8659854\n",
      "[Epoch 2/5] [Batch 232/360] [D loss: 0.081406] [G loss: 6.253323] time: 0:01:58.862321\n",
      "(30, 32, 32, 3)\n",
      "0.8844023\n",
      "[Epoch 2/5] [Batch 233/360] [D loss: 0.057919] [G loss: 5.527857] time: 0:01:58.971174\n",
      "(30, 32, 32, 3)\n",
      "0.859376\n",
      "[Epoch 2/5] [Batch 234/360] [D loss: 0.024937] [G loss: 5.937115] time: 0:01:59.076227\n",
      "(30, 32, 32, 3)\n",
      "0.8733793\n",
      "[Epoch 2/5] [Batch 235/360] [D loss: 0.075445] [G loss: 6.371192] time: 0:01:59.180670\n",
      "(30, 32, 32, 3)\n",
      "0.91801375\n",
      "[Epoch 2/5] [Batch 236/360] [D loss: 0.132125] [G loss: 6.038688] time: 0:01:59.289313\n",
      "(30, 32, 32, 3)\n",
      "0.86697847\n",
      "[Epoch 2/5] [Batch 237/360] [D loss: 0.102124] [G loss: 6.502769] time: 0:01:59.398812\n",
      "(30, 32, 32, 3)\n",
      "0.86711925\n",
      "[Epoch 2/5] [Batch 238/360] [D loss: 0.053832] [G loss: 5.569645] time: 0:01:59.502822\n",
      "(30, 32, 32, 3)\n",
      "0.8524539\n",
      "[Epoch 2/5] [Batch 239/360] [D loss: 0.018948] [G loss: 6.044348] time: 0:01:59.607824\n",
      "(30, 32, 32, 3)\n",
      "0.9191503\n",
      "[Epoch 2/5] [Batch 240/360] [D loss: 0.041691] [G loss: 7.024674] time: 0:01:59.716992\n",
      "(30, 32, 32, 3)\n",
      "0.94751614\n",
      "[Epoch 2/5] [Batch 241/360] [D loss: 0.047376] [G loss: 6.049063] time: 0:01:59.822313\n",
      "(30, 32, 32, 3)\n",
      "0.81938916\n",
      "[Epoch 2/5] [Batch 242/360] [D loss: 0.020677] [G loss: 5.986745] time: 0:02:00.012357\n",
      "(30, 32, 32, 3)\n",
      "0.84648633\n",
      "[Epoch 2/5] [Batch 243/360] [D loss: 0.179683] [G loss: 6.346749] time: 0:02:00.121776\n",
      "(30, 32, 32, 3)\n",
      "0.8685129\n",
      "[Epoch 2/5] [Batch 244/360] [D loss: 0.477131] [G loss: 5.848848] time: 0:02:00.230489\n",
      "(30, 32, 32, 3)\n",
      "0.8394359\n",
      "[Epoch 2/5] [Batch 245/360] [D loss: 0.084829] [G loss: 6.204247] time: 0:02:00.340784\n",
      "(30, 32, 32, 3)\n",
      "0.91214687\n",
      "[Epoch 2/5] [Batch 246/360] [D loss: 0.079766] [G loss: 6.123759] time: 0:02:00.454338\n",
      "(30, 32, 32, 3)\n",
      "0.8183338\n",
      "[Epoch 2/5] [Batch 247/360] [D loss: 0.029046] [G loss: 6.334764] time: 0:02:00.562115\n",
      "(30, 32, 32, 3)\n",
      "0.921182\n",
      "[Epoch 2/5] [Batch 248/360] [D loss: 0.028750] [G loss: 6.023028] time: 0:02:00.671017\n",
      "(30, 32, 32, 3)\n",
      "0.8503509\n",
      "[Epoch 2/5] [Batch 249/360] [D loss: 0.028358] [G loss: 6.317344] time: 0:02:00.779100\n",
      "(30, 32, 32, 3)\n",
      "0.883872\n",
      "[Epoch 2/5] [Batch 250/360] [D loss: 0.033698] [G loss: 6.290509] time: 0:02:00.888123\n",
      "(30, 32, 32, 3)\n",
      "0.89448357\n",
      "[Epoch 2/5] [Batch 251/360] [D loss: 0.035820] [G loss: 5.976247] time: 0:02:00.999250\n",
      "(30, 32, 32, 3)\n",
      "0.90882015\n",
      "[Epoch 2/5] [Batch 252/360] [D loss: 0.021288] [G loss: 5.488892] time: 0:02:01.103630\n",
      "(30, 32, 32, 3)\n",
      "0.95382816\n",
      "[Epoch 2/5] [Batch 253/360] [D loss: 0.098315] [G loss: 6.107832] time: 0:02:01.207786\n",
      "(30, 32, 32, 3)\n",
      "0.92521495\n",
      "[Epoch 2/5] [Batch 254/360] [D loss: 0.349713] [G loss: 6.587697] time: 0:02:01.312738\n",
      "(30, 32, 32, 3)\n",
      "0.8918004\n",
      "[Epoch 2/5] [Batch 255/360] [D loss: 0.280786] [G loss: 6.397411] time: 0:02:01.426572\n",
      "(30, 32, 32, 3)\n",
      "0.8856283\n",
      "[Epoch 2/5] [Batch 256/360] [D loss: 0.032620] [G loss: 6.318149] time: 0:02:01.537857\n",
      "(30, 32, 32, 3)\n",
      "0.9046081\n",
      "[Epoch 2/5] [Batch 257/360] [D loss: 0.058448] [G loss: 6.052617] time: 0:02:01.654953\n",
      "(30, 32, 32, 3)\n",
      "0.8869955\n",
      "[Epoch 2/5] [Batch 258/360] [D loss: 0.016893] [G loss: 5.792158] time: 0:02:01.766451\n",
      "(30, 32, 32, 3)\n",
      "0.89172584\n",
      "[Epoch 2/5] [Batch 259/360] [D loss: 0.028047] [G loss: 5.390403] time: 0:02:01.874094\n",
      "(30, 32, 32, 3)\n",
      "0.8545547\n",
      "[Epoch 2/5] [Batch 260/360] [D loss: 0.034796] [G loss: 5.975331] time: 0:02:01.987437\n",
      "(30, 32, 32, 3)\n",
      "0.8810137\n",
      "[Epoch 2/5] [Batch 261/360] [D loss: 0.084523] [G loss: 5.731803] time: 0:02:02.098368\n",
      "(30, 32, 32, 3)\n",
      "0.8799251\n",
      "[Epoch 2/5] [Batch 262/360] [D loss: 0.058346] [G loss: 5.224215] time: 0:02:02.206992\n",
      "(30, 32, 32, 3)\n",
      "0.8270788\n",
      "[Epoch 2/5] [Batch 263/360] [D loss: 0.038470] [G loss: 6.723471] time: 0:02:02.313377\n",
      "(30, 32, 32, 3)\n",
      "0.90653557\n",
      "[Epoch 2/5] [Batch 264/360] [D loss: 0.103116] [G loss: 5.843201] time: 0:02:02.424283\n",
      "(30, 32, 32, 3)\n",
      "0.91982174\n",
      "[Epoch 2/5] [Batch 265/360] [D loss: 0.017054] [G loss: 5.982265] time: 0:02:02.528378\n",
      "(30, 32, 32, 3)\n",
      "0.8789661\n",
      "[Epoch 2/5] [Batch 266/360] [D loss: 0.091274] [G loss: 5.944655] time: 0:02:02.635229\n",
      "(30, 32, 32, 3)\n",
      "0.8735954\n",
      "[Epoch 2/5] [Batch 267/360] [D loss: 0.118541] [G loss: 5.538995] time: 0:02:02.738962\n",
      "(30, 32, 32, 3)\n",
      "0.9161853\n",
      "[Epoch 2/5] [Batch 268/360] [D loss: 0.026018] [G loss: 5.967174] time: 0:02:02.847572\n",
      "(30, 32, 32, 3)\n",
      "0.86369467\n",
      "[Epoch 2/5] [Batch 269/360] [D loss: 0.106473] [G loss: 5.948866] time: 0:02:02.957649\n",
      "(30, 32, 32, 3)\n",
      "0.88026905\n",
      "[Epoch 2/5] [Batch 270/360] [D loss: 0.116198] [G loss: 6.143311] time: 0:02:03.066303\n",
      "(30, 32, 32, 3)\n",
      "0.88006467\n",
      "[Epoch 2/5] [Batch 271/360] [D loss: 0.119390] [G loss: 6.206208] time: 0:02:03.174409\n",
      "(30, 32, 32, 3)\n",
      "0.8553478\n",
      "[Epoch 2/5] [Batch 272/360] [D loss: 0.059509] [G loss: 5.260273] time: 0:02:03.295392\n",
      "(30, 32, 32, 3)\n",
      "0.8823204\n",
      "[Epoch 2/5] [Batch 273/360] [D loss: 0.017921] [G loss: 6.076840] time: 0:02:03.412085\n",
      "(30, 32, 32, 3)\n",
      "0.866456\n",
      "[Epoch 2/5] [Batch 274/360] [D loss: 0.045470] [G loss: 5.325084] time: 0:02:03.519622\n",
      "(30, 32, 32, 3)\n",
      "0.8775735\n",
      "[Epoch 2/5] [Batch 275/360] [D loss: 0.098744] [G loss: 6.883021] time: 0:02:03.628246\n",
      "(30, 32, 32, 3)\n",
      "0.87696713\n",
      "[Epoch 2/5] [Batch 276/360] [D loss: 0.501491] [G loss: 5.974073] time: 0:02:03.739587\n",
      "(30, 32, 32, 3)\n",
      "0.8667741\n",
      "[Epoch 2/5] [Batch 277/360] [D loss: 0.028839] [G loss: 6.028306] time: 0:02:03.848900\n",
      "(30, 32, 32, 3)\n",
      "0.86351675\n",
      "[Epoch 2/5] [Batch 278/360] [D loss: 0.258457] [G loss: 5.571825] time: 0:02:03.960558\n",
      "(30, 32, 32, 3)\n",
      "0.90268564\n",
      "[Epoch 2/5] [Batch 279/360] [D loss: 0.054636] [G loss: 5.244878] time: 0:02:04.067316\n",
      "(30, 32, 32, 3)\n",
      "0.8787344\n",
      "[Epoch 2/5] [Batch 280/360] [D loss: 0.073264] [G loss: 6.020049] time: 0:02:04.173872\n",
      "(30, 32, 32, 3)\n",
      "0.8707759\n",
      "[Epoch 2/5] [Batch 281/360] [D loss: 0.107055] [G loss: 5.344005] time: 0:02:04.283309\n",
      "(30, 32, 32, 3)\n",
      "0.90099794\n",
      "[Epoch 2/5] [Batch 282/360] [D loss: 0.027316] [G loss: 5.967638] time: 0:02:04.400048\n",
      "(30, 32, 32, 3)\n",
      "0.89713174\n",
      "[Epoch 2/5] [Batch 283/360] [D loss: 0.056183] [G loss: 5.646866] time: 0:02:04.512901\n",
      "(30, 32, 32, 3)\n",
      "0.8562911\n",
      "[Epoch 2/5] [Batch 284/360] [D loss: 0.044253] [G loss: 5.621089] time: 0:02:04.620527\n",
      "(30, 32, 32, 3)\n",
      "0.892773\n",
      "[Epoch 2/5] [Batch 285/360] [D loss: 0.047355] [G loss: 6.144863] time: 0:02:04.729005\n",
      "(30, 32, 32, 3)\n",
      "0.88595754\n",
      "[Epoch 2/5] [Batch 286/360] [D loss: 0.083184] [G loss: 5.821758] time: 0:02:04.837807\n",
      "(30, 32, 32, 3)\n",
      "0.91457206\n",
      "[Epoch 2/5] [Batch 287/360] [D loss: 0.091237] [G loss: 5.658620] time: 0:02:04.947589\n",
      "(30, 32, 32, 3)\n",
      "0.91416997\n",
      "[Epoch 2/5] [Batch 288/360] [D loss: 0.092286] [G loss: 6.270540] time: 0:02:05.058629\n",
      "(30, 32, 32, 3)\n",
      "0.87200737\n",
      "[Epoch 2/5] [Batch 289/360] [D loss: 0.062598] [G loss: 6.491856] time: 0:02:05.168963\n",
      "(30, 32, 32, 3)\n",
      "0.8902385\n",
      "[Epoch 2/5] [Batch 290/360] [D loss: 0.019760] [G loss: 5.726181] time: 0:02:05.275292\n",
      "(30, 32, 32, 3)\n",
      "0.9177456\n",
      "[Epoch 2/5] [Batch 291/360] [D loss: 0.035820] [G loss: 6.206762] time: 0:02:05.389422\n",
      "(30, 32, 32, 3)\n",
      "0.889235\n",
      "[Epoch 2/5] [Batch 292/360] [D loss: 0.124111] [G loss: 6.039357] time: 0:02:05.497282\n",
      "(30, 32, 32, 3)\n",
      "0.90668625\n",
      "[Epoch 2/5] [Batch 293/360] [D loss: 0.102212] [G loss: 5.981458] time: 0:02:05.607801\n",
      "(30, 32, 32, 3)\n",
      "0.9148852\n",
      "[Epoch 2/5] [Batch 294/360] [D loss: 0.052578] [G loss: 6.222422] time: 0:02:05.714274\n",
      "(30, 32, 32, 3)\n",
      "0.8748989\n",
      "[Epoch 2/5] [Batch 295/360] [D loss: 0.049944] [G loss: 5.707909] time: 0:02:05.820239\n",
      "(30, 32, 32, 3)\n",
      "0.8346679\n",
      "[Epoch 2/5] [Batch 296/360] [D loss: 0.023547] [G loss: 5.496449] time: 0:02:05.929235\n",
      "(30, 32, 32, 3)\n",
      "0.8723738\n",
      "[Epoch 2/5] [Batch 297/360] [D loss: 0.093483] [G loss: 5.619120] time: 0:02:06.035399\n",
      "(30, 32, 32, 3)\n",
      "0.91275376\n",
      "[Epoch 2/5] [Batch 298/360] [D loss: 0.293460] [G loss: 5.437009] time: 0:02:06.141383\n",
      "(30, 32, 32, 3)\n",
      "0.9016943\n",
      "[Epoch 2/5] [Batch 299/360] [D loss: 0.154914] [G loss: 5.512301] time: 0:02:06.248114\n",
      "(30, 32, 32, 3)\n",
      "0.8549842\n",
      "[Epoch 2/5] [Batch 300/360] [D loss: 0.029975] [G loss: 5.601059] time: 0:02:06.358630\n",
      "(30, 32, 32, 3)\n",
      "0.93549556\n",
      "[Epoch 2/5] [Batch 301/360] [D loss: 0.033181] [G loss: 5.182945] time: 0:02:06.464200\n",
      "(30, 32, 32, 3)\n",
      "0.816931\n",
      "[Epoch 2/5] [Batch 302/360] [D loss: 0.031908] [G loss: 5.656274] time: 0:02:06.570387\n",
      "(30, 32, 32, 3)\n",
      "0.87729305\n",
      "[Epoch 2/5] [Batch 303/360] [D loss: 0.078175] [G loss: 5.694484] time: 0:02:06.681870\n",
      "(30, 32, 32, 3)\n",
      "0.86830753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/5] [Batch 304/360] [D loss: 0.035219] [G loss: 5.564858] time: 0:02:06.791043\n",
      "(30, 32, 32, 3)\n",
      "0.8503998\n",
      "[Epoch 2/5] [Batch 305/360] [D loss: 0.050776] [G loss: 5.688139] time: 0:02:06.909411\n",
      "(30, 32, 32, 3)\n",
      "0.84297013\n",
      "[Epoch 2/5] [Batch 306/360] [D loss: 0.029719] [G loss: 5.083466] time: 0:02:07.022315\n",
      "(30, 32, 32, 3)\n",
      "0.87646914\n",
      "[Epoch 2/5] [Batch 307/360] [D loss: 0.022567] [G loss: 5.872311] time: 0:02:07.134367\n",
      "(30, 32, 32, 3)\n",
      "0.8577474\n",
      "[Epoch 2/5] [Batch 308/360] [D loss: 0.094117] [G loss: 5.555314] time: 0:02:07.251253\n",
      "(30, 32, 32, 3)\n",
      "0.92332715\n",
      "[Epoch 2/5] [Batch 309/360] [D loss: 0.187872] [G loss: 6.038234] time: 0:02:07.367511\n",
      "(30, 32, 32, 3)\n",
      "0.8958563\n",
      "[Epoch 2/5] [Batch 310/360] [D loss: 0.384290] [G loss: 5.199604] time: 0:02:07.480406\n",
      "(30, 32, 32, 3)\n",
      "0.86765593\n",
      "[Epoch 2/5] [Batch 311/360] [D loss: 0.058049] [G loss: 4.954725] time: 0:02:07.594673\n",
      "(30, 32, 32, 3)\n",
      "0.91917354\n",
      "[Epoch 2/5] [Batch 312/360] [D loss: 0.053649] [G loss: 5.864861] time: 0:02:07.708619\n",
      "(30, 32, 32, 3)\n",
      "0.8701194\n",
      "[Epoch 2/5] [Batch 313/360] [D loss: 0.322002] [G loss: 5.401496] time: 0:02:07.820754\n",
      "(30, 32, 32, 3)\n",
      "0.8825233\n",
      "[Epoch 2/5] [Batch 314/360] [D loss: 0.076036] [G loss: 5.644825] time: 0:02:07.946563\n",
      "(30, 32, 32, 3)\n",
      "0.8652374\n",
      "[Epoch 2/5] [Batch 315/360] [D loss: 0.038968] [G loss: 5.356348] time: 0:02:08.057566\n",
      "(30, 32, 32, 3)\n",
      "0.91361624\n",
      "[Epoch 2/5] [Batch 316/360] [D loss: 0.026429] [G loss: 5.418777] time: 0:02:08.167684\n",
      "(30, 32, 32, 3)\n",
      "0.9056359\n",
      "[Epoch 2/5] [Batch 317/360] [D loss: 0.033746] [G loss: 5.370920] time: 0:02:08.285431\n",
      "(30, 32, 32, 3)\n",
      "0.86037546\n",
      "[Epoch 2/5] [Batch 318/360] [D loss: 0.094044] [G loss: 5.251762] time: 0:02:08.395125\n",
      "(30, 32, 32, 3)\n",
      "0.87809676\n",
      "[Epoch 2/5] [Batch 319/360] [D loss: 0.081326] [G loss: 5.645670] time: 0:02:08.509444\n",
      "(30, 32, 32, 3)\n",
      "0.9092005\n",
      "[Epoch 2/5] [Batch 320/360] [D loss: 0.135020] [G loss: 6.005356] time: 0:02:08.620180\n",
      "(30, 32, 32, 3)\n",
      "0.8777278\n",
      "[Epoch 2/5] [Batch 321/360] [D loss: 0.222487] [G loss: 5.859910] time: 0:02:08.728108\n",
      "(30, 32, 32, 3)\n",
      "0.90414256\n",
      "[Epoch 2/5] [Batch 322/360] [D loss: 0.038327] [G loss: 5.531294] time: 0:02:08.835321\n",
      "(30, 32, 32, 3)\n",
      "0.88015777\n",
      "[Epoch 2/5] [Batch 323/360] [D loss: 0.035447] [G loss: 5.347254] time: 0:02:08.943085\n",
      "(30, 32, 32, 3)\n",
      "0.91276264\n",
      "[Epoch 2/5] [Batch 324/360] [D loss: 0.052128] [G loss: 5.299835] time: 0:02:09.051468\n",
      "(30, 32, 32, 3)\n",
      "0.8608198\n",
      "[Epoch 2/5] [Batch 325/360] [D loss: 0.079011] [G loss: 5.768745] time: 0:02:09.153937\n",
      "(30, 32, 32, 3)\n",
      "0.86639285\n",
      "[Epoch 2/5] [Batch 326/360] [D loss: 0.203637] [G loss: 5.830551] time: 0:02:09.256952\n",
      "(30, 32, 32, 3)\n",
      "0.8352379\n",
      "[Epoch 2/5] [Batch 327/360] [D loss: 0.239266] [G loss: 5.716954] time: 0:02:09.367660\n",
      "(30, 32, 32, 3)\n",
      "0.91651696\n",
      "[Epoch 2/5] [Batch 328/360] [D loss: 0.059370] [G loss: 4.910791] time: 0:02:09.471632\n",
      "(30, 32, 32, 3)\n",
      "0.8937721\n",
      "[Epoch 2/5] [Batch 329/360] [D loss: 0.043209] [G loss: 5.786257] time: 0:02:09.573941\n",
      "(30, 32, 32, 3)\n",
      "0.8764195\n",
      "[Epoch 2/5] [Batch 330/360] [D loss: 0.243858] [G loss: 5.927694] time: 0:02:09.675924\n",
      "(30, 32, 32, 3)\n",
      "0.9150832\n",
      "[Epoch 2/5] [Batch 331/360] [D loss: 0.110278] [G loss: 5.208495] time: 0:02:09.778831\n",
      "(30, 32, 32, 3)\n",
      "0.88092595\n",
      "[Epoch 2/5] [Batch 332/360] [D loss: 0.039874] [G loss: 5.580108] time: 0:02:09.887705\n",
      "(30, 32, 32, 3)\n",
      "0.8503618\n",
      "[Epoch 2/5] [Batch 333/360] [D loss: 0.119664] [G loss: 6.153104] time: 0:02:09.992981\n",
      "(30, 32, 32, 3)\n",
      "0.92149335\n",
      "[Epoch 2/5] [Batch 334/360] [D loss: 0.149217] [G loss: 5.274908] time: 0:02:10.102038\n",
      "(30, 32, 32, 3)\n",
      "0.8974786\n",
      "[Epoch 2/5] [Batch 335/360] [D loss: 0.106055] [G loss: 5.188977] time: 0:02:10.207283\n",
      "(30, 32, 32, 3)\n",
      "0.9099827\n",
      "[Epoch 2/5] [Batch 336/360] [D loss: 0.170400] [G loss: 5.909557] time: 0:02:10.313574\n",
      "(30, 32, 32, 3)\n",
      "0.91343254\n",
      "[Epoch 2/5] [Batch 337/360] [D loss: 0.479476] [G loss: 5.043353] time: 0:02:10.424127\n",
      "(30, 32, 32, 3)\n",
      "0.88856393\n",
      "[Epoch 2/5] [Batch 338/360] [D loss: 0.044102] [G loss: 5.234045] time: 0:02:10.527897\n",
      "(30, 32, 32, 3)\n",
      "0.87975067\n",
      "[Epoch 2/5] [Batch 339/360] [D loss: 0.267811] [G loss: 5.098640] time: 0:02:10.634958\n",
      "(30, 32, 32, 3)\n",
      "0.9087265\n",
      "[Epoch 2/5] [Batch 340/360] [D loss: 0.078352] [G loss: 5.033654] time: 0:02:10.739676\n",
      "(30, 32, 32, 3)\n",
      "0.8787894\n",
      "[Epoch 2/5] [Batch 341/360] [D loss: 0.089442] [G loss: 5.640777] time: 0:02:10.846220\n",
      "(30, 32, 32, 3)\n",
      "0.9107148\n",
      "[Epoch 2/5] [Batch 342/360] [D loss: 0.124859] [G loss: 5.009709] time: 0:02:10.953741\n",
      "(30, 32, 32, 3)\n",
      "0.9270113\n",
      "[Epoch 2/5] [Batch 343/360] [D loss: 0.066407] [G loss: 5.814495] time: 0:02:11.058766\n",
      "(30, 32, 32, 3)\n",
      "0.8729773\n",
      "[Epoch 2/5] [Batch 344/360] [D loss: 0.122256] [G loss: 5.655776] time: 0:02:11.164963\n",
      "(30, 32, 32, 3)\n",
      "0.95296717\n",
      "[Epoch 2/5] [Batch 345/360] [D loss: 0.060465] [G loss: 5.152431] time: 0:02:11.282611\n",
      "(30, 32, 32, 3)\n",
      "0.88605404\n",
      "[Epoch 2/5] [Batch 346/360] [D loss: 0.037652] [G loss: 5.333564] time: 0:02:11.407107\n",
      "(30, 32, 32, 3)\n",
      "0.85628766\n",
      "[Epoch 2/5] [Batch 347/360] [D loss: 0.163293] [G loss: 5.237270] time: 0:02:11.515796\n",
      "(30, 32, 32, 3)\n",
      "0.920059\n",
      "[Epoch 2/5] [Batch 348/360] [D loss: 0.381183] [G loss: 5.869112] time: 0:02:11.626866\n",
      "(30, 32, 32, 3)\n",
      "0.88067645\n",
      "[Epoch 2/5] [Batch 349/360] [D loss: 0.111990] [G loss: 5.159976] time: 0:02:11.736819\n",
      "(30, 32, 32, 3)\n",
      "0.84305304\n",
      "[Epoch 2/5] [Batch 350/360] [D loss: 0.047682] [G loss: 5.074965] time: 0:02:11.850702\n",
      "(30, 32, 32, 3)\n",
      "0.8867542\n",
      "[Epoch 2/5] [Batch 351/360] [D loss: 0.092087] [G loss: 5.505214] time: 0:02:11.963821\n",
      "(30, 32, 32, 3)\n",
      "0.79631376\n",
      "[Epoch 2/5] [Batch 352/360] [D loss: 0.116656] [G loss: 5.383410] time: 0:02:12.073656\n",
      "(30, 32, 32, 3)\n",
      "0.90705514\n",
      "[Epoch 2/5] [Batch 353/360] [D loss: 0.196451] [G loss: 5.368925] time: 0:02:12.186733\n",
      "(30, 32, 32, 3)\n",
      "0.91376734\n",
      "[Epoch 2/5] [Batch 354/360] [D loss: 0.204162] [G loss: 5.412941] time: 0:02:12.296448\n",
      "(30, 32, 32, 3)\n",
      "0.8937304\n",
      "[Epoch 2/5] [Batch 355/360] [D loss: 0.085170] [G loss: 4.798981] time: 0:02:12.404029\n",
      "(30, 32, 32, 3)\n",
      "0.92700547\n",
      "[Epoch 2/5] [Batch 356/360] [D loss: 0.050642] [G loss: 5.452254] time: 0:02:12.509891\n",
      "(30, 32, 32, 3)\n",
      "0.9054013\n",
      "[Epoch 2/5] [Batch 357/360] [D loss: 0.054738] [G loss: 4.689461] time: 0:02:12.618400\n",
      "(30, 32, 32, 3)\n",
      "0.9159542\n",
      "[Epoch 2/5] [Batch 358/360] [D loss: 0.032590] [G loss: 5.251600] time: 0:02:12.728151\n",
      "(30, 32, 32, 3)\n",
      "0.8802748\n",
      "[Epoch 2/5] [Batch 359/360] [D loss: 0.095705] [G loss: 5.061430] time: 0:02:12.837623\n",
      "(30, 32, 32, 3)\n",
      "0.8827336\n",
      "[Epoch 3/5] [Batch 1/360] [D loss: 0.108463] [G loss: 5.412283] time: 0:02:12.957310\n",
      "(30, 32, 32, 3)\n",
      "0.84741896\n",
      "[Epoch 3/5] [Batch 2/360] [D loss: 0.327680] [G loss: 5.237975] time: 0:02:13.069944\n",
      "(30, 32, 32, 3)\n",
      "0.93934774\n",
      "[Epoch 3/5] [Batch 3/360] [D loss: 0.100480] [G loss: 5.016615] time: 0:02:13.186283\n",
      "(30, 32, 32, 3)\n",
      "0.9063286\n",
      "[Epoch 3/5] [Batch 4/360] [D loss: 0.050763] [G loss: 5.274345] time: 0:02:13.296839\n",
      "(30, 32, 32, 3)\n",
      "0.89903337\n",
      "[Epoch 3/5] [Batch 5/360] [D loss: 0.199320] [G loss: 5.343625] time: 0:02:13.410441\n",
      "(30, 32, 32, 3)\n",
      "0.8787829\n",
      "[Epoch 3/5] [Batch 6/360] [D loss: 0.088731] [G loss: 5.175649] time: 0:02:13.526669\n",
      "(30, 32, 32, 3)\n",
      "0.8509378\n",
      "[Epoch 3/5] [Batch 7/360] [D loss: 0.067663] [G loss: 5.215181] time: 0:02:13.652352\n",
      "(30, 32, 32, 3)\n",
      "0.89415663\n",
      "[Epoch 3/5] [Batch 8/360] [D loss: 0.076224] [G loss: 5.021109] time: 0:02:13.762915\n",
      "(30, 32, 32, 3)\n",
      "0.8840825\n",
      "[Epoch 3/5] [Batch 9/360] [D loss: 0.071299] [G loss: 4.629870] time: 0:02:13.876275\n",
      "(30, 32, 32, 3)\n",
      "0.8755824\n",
      "[Epoch 3/5] [Batch 10/360] [D loss: 0.060960] [G loss: 5.912352] time: 0:02:13.986076\n",
      "(30, 32, 32, 3)\n",
      "0.9060214\n",
      "[Epoch 3/5] [Batch 11/360] [D loss: 0.157394] [G loss: 4.762830] time: 0:02:14.099902\n",
      "(30, 32, 32, 3)\n",
      "0.95330304\n",
      "[Epoch 3/5] [Batch 12/360] [D loss: 0.027751] [G loss: 5.345774] time: 0:02:14.216132\n",
      "(30, 32, 32, 3)\n",
      "0.8928695\n",
      "[Epoch 3/5] [Batch 13/360] [D loss: 0.258047] [G loss: 5.265487] time: 0:02:14.336959\n",
      "(30, 32, 32, 3)\n",
      "0.83330935\n",
      "[Epoch 3/5] [Batch 14/360] [D loss: 0.210934] [G loss: 5.106328] time: 0:02:14.448681\n",
      "(30, 32, 32, 3)\n",
      "0.88306135\n",
      "[Epoch 3/5] [Batch 15/360] [D loss: 0.146627] [G loss: 5.262223] time: 0:02:14.560784\n",
      "(30, 32, 32, 3)\n",
      "0.86079\n",
      "[Epoch 3/5] [Batch 16/360] [D loss: 0.060526] [G loss: 5.304596] time: 0:02:14.669429\n",
      "(30, 32, 32, 3)\n",
      "0.8731932\n",
      "[Epoch 3/5] [Batch 17/360] [D loss: 0.039007] [G loss: 5.215264] time: 0:02:14.778807\n",
      "(30, 32, 32, 3)\n",
      "0.92078274\n",
      "[Epoch 3/5] [Batch 18/360] [D loss: 0.056733] [G loss: 4.811244] time: 0:02:14.890855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9518347\n",
      "[Epoch 3/5] [Batch 19/360] [D loss: 0.163361] [G loss: 5.160936] time: 0:02:15.003826\n",
      "(30, 32, 32, 3)\n",
      "0.906882\n",
      "[Epoch 3/5] [Batch 20/360] [D loss: 0.455916] [G loss: 4.943287] time: 0:02:15.108801\n",
      "(30, 32, 32, 3)\n",
      "0.8833211\n",
      "[Epoch 3/5] [Batch 21/360] [D loss: 0.035514] [G loss: 4.985945] time: 0:02:15.214643\n",
      "(30, 32, 32, 3)\n",
      "0.89034224\n",
      "[Epoch 3/5] [Batch 22/360] [D loss: 0.162351] [G loss: 5.367644] time: 0:02:15.337819\n",
      "(30, 32, 32, 3)\n",
      "0.8899889\n",
      "[Epoch 3/5] [Batch 23/360] [D loss: 0.092631] [G loss: 5.110550] time: 0:02:15.446393\n",
      "(30, 32, 32, 3)\n",
      "0.82531375\n",
      "[Epoch 3/5] [Batch 24/360] [D loss: 0.053472] [G loss: 4.845819] time: 0:02:15.552319\n",
      "(30, 32, 32, 3)\n",
      "0.8947099\n",
      "[Epoch 3/5] [Batch 25/360] [D loss: 0.094289] [G loss: 5.609624] time: 0:02:15.656672\n",
      "(30, 32, 32, 3)\n",
      "0.88503104\n",
      "[Epoch 3/5] [Batch 26/360] [D loss: 0.335620] [G loss: 5.320622] time: 0:02:15.760301\n",
      "(30, 32, 32, 3)\n",
      "0.89899635\n",
      "[Epoch 3/5] [Batch 27/360] [D loss: 0.156056] [G loss: 5.320042] time: 0:02:15.869817\n",
      "(30, 32, 32, 3)\n",
      "0.90896064\n",
      "[Epoch 3/5] [Batch 28/360] [D loss: 0.064193] [G loss: 4.686250] time: 0:02:15.974728\n",
      "(30, 32, 32, 3)\n",
      "0.9347884\n",
      "[Epoch 3/5] [Batch 29/360] [D loss: 0.039978] [G loss: 5.372268] time: 0:02:16.079552\n",
      "(30, 32, 32, 3)\n",
      "0.87380034\n",
      "[Epoch 3/5] [Batch 30/360] [D loss: 0.159861] [G loss: 4.980634] time: 0:02:16.190224\n",
      "(30, 32, 32, 3)\n",
      "0.86204046\n",
      "[Epoch 3/5] [Batch 31/360] [D loss: 0.097695] [G loss: 5.043731] time: 0:02:16.301976\n",
      "(30, 32, 32, 3)\n",
      "0.8697822\n",
      "[Epoch 3/5] [Batch 32/360] [D loss: 0.105312] [G loss: 5.079027] time: 0:02:16.407424\n",
      "(30, 32, 32, 3)\n",
      "0.89511734\n",
      "[Epoch 3/5] [Batch 33/360] [D loss: 0.151753] [G loss: 4.922171] time: 0:02:16.519788\n",
      "(30, 32, 32, 3)\n",
      "0.8503458\n",
      "[Epoch 3/5] [Batch 34/360] [D loss: 0.113830] [G loss: 4.886860] time: 0:02:16.628319\n",
      "(30, 32, 32, 3)\n",
      "0.8654006\n",
      "[Epoch 3/5] [Batch 35/360] [D loss: 0.066615] [G loss: 4.738297] time: 0:02:16.741944\n",
      "(30, 32, 32, 3)\n",
      "0.90098304\n",
      "[Epoch 3/5] [Batch 36/360] [D loss: 0.108270] [G loss: 5.355375] time: 0:02:16.849432\n",
      "(30, 32, 32, 3)\n",
      "0.86670905\n",
      "[Epoch 3/5] [Batch 37/360] [D loss: 0.322892] [G loss: 4.714462] time: 0:02:16.957456\n",
      "(30, 32, 32, 3)\n",
      "0.8950049\n",
      "[Epoch 3/5] [Batch 38/360] [D loss: 0.041718] [G loss: 5.239848] time: 0:02:17.060582\n",
      "(30, 32, 32, 3)\n",
      "0.8977428\n",
      "[Epoch 3/5] [Batch 39/360] [D loss: 0.105561] [G loss: 5.077960] time: 0:02:17.164542\n",
      "(30, 32, 32, 3)\n",
      "0.92818993\n",
      "[Epoch 3/5] [Batch 40/360] [D loss: 0.468838] [G loss: 4.977429] time: 0:02:17.273131\n",
      "(30, 32, 32, 3)\n",
      "0.8623982\n",
      "[Epoch 3/5] [Batch 41/360] [D loss: 0.052798] [G loss: 4.960022] time: 0:02:17.385323\n",
      "(30, 32, 32, 3)\n",
      "0.87443733\n",
      "[Epoch 3/5] [Batch 42/360] [D loss: 0.200238] [G loss: 5.127576] time: 0:02:17.489358\n",
      "(30, 32, 32, 3)\n",
      "0.86355907\n",
      "[Epoch 3/5] [Batch 43/360] [D loss: 0.058925] [G loss: 4.936560] time: 0:02:17.598899\n",
      "(30, 32, 32, 3)\n",
      "0.95051193\n",
      "[Epoch 3/5] [Batch 44/360] [D loss: 0.064552] [G loss: 4.889752] time: 0:02:17.711476\n",
      "(30, 32, 32, 3)\n",
      "0.8646702\n",
      "[Epoch 3/5] [Batch 45/360] [D loss: 0.058732] [G loss: 4.834875] time: 0:02:17.822841\n",
      "(30, 32, 32, 3)\n",
      "0.8948907\n",
      "[Epoch 3/5] [Batch 46/360] [D loss: 0.123329] [G loss: 5.187721] time: 0:02:17.930428\n",
      "(30, 32, 32, 3)\n",
      "0.91491574\n",
      "[Epoch 3/5] [Batch 47/360] [D loss: 0.339114] [G loss: 5.407198] time: 0:02:18.039014\n",
      "(30, 32, 32, 3)\n",
      "0.9171745\n",
      "[Epoch 3/5] [Batch 48/360] [D loss: 0.088323] [G loss: 5.641203] time: 0:02:18.144086\n",
      "(30, 32, 32, 3)\n",
      "0.898586\n",
      "[Epoch 3/5] [Batch 49/360] [D loss: 0.050825] [G loss: 4.878702] time: 0:02:18.253692\n",
      "(30, 32, 32, 3)\n",
      "0.89385206\n",
      "[Epoch 3/5] [Batch 50/360] [D loss: 0.050715] [G loss: 4.598836] time: 0:02:18.358152\n",
      "(30, 32, 32, 3)\n",
      "0.90855485\n",
      "[Epoch 3/5] [Batch 51/360] [D loss: 0.138205] [G loss: 5.076790] time: 0:02:18.464842\n",
      "(30, 32, 32, 3)\n",
      "0.8398841\n",
      "[Epoch 3/5] [Batch 52/360] [D loss: 0.151095] [G loss: 4.642203] time: 0:02:18.572385\n",
      "(30, 32, 32, 3)\n",
      "0.90536135\n",
      "[Epoch 3/5] [Batch 53/360] [D loss: 0.037035] [G loss: 5.272512] time: 0:02:18.682712\n",
      "(30, 32, 32, 3)\n",
      "0.84187174\n",
      "[Epoch 3/5] [Batch 54/360] [D loss: 0.054092] [G loss: 4.893441] time: 0:02:18.793222\n",
      "(30, 32, 32, 3)\n",
      "0.903107\n",
      "[Epoch 3/5] [Batch 55/360] [D loss: 0.047730] [G loss: 5.434235] time: 0:02:18.900530\n",
      "(30, 32, 32, 3)\n",
      "0.9214193\n",
      "[Epoch 3/5] [Batch 56/360] [D loss: 0.140397] [G loss: 5.115057] time: 0:02:19.006364\n",
      "(30, 32, 32, 3)\n",
      "0.868946\n",
      "[Epoch 3/5] [Batch 57/360] [D loss: 0.282015] [G loss: 4.654675] time: 0:02:19.116045\n",
      "(30, 32, 32, 3)\n",
      "0.8777092\n",
      "[Epoch 3/5] [Batch 58/360] [D loss: 0.031127] [G loss: 4.904135] time: 0:02:19.226711\n",
      "(30, 32, 32, 3)\n",
      "0.91885215\n",
      "[Epoch 3/5] [Batch 59/360] [D loss: 0.105459] [G loss: 4.734919] time: 0:02:19.333185\n",
      "(30, 32, 32, 3)\n",
      "0.86610883\n",
      "[Epoch 3/5] [Batch 60/360] [D loss: 0.219602] [G loss: 4.796044] time: 0:02:19.439040\n",
      "(30, 32, 32, 3)\n",
      "0.90203905\n",
      "[Epoch 3/5] [Batch 61/360] [D loss: 0.113839] [G loss: 4.472809] time: 0:02:19.544843\n",
      "(30, 32, 32, 3)\n",
      "0.88744116\n",
      "[Epoch 3/5] [Batch 62/360] [D loss: 0.057434] [G loss: 4.958783] time: 0:02:19.648678\n",
      "(30, 32, 32, 3)\n",
      "0.92136526\n",
      "[Epoch 3/5] [Batch 63/360] [D loss: 0.210623] [G loss: 4.835810] time: 0:02:19.759250\n",
      "(30, 32, 32, 3)\n",
      "0.92012054\n",
      "[Epoch 3/5] [Batch 64/360] [D loss: 0.349751] [G loss: 4.538842] time: 0:02:19.864897\n",
      "(30, 32, 32, 3)\n",
      "0.8693796\n",
      "[Epoch 3/5] [Batch 65/360] [D loss: 0.093823] [G loss: 5.045647] time: 0:02:19.977092\n",
      "(30, 32, 32, 3)\n",
      "0.9128806\n",
      "[Epoch 3/5] [Batch 66/360] [D loss: 0.113960] [G loss: 4.909798] time: 0:02:20.087773\n",
      "(30, 32, 32, 3)\n",
      "0.91199166\n",
      "[Epoch 3/5] [Batch 67/360] [D loss: 0.079722] [G loss: 4.676369] time: 0:02:20.195790\n",
      "(30, 32, 32, 3)\n",
      "0.91700155\n",
      "[Epoch 3/5] [Batch 68/360] [D loss: 0.074202] [G loss: 4.423345] time: 0:02:20.304768\n",
      "(30, 32, 32, 3)\n",
      "0.9452042\n",
      "[Epoch 3/5] [Batch 69/360] [D loss: 0.275197] [G loss: 5.196919] time: 0:02:20.410425\n",
      "(30, 32, 32, 3)\n",
      "0.91510195\n",
      "[Epoch 3/5] [Batch 70/360] [D loss: 0.330912] [G loss: 4.430310] time: 0:02:20.514885\n",
      "(30, 32, 32, 3)\n",
      "0.91279435\n",
      "[Epoch 3/5] [Batch 71/360] [D loss: 0.037627] [G loss: 5.140084] time: 0:02:20.619311\n",
      "(30, 32, 32, 3)\n",
      "0.8827284\n",
      "[Epoch 3/5] [Batch 72/360] [D loss: 0.109145] [G loss: 4.861690] time: 0:02:20.732016\n",
      "(30, 32, 32, 3)\n",
      "0.9133546\n",
      "[Epoch 3/5] [Batch 73/360] [D loss: 0.088898] [G loss: 4.794528] time: 0:02:20.837041\n",
      "(30, 32, 32, 3)\n",
      "0.8780668\n",
      "[Epoch 3/5] [Batch 74/360] [D loss: 0.205651] [G loss: 4.977613] time: 0:02:20.942984\n",
      "(30, 32, 32, 3)\n",
      "0.90367514\n",
      "[Epoch 3/5] [Batch 75/360] [D loss: 0.197102] [G loss: 4.919296] time: 0:02:21.053200\n",
      "(30, 32, 32, 3)\n",
      "0.88822705\n",
      "[Epoch 3/5] [Batch 76/360] [D loss: 0.042536] [G loss: 4.735587] time: 0:02:21.159833\n",
      "(30, 32, 32, 3)\n",
      "0.91105443\n",
      "[Epoch 3/5] [Batch 77/360] [D loss: 0.104923] [G loss: 4.505340] time: 0:02:21.264679\n",
      "(30, 32, 32, 3)\n",
      "0.88616544\n",
      "[Epoch 3/5] [Batch 78/360] [D loss: 0.224546] [G loss: 5.127620] time: 0:02:21.373731\n",
      "(30, 32, 32, 3)\n",
      "0.9187482\n",
      "[Epoch 3/5] [Batch 79/360] [D loss: 0.068787] [G loss: 4.439839] time: 0:02:21.479188\n",
      "(30, 32, 32, 3)\n",
      "0.90878963\n",
      "[Epoch 3/5] [Batch 80/360] [D loss: 0.124721] [G loss: 4.985255] time: 0:02:21.584679\n",
      "(30, 32, 32, 3)\n",
      "0.86405987\n",
      "[Epoch 3/5] [Batch 81/360] [D loss: 0.342859] [G loss: 4.410207] time: 0:02:21.690716\n",
      "(30, 32, 32, 3)\n",
      "0.88953656\n",
      "[Epoch 3/5] [Batch 82/360] [D loss: 0.069467] [G loss: 4.544964] time: 0:02:21.795030\n",
      "(30, 32, 32, 3)\n",
      "0.91065294\n",
      "[Epoch 3/5] [Batch 83/360] [D loss: 0.174215] [G loss: 5.252845] time: 0:02:21.899473\n",
      "(30, 32, 32, 3)\n",
      "0.87945986\n",
      "[Epoch 3/5] [Batch 84/360] [D loss: 0.158559] [G loss: 4.084407] time: 0:02:22.002818\n",
      "(30, 32, 32, 3)\n",
      "0.86039805\n",
      "[Epoch 3/5] [Batch 85/360] [D loss: 0.057209] [G loss: 4.481989] time: 0:02:22.110001\n",
      "(30, 32, 32, 3)\n",
      "0.8974722\n",
      "[Epoch 3/5] [Batch 86/360] [D loss: 0.308993] [G loss: 4.779393] time: 0:02:22.214411\n",
      "(30, 32, 32, 3)\n",
      "0.9276657\n",
      "[Epoch 3/5] [Batch 87/360] [D loss: 0.183556] [G loss: 4.277462] time: 0:02:22.328139\n",
      "(30, 32, 32, 3)\n",
      "0.902364\n",
      "[Epoch 3/5] [Batch 88/360] [D loss: 0.071720] [G loss: 4.509610] time: 0:02:22.439725\n",
      "(30, 32, 32, 3)\n",
      "0.9282921\n",
      "[Epoch 3/5] [Batch 89/360] [D loss: 0.107975] [G loss: 5.059020] time: 0:02:22.545480\n",
      "(30, 32, 32, 3)\n",
      "0.913829\n",
      "[Epoch 3/5] [Batch 90/360] [D loss: 0.294228] [G loss: 4.925145] time: 0:02:22.653414\n",
      "(30, 32, 32, 3)\n",
      "0.91478586\n",
      "[Epoch 3/5] [Batch 91/360] [D loss: 0.050805] [G loss: 5.155684] time: 0:02:22.763868\n",
      "(30, 32, 32, 3)\n",
      "0.89770585\n",
      "[Epoch 3/5] [Batch 92/360] [D loss: 0.135380] [G loss: 4.761996] time: 0:02:22.870612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.8782718\n",
      "[Epoch 3/5] [Batch 93/360] [D loss: 0.084842] [G loss: 4.510068] time: 0:02:22.979188\n",
      "(30, 32, 32, 3)\n",
      "0.8932393\n",
      "[Epoch 3/5] [Batch 94/360] [D loss: 0.165882] [G loss: 5.086541] time: 0:02:23.093667\n",
      "(30, 32, 32, 3)\n",
      "0.8919936\n",
      "[Epoch 3/5] [Batch 95/360] [D loss: 0.210122] [G loss: 4.634564] time: 0:02:23.210959\n",
      "(30, 32, 32, 3)\n",
      "0.9394884\n",
      "[Epoch 3/5] [Batch 96/360] [D loss: 0.073770] [G loss: 5.098966] time: 0:02:23.350265\n",
      "(30, 32, 32, 3)\n",
      "0.9238038\n",
      "[Epoch 3/5] [Batch 97/360] [D loss: 0.165442] [G loss: 4.643304] time: 0:02:23.465540\n",
      "(30, 32, 32, 3)\n",
      "0.89799637\n",
      "[Epoch 3/5] [Batch 98/360] [D loss: 0.035336] [G loss: 4.818523] time: 0:02:23.577555\n",
      "(30, 32, 32, 3)\n",
      "0.88958865\n",
      "[Epoch 3/5] [Batch 99/360] [D loss: 0.131034] [G loss: 4.524772] time: 0:02:23.689593\n",
      "(30, 32, 32, 3)\n",
      "0.92539626\n",
      "[Epoch 3/5] [Batch 100/360] [D loss: 0.148352] [G loss: 4.831443] time: 0:02:23.806324\n",
      "(30, 32, 32, 3)\n",
      "0.8658633\n",
      "[Epoch 3/5] [Batch 101/360] [D loss: 0.198639] [G loss: 4.703774] time: 0:02:23.920771\n",
      "(30, 32, 32, 3)\n",
      "0.91501635\n",
      "[Epoch 3/5] [Batch 102/360] [D loss: 0.204905] [G loss: 4.700269] time: 0:02:24.033176\n",
      "(30, 32, 32, 3)\n",
      "0.9339188\n",
      "[Epoch 3/5] [Batch 103/360] [D loss: 0.396074] [G loss: 4.712926] time: 0:02:24.144767\n",
      "(30, 32, 32, 3)\n",
      "0.91338474\n",
      "[Epoch 3/5] [Batch 104/360] [D loss: 0.072073] [G loss: 4.510101] time: 0:02:24.252973\n",
      "(30, 32, 32, 3)\n",
      "0.9084919\n",
      "[Epoch 3/5] [Batch 105/360] [D loss: 0.116130] [G loss: 4.360821] time: 0:02:24.362560\n",
      "(30, 32, 32, 3)\n",
      "0.91894454\n",
      "[Epoch 3/5] [Batch 106/360] [D loss: 0.136763] [G loss: 4.130006] time: 0:02:24.472230\n",
      "(30, 32, 32, 3)\n",
      "0.9125957\n",
      "[Epoch 3/5] [Batch 107/360] [D loss: 0.063286] [G loss: 4.911647] time: 0:02:24.578207\n",
      "(30, 32, 32, 3)\n",
      "0.86873597\n",
      "[Epoch 3/5] [Batch 108/360] [D loss: 0.144041] [G loss: 5.071559] time: 0:02:24.684931\n",
      "(30, 32, 32, 3)\n",
      "0.94180995\n",
      "[Epoch 3/5] [Batch 109/360] [D loss: 0.125924] [G loss: 4.332170] time: 0:02:24.789682\n",
      "(30, 32, 32, 3)\n",
      "0.8414767\n",
      "[Epoch 3/5] [Batch 110/360] [D loss: 0.076024] [G loss: 4.647925] time: 0:02:24.900203\n",
      "(30, 32, 32, 3)\n",
      "0.9312601\n",
      "[Epoch 3/5] [Batch 111/360] [D loss: 0.283037] [G loss: 4.649977] time: 0:02:25.008882\n",
      "(30, 32, 32, 3)\n",
      "0.899213\n",
      "[Epoch 3/5] [Batch 112/360] [D loss: 0.159820] [G loss: 4.500786] time: 0:02:25.128388\n",
      "(30, 32, 32, 3)\n",
      "0.84937686\n",
      "[Epoch 3/5] [Batch 113/360] [D loss: 0.175956] [G loss: 4.205287] time: 0:02:25.240367\n",
      "(30, 32, 32, 3)\n",
      "0.8581236\n",
      "[Epoch 3/5] [Batch 114/360] [D loss: 0.161619] [G loss: 4.606663] time: 0:02:25.349966\n",
      "(30, 32, 32, 3)\n",
      "0.8894954\n",
      "[Epoch 3/5] [Batch 115/360] [D loss: 0.262098] [G loss: 4.279748] time: 0:02:25.458170\n",
      "(30, 32, 32, 3)\n",
      "0.86536074\n",
      "[Epoch 3/5] [Batch 116/360] [D loss: 0.139077] [G loss: 4.820828] time: 0:02:25.566259\n",
      "(30, 32, 32, 3)\n",
      "0.9290638\n",
      "[Epoch 3/5] [Batch 117/360] [D loss: 0.226615] [G loss: 4.565890] time: 0:02:25.674487\n",
      "(30, 32, 32, 3)\n",
      "0.91475886\n",
      "[Epoch 3/5] [Batch 118/360] [D loss: 0.073659] [G loss: 4.319755] time: 0:02:25.784159\n",
      "(30, 32, 32, 3)\n",
      "0.90133125\n",
      "[Epoch 3/5] [Batch 119/360] [D loss: 0.197088] [G loss: 5.484148] time: 0:02:25.899496\n",
      "(30, 32, 32, 3)\n",
      "0.9221571\n",
      "[Epoch 3/5] [Batch 120/360] [D loss: 0.075947] [G loss: 4.346764] time: 0:02:26.011555\n",
      "(30, 32, 32, 3)\n",
      "0.93261176\n",
      "[Epoch 3/5] [Batch 121/360] [D loss: 0.124345] [G loss: 4.727564] time: 0:02:26.127897\n",
      "(30, 32, 32, 3)\n",
      "0.9175803\n",
      "[Epoch 3/5] [Batch 122/360] [D loss: 0.497022] [G loss: 3.650984] time: 0:02:26.232658\n",
      "(30, 32, 32, 3)\n",
      "0.892926\n",
      "[Epoch 3/5] [Batch 123/360] [D loss: 0.059372] [G loss: 4.462800] time: 0:02:26.339498\n",
      "(30, 32, 32, 3)\n",
      "0.9484994\n",
      "[Epoch 3/5] [Batch 124/360] [D loss: 0.251478] [G loss: 4.181071] time: 0:02:26.455288\n",
      "(30, 32, 32, 3)\n",
      "0.88305444\n",
      "[Epoch 3/5] [Batch 125/360] [D loss: 0.081334] [G loss: 4.537632] time: 0:02:26.561721\n",
      "(30, 32, 32, 3)\n",
      "0.8732552\n",
      "[Epoch 3/5] [Batch 126/360] [D loss: 0.359739] [G loss: 3.801074] time: 0:02:26.669910\n",
      "(30, 32, 32, 3)\n",
      "0.93918246\n",
      "[Epoch 3/5] [Batch 127/360] [D loss: 0.097632] [G loss: 4.713186] time: 0:02:26.775329\n",
      "(30, 32, 32, 3)\n",
      "0.8698321\n",
      "[Epoch 3/5] [Batch 128/360] [D loss: 0.165834] [G loss: 3.980795] time: 0:02:26.882198\n",
      "(30, 32, 32, 3)\n",
      "0.8699298\n",
      "[Epoch 3/5] [Batch 129/360] [D loss: 0.102024] [G loss: 4.343108] time: 0:02:26.993268\n",
      "(30, 32, 32, 3)\n",
      "0.8868769\n",
      "[Epoch 3/5] [Batch 130/360] [D loss: 0.280534] [G loss: 4.526993] time: 0:02:27.104801\n",
      "(30, 32, 32, 3)\n",
      "0.9053173\n",
      "[Epoch 3/5] [Batch 131/360] [D loss: 0.079075] [G loss: 4.879532] time: 0:02:27.210417\n",
      "(30, 32, 32, 3)\n",
      "0.8839404\n",
      "[Epoch 3/5] [Batch 132/360] [D loss: 0.153977] [G loss: 4.720632] time: 0:02:27.314806\n",
      "(30, 32, 32, 3)\n",
      "0.93736714\n",
      "[Epoch 3/5] [Batch 133/360] [D loss: 0.074983] [G loss: 4.287932] time: 0:02:27.421777\n",
      "(30, 32, 32, 3)\n",
      "0.92648125\n",
      "[Epoch 3/5] [Batch 134/360] [D loss: 0.122915] [G loss: 4.547906] time: 0:02:27.528922\n",
      "(30, 32, 32, 3)\n",
      "0.90641516\n",
      "[Epoch 3/5] [Batch 135/360] [D loss: 0.267711] [G loss: 4.204813] time: 0:02:27.639638\n",
      "(30, 32, 32, 3)\n",
      "0.9235428\n",
      "[Epoch 3/5] [Batch 136/360] [D loss: 0.092643] [G loss: 4.858294] time: 0:02:27.746148\n",
      "(30, 32, 32, 3)\n",
      "0.9064663\n",
      "[Epoch 3/5] [Batch 137/360] [D loss: 0.169600] [G loss: 4.229981] time: 0:02:27.856217\n",
      "(30, 32, 32, 3)\n",
      "0.87631315\n",
      "[Epoch 3/5] [Batch 138/360] [D loss: 0.132274] [G loss: 4.275217] time: 0:02:27.963907\n",
      "(30, 32, 32, 3)\n",
      "0.9016916\n",
      "[Epoch 3/5] [Batch 139/360] [D loss: 0.188844] [G loss: 4.120082] time: 0:02:28.073716\n",
      "(30, 32, 32, 3)\n",
      "0.9368909\n",
      "[Epoch 3/5] [Batch 140/360] [D loss: 0.087235] [G loss: 4.460442] time: 0:02:28.179600\n",
      "(30, 32, 32, 3)\n",
      "0.9066549\n",
      "[Epoch 3/5] [Batch 141/360] [D loss: 0.179651] [G loss: 4.361209] time: 0:02:28.287079\n",
      "(30, 32, 32, 3)\n",
      "0.89104086\n",
      "[Epoch 3/5] [Batch 142/360] [D loss: 0.244582] [G loss: 4.307296] time: 0:02:28.399232\n",
      "(30, 32, 32, 3)\n",
      "0.9141124\n",
      "[Epoch 3/5] [Batch 143/360] [D loss: 0.313051] [G loss: 4.357280] time: 0:02:28.506531\n",
      "(30, 32, 32, 3)\n",
      "0.9216146\n",
      "[Epoch 3/5] [Batch 144/360] [D loss: 0.092782] [G loss: 4.417160] time: 0:02:28.613084\n",
      "(30, 32, 32, 3)\n",
      "0.9059267\n",
      "[Epoch 3/5] [Batch 145/360] [D loss: 0.169111] [G loss: 4.332725] time: 0:02:28.719867\n",
      "(30, 32, 32, 3)\n",
      "0.8313802\n",
      "[Epoch 3/5] [Batch 146/360] [D loss: 0.082229] [G loss: 4.140599] time: 0:02:28.823664\n",
      "(30, 32, 32, 3)\n",
      "0.93680924\n",
      "[Epoch 3/5] [Batch 147/360] [D loss: 0.139242] [G loss: 4.518516] time: 0:02:28.930291\n",
      "(30, 32, 32, 3)\n",
      "0.92668647\n",
      "[Epoch 3/5] [Batch 148/360] [D loss: 0.208647] [G loss: 4.333431] time: 0:02:29.037341\n",
      "(30, 32, 32, 3)\n",
      "0.8225343\n",
      "[Epoch 3/5] [Batch 149/360] [D loss: 0.161522] [G loss: 3.911391] time: 0:02:29.146654\n",
      "(30, 32, 32, 3)\n",
      "0.88651866\n",
      "[Epoch 3/5] [Batch 150/360] [D loss: 0.085690] [G loss: 4.483483] time: 0:02:29.254793\n",
      "(30, 32, 32, 3)\n",
      "0.89313954\n",
      "[Epoch 3/5] [Batch 151/360] [D loss: 0.290707] [G loss: 3.958680] time: 0:02:29.360424\n",
      "(30, 32, 32, 3)\n",
      "0.8723135\n",
      "[Epoch 3/5] [Batch 152/360] [D loss: 0.045838] [G loss: 4.151160] time: 0:02:29.464909\n",
      "(30, 32, 32, 3)\n",
      "0.9090746\n",
      "[Epoch 3/5] [Batch 153/360] [D loss: 0.301493] [G loss: 3.996601] time: 0:02:29.581100\n",
      "(30, 32, 32, 3)\n",
      "0.8762584\n",
      "[Epoch 3/5] [Batch 154/360] [D loss: 0.081192] [G loss: 4.023689] time: 0:02:29.691396\n",
      "(30, 32, 32, 3)\n",
      "0.8612053\n",
      "[Epoch 3/5] [Batch 155/360] [D loss: 0.213624] [G loss: 4.389571] time: 0:02:29.799971\n",
      "(30, 32, 32, 3)\n",
      "0.88047713\n",
      "[Epoch 3/5] [Batch 156/360] [D loss: 0.269861] [G loss: 3.800196] time: 0:02:29.904964\n",
      "(30, 32, 32, 3)\n",
      "0.9311595\n",
      "[Epoch 3/5] [Batch 157/360] [D loss: 0.082447] [G loss: 4.064518] time: 0:02:30.013384\n",
      "(30, 32, 32, 3)\n",
      "0.91993284\n",
      "[Epoch 3/5] [Batch 158/360] [D loss: 0.351318] [G loss: 3.482299] time: 0:02:30.117279\n",
      "(30, 32, 32, 3)\n",
      "0.90442324\n",
      "[Epoch 3/5] [Batch 159/360] [D loss: 0.091183] [G loss: 4.486754] time: 0:02:30.225543\n",
      "(30, 32, 32, 3)\n",
      "0.9115333\n",
      "[Epoch 3/5] [Batch 160/360] [D loss: 0.373718] [G loss: 3.240531] time: 0:02:30.329775\n",
      "(30, 32, 32, 3)\n",
      "0.890604\n",
      "[Epoch 3/5] [Batch 161/360] [D loss: 0.128162] [G loss: 4.434951] time: 0:02:30.435726\n",
      "(30, 32, 32, 3)\n",
      "0.8870446\n",
      "[Epoch 3/5] [Batch 162/360] [D loss: 0.207048] [G loss: 4.123610] time: 0:02:30.542499\n",
      "(30, 32, 32, 3)\n",
      "0.90913516\n",
      "[Epoch 3/5] [Batch 163/360] [D loss: 0.112005] [G loss: 4.332317] time: 0:02:30.651043\n",
      "(30, 32, 32, 3)\n",
      "0.8887098\n",
      "[Epoch 3/5] [Batch 164/360] [D loss: 0.142285] [G loss: 4.298750] time: 0:02:30.759362\n",
      "(30, 32, 32, 3)\n",
      "0.89337236\n",
      "[Epoch 3/5] [Batch 165/360] [D loss: 0.218717] [G loss: 4.221214] time: 0:02:30.865292\n",
      "(30, 32, 32, 3)\n",
      "0.9011628\n",
      "[Epoch 3/5] [Batch 166/360] [D loss: 0.234337] [G loss: 4.270389] time: 0:02:30.973321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9624062\n",
      "[Epoch 3/5] [Batch 167/360] [D loss: 0.119902] [G loss: 3.976558] time: 0:02:31.085509\n",
      "(30, 32, 32, 3)\n",
      "0.9341349\n",
      "[Epoch 3/5] [Batch 168/360] [D loss: 0.215031] [G loss: 4.111810] time: 0:02:31.189016\n",
      "(30, 32, 32, 3)\n",
      "0.9000147\n",
      "[Epoch 3/5] [Batch 169/360] [D loss: 0.132990] [G loss: 4.204776] time: 0:02:31.293371\n",
      "(30, 32, 32, 3)\n",
      "0.90609455\n",
      "[Epoch 3/5] [Batch 170/360] [D loss: 0.333276] [G loss: 4.008218] time: 0:02:31.398264\n",
      "(30, 32, 32, 3)\n",
      "0.8881692\n",
      "[Epoch 3/5] [Batch 171/360] [D loss: 0.084030] [G loss: 4.288616] time: 0:02:31.505953\n",
      "(30, 32, 32, 3)\n",
      "0.9159855\n",
      "[Epoch 3/5] [Batch 172/360] [D loss: 0.345211] [G loss: 3.885163] time: 0:02:31.611156\n",
      "(30, 32, 32, 3)\n",
      "0.8874306\n",
      "[Epoch 3/5] [Batch 173/360] [D loss: 0.085991] [G loss: 4.035859] time: 0:02:31.721815\n",
      "(30, 32, 32, 3)\n",
      "0.9038477\n",
      "[Epoch 3/5] [Batch 174/360] [D loss: 0.394982] [G loss: 3.540571] time: 0:02:31.826747\n",
      "(30, 32, 32, 3)\n",
      "0.9264191\n",
      "[Epoch 3/5] [Batch 175/360] [D loss: 0.127919] [G loss: 4.281541] time: 0:02:31.937337\n",
      "(30, 32, 32, 3)\n",
      "0.8962714\n",
      "[Epoch 3/5] [Batch 176/360] [D loss: 0.216121] [G loss: 4.100342] time: 0:02:32.041614\n",
      "(30, 32, 32, 3)\n",
      "0.8873901\n",
      "[Epoch 3/5] [Batch 177/360] [D loss: 0.133891] [G loss: 3.961435] time: 0:02:32.151622\n",
      "(30, 32, 32, 3)\n",
      "0.9114146\n",
      "[Epoch 3/5] [Batch 178/360] [D loss: 0.151737] [G loss: 4.009795] time: 0:02:32.258837\n",
      "(30, 32, 32, 3)\n",
      "0.8853767\n",
      "[Epoch 3/5] [Batch 179/360] [D loss: 0.278821] [G loss: 3.703523] time: 0:02:32.365901\n",
      "(30, 32, 32, 3)\n",
      "0.895976\n",
      "[Epoch 3/5] [Batch 180/360] [D loss: 0.137187] [G loss: 4.313203] time: 0:02:32.473778\n",
      "(30, 32, 32, 3)\n",
      "0.8757784\n",
      "[Epoch 3/5] [Batch 181/360] [D loss: 0.364634] [G loss: 3.503939] time: 0:02:32.579280\n",
      "(30, 32, 32, 3)\n",
      "0.9231115\n",
      "[Epoch 3/5] [Batch 182/360] [D loss: 0.128315] [G loss: 4.330317] time: 0:02:32.683168\n",
      "(30, 32, 32, 3)\n",
      "0.9238801\n",
      "[Epoch 3/5] [Batch 183/360] [D loss: 0.224947] [G loss: 4.277208] time: 0:02:32.788641\n",
      "(30, 32, 32, 3)\n",
      "0.882911\n",
      "[Epoch 3/5] [Batch 184/360] [D loss: 0.121884] [G loss: 3.921909] time: 0:02:32.894674\n",
      "(30, 32, 32, 3)\n",
      "0.902952\n",
      "[Epoch 3/5] [Batch 185/360] [D loss: 0.207289] [G loss: 3.899735] time: 0:02:33.020004\n",
      "(30, 32, 32, 3)\n",
      "0.9076088\n",
      "[Epoch 3/5] [Batch 186/360] [D loss: 0.142580] [G loss: 4.060510] time: 0:02:33.123893\n",
      "(30, 32, 32, 3)\n",
      "0.9338603\n",
      "[Epoch 3/5] [Batch 187/360] [D loss: 0.214280] [G loss: 4.231219] time: 0:02:33.229686\n",
      "(30, 32, 32, 3)\n",
      "0.90831393\n",
      "[Epoch 3/5] [Batch 188/360] [D loss: 0.172608] [G loss: 4.240522] time: 0:02:33.333550\n",
      "(30, 32, 32, 3)\n",
      "0.87112784\n",
      "[Epoch 3/5] [Batch 189/360] [D loss: 0.176835] [G loss: 3.829701] time: 0:02:33.443274\n",
      "(30, 32, 32, 3)\n",
      "0.95498735\n",
      "[Epoch 3/5] [Batch 190/360] [D loss: 0.130536] [G loss: 4.057444] time: 0:02:33.549463\n",
      "(30, 32, 32, 3)\n",
      "0.89253116\n",
      "[Epoch 3/5] [Batch 191/360] [D loss: 0.278268] [G loss: 4.034315] time: 0:02:33.668041\n",
      "(30, 32, 32, 3)\n",
      "0.9038678\n",
      "[Epoch 3/5] [Batch 192/360] [D loss: 0.159270] [G loss: 4.229111] time: 0:02:33.772262\n",
      "(30, 32, 32, 3)\n",
      "0.8822093\n",
      "[Epoch 3/5] [Batch 193/360] [D loss: 0.266735] [G loss: 3.953408] time: 0:02:33.883823\n",
      "(30, 32, 32, 3)\n",
      "0.88582736\n",
      "[Epoch 3/5] [Batch 194/360] [D loss: 0.113213] [G loss: 4.285647] time: 0:02:33.989842\n",
      "(30, 32, 32, 3)\n",
      "0.9288942\n",
      "[Epoch 3/5] [Batch 195/360] [D loss: 0.421461] [G loss: 3.433150] time: 0:02:34.101209\n",
      "(30, 32, 32, 3)\n",
      "0.9232709\n",
      "[Epoch 3/5] [Batch 196/360] [D loss: 0.142277] [G loss: 4.160074] time: 0:02:34.206487\n",
      "(30, 32, 32, 3)\n",
      "0.8872494\n",
      "[Epoch 3/5] [Batch 197/360] [D loss: 0.242897] [G loss: 3.671065] time: 0:02:34.329277\n",
      "(30, 32, 32, 3)\n",
      "0.8493767\n",
      "[Epoch 3/5] [Batch 198/360] [D loss: 0.130235] [G loss: 4.260454] time: 0:02:34.436032\n",
      "(30, 32, 32, 3)\n",
      "0.9611218\n",
      "[Epoch 3/5] [Batch 199/360] [D loss: 0.214600] [G loss: 4.409140] time: 0:02:34.541811\n",
      "(30, 32, 32, 3)\n",
      "0.89864224\n",
      "[Epoch 3/5] [Batch 200/360] [D loss: 0.119396] [G loss: 3.678513] time: 0:02:34.645352\n",
      "(30, 32, 32, 3)\n",
      "0.9162944\n",
      "[Epoch 3/5] [Batch 201/360] [D loss: 0.140964] [G loss: 4.336513] time: 0:02:34.753514\n",
      "(30, 32, 32, 3)\n",
      "0.8600149\n",
      "[Epoch 3/5] [Batch 202/360] [D loss: 0.347703] [G loss: 3.873260] time: 0:02:34.860352\n",
      "(30, 32, 32, 3)\n",
      "0.8971383\n",
      "[Epoch 3/5] [Batch 203/360] [D loss: 0.083940] [G loss: 4.552320] time: 0:02:34.966776\n",
      "(30, 32, 32, 3)\n",
      "0.87531966\n",
      "[Epoch 3/5] [Batch 204/360] [D loss: 0.342570] [G loss: 3.375098] time: 0:02:35.070776\n",
      "(30, 32, 32, 3)\n",
      "0.9003871\n",
      "[Epoch 3/5] [Batch 205/360] [D loss: 0.109980] [G loss: 3.761617] time: 0:02:35.176917\n",
      "(30, 32, 32, 3)\n",
      "0.8890214\n",
      "[Epoch 3/5] [Batch 206/360] [D loss: 0.176057] [G loss: 3.880849] time: 0:02:35.280113\n",
      "(30, 32, 32, 3)\n",
      "0.87930393\n",
      "[Epoch 3/5] [Batch 207/360] [D loss: 0.200343] [G loss: 3.634185] time: 0:02:35.391265\n",
      "(30, 32, 32, 3)\n",
      "0.87162143\n",
      "[Epoch 3/5] [Batch 208/360] [D loss: 0.190096] [G loss: 3.868825] time: 0:02:35.500893\n",
      "(30, 32, 32, 3)\n",
      "0.9218113\n",
      "[Epoch 3/5] [Batch 209/360] [D loss: 0.253140] [G loss: 4.091089] time: 0:02:35.611350\n",
      "(30, 32, 32, 3)\n",
      "0.9007702\n",
      "[Epoch 3/5] [Batch 210/360] [D loss: 0.140870] [G loss: 3.656500] time: 0:02:35.715209\n",
      "(30, 32, 32, 3)\n",
      "0.8944389\n",
      "[Epoch 3/5] [Batch 211/360] [D loss: 0.270660] [G loss: 3.745969] time: 0:02:35.824055\n",
      "(30, 32, 32, 3)\n",
      "0.96639013\n",
      "[Epoch 3/5] [Batch 212/360] [D loss: 0.141543] [G loss: 4.080812] time: 0:02:35.931156\n",
      "(30, 32, 32, 3)\n",
      "0.8568425\n",
      "[Epoch 3/5] [Batch 213/360] [D loss: 0.345586] [G loss: 3.630310] time: 0:02:36.039762\n",
      "(30, 32, 32, 3)\n",
      "0.89477855\n",
      "[Epoch 3/5] [Batch 214/360] [D loss: 0.108536] [G loss: 4.219818] time: 0:02:36.144551\n",
      "(30, 32, 32, 3)\n",
      "0.92447275\n",
      "[Epoch 3/5] [Batch 215/360] [D loss: 0.321067] [G loss: 3.419912] time: 0:02:36.248990\n",
      "(30, 32, 32, 3)\n",
      "0.84757805\n",
      "[Epoch 3/5] [Batch 216/360] [D loss: 0.118246] [G loss: 4.138011] time: 0:02:36.361890\n",
      "(30, 32, 32, 3)\n",
      "0.8899067\n",
      "[Epoch 3/5] [Batch 217/360] [D loss: 0.338855] [G loss: 4.017482] time: 0:02:36.467697\n",
      "(30, 32, 32, 3)\n",
      "0.9193392\n",
      "[Epoch 3/5] [Batch 218/360] [D loss: 0.126886] [G loss: 3.877173] time: 0:02:36.574603\n",
      "(30, 32, 32, 3)\n",
      "0.91917974\n",
      "[Epoch 3/5] [Batch 219/360] [D loss: 0.315656] [G loss: 3.474918] time: 0:02:36.678903\n",
      "(30, 32, 32, 3)\n",
      "0.9305489\n",
      "[Epoch 3/5] [Batch 220/360] [D loss: 0.145558] [G loss: 4.062808] time: 0:02:36.785701\n",
      "(30, 32, 32, 3)\n",
      "0.93021417\n",
      "[Epoch 3/5] [Batch 221/360] [D loss: 0.199364] [G loss: 3.833369] time: 0:02:36.896799\n",
      "(30, 32, 32, 3)\n",
      "0.8635799\n",
      "[Epoch 3/5] [Batch 222/360] [D loss: 0.211081] [G loss: 3.479575] time: 0:02:37.005775\n",
      "(30, 32, 32, 3)\n",
      "0.9409154\n",
      "[Epoch 3/5] [Batch 223/360] [D loss: 0.200202] [G loss: 3.641273] time: 0:02:37.112055\n",
      "(30, 32, 32, 3)\n",
      "0.9356448\n",
      "[Epoch 3/5] [Batch 224/360] [D loss: 0.187606] [G loss: 3.658019] time: 0:02:37.224320\n",
      "(30, 32, 32, 3)\n",
      "0.90273136\n",
      "[Epoch 3/5] [Batch 225/360] [D loss: 0.215328] [G loss: 4.116307] time: 0:02:37.339893\n",
      "(30, 32, 32, 3)\n",
      "0.8943432\n",
      "[Epoch 3/5] [Batch 226/360] [D loss: 0.216777] [G loss: 3.553473] time: 0:02:37.445531\n",
      "(30, 32, 32, 3)\n",
      "0.8623213\n",
      "[Epoch 3/5] [Batch 227/360] [D loss: 0.116974] [G loss: 4.006236] time: 0:02:37.554902\n",
      "(30, 32, 32, 3)\n",
      "0.89897203\n",
      "[Epoch 3/5] [Batch 228/360] [D loss: 0.321738] [G loss: 3.646502] time: 0:02:37.658819\n",
      "(30, 32, 32, 3)\n",
      "0.8642273\n",
      "[Epoch 3/5] [Batch 229/360] [D loss: 0.101290] [G loss: 3.726493] time: 0:02:37.770284\n",
      "(30, 32, 32, 3)\n",
      "0.91055304\n",
      "[Epoch 3/5] [Batch 230/360] [D loss: 0.290670] [G loss: 3.280874] time: 0:02:37.875153\n",
      "(30, 32, 32, 3)\n",
      "0.89131624\n",
      "[Epoch 3/5] [Batch 231/360] [D loss: 0.143161] [G loss: 4.447219] time: 0:02:37.985125\n",
      "(30, 32, 32, 3)\n",
      "0.8821158\n",
      "[Epoch 3/5] [Batch 232/360] [D loss: 0.366445] [G loss: 3.360704] time: 0:02:38.091378\n",
      "(30, 32, 32, 3)\n",
      "0.93602794\n",
      "[Epoch 3/5] [Batch 233/360] [D loss: 0.155112] [G loss: 3.640037] time: 0:02:38.198709\n",
      "(30, 32, 32, 3)\n",
      "0.95227265\n",
      "[Epoch 3/5] [Batch 234/360] [D loss: 0.176148] [G loss: 4.459494] time: 0:02:38.305412\n",
      "(30, 32, 32, 3)\n",
      "0.95705\n",
      "[Epoch 3/5] [Batch 235/360] [D loss: 0.260906] [G loss: 3.422866] time: 0:02:38.424173\n",
      "(30, 32, 32, 3)\n",
      "0.885842\n",
      "[Epoch 3/5] [Batch 236/360] [D loss: 0.137070] [G loss: 4.239265] time: 0:02:38.527798\n",
      "(30, 32, 32, 3)\n",
      "0.8885908\n",
      "[Epoch 3/5] [Batch 237/360] [D loss: 0.289749] [G loss: 3.727931] time: 0:02:38.635625\n",
      "(30, 32, 32, 3)\n",
      "0.8943324\n",
      "[Epoch 3/5] [Batch 238/360] [D loss: 0.146512] [G loss: 3.718270] time: 0:02:38.746238\n",
      "(30, 32, 32, 3)\n",
      "0.93933016\n",
      "[Epoch 3/5] [Batch 239/360] [D loss: 0.292575] [G loss: 3.590048] time: 0:02:38.855425\n",
      "(30, 32, 32, 3)\n",
      "0.87787175\n",
      "[Epoch 3/5] [Batch 240/360] [D loss: 0.135936] [G loss: 3.638678] time: 0:02:38.963083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.929074\n",
      "[Epoch 3/5] [Batch 241/360] [D loss: 0.246528] [G loss: 3.426146] time: 0:02:39.075871\n",
      "(30, 32, 32, 3)\n",
      "0.9297517\n",
      "[Epoch 3/5] [Batch 242/360] [D loss: 0.183763] [G loss: 3.625642] time: 0:02:39.182830\n",
      "(30, 32, 32, 3)\n",
      "0.93552357\n",
      "[Epoch 3/5] [Batch 243/360] [D loss: 0.227311] [G loss: 3.772077] time: 0:02:39.290732\n",
      "(30, 32, 32, 3)\n",
      "0.8974436\n",
      "[Epoch 3/5] [Batch 244/360] [D loss: 0.164716] [G loss: 3.361004] time: 0:02:39.398853\n",
      "(30, 32, 32, 3)\n",
      "0.8848452\n",
      "[Epoch 3/5] [Batch 245/360] [D loss: 0.232220] [G loss: 4.029387] time: 0:02:39.507900\n",
      "(30, 32, 32, 3)\n",
      "0.88336015\n",
      "[Epoch 3/5] [Batch 246/360] [D loss: 0.156089] [G loss: 3.786688] time: 0:02:39.613986\n",
      "(30, 32, 32, 3)\n",
      "0.92710966\n",
      "[Epoch 3/5] [Batch 247/360] [D loss: 0.189927] [G loss: 3.607611] time: 0:02:39.724371\n",
      "(30, 32, 32, 3)\n",
      "0.91924816\n",
      "[Epoch 3/5] [Batch 248/360] [D loss: 0.266160] [G loss: 4.251887] time: 0:02:39.832240\n",
      "(30, 32, 32, 3)\n",
      "0.8900707\n",
      "[Epoch 3/5] [Batch 249/360] [D loss: 0.221284] [G loss: 3.880113] time: 0:02:39.948445\n",
      "(30, 32, 32, 3)\n",
      "0.87856144\n",
      "[Epoch 3/5] [Batch 250/360] [D loss: 0.169919] [G loss: 3.871097] time: 0:02:40.053884\n",
      "(30, 32, 32, 3)\n",
      "0.92602587\n",
      "[Epoch 3/5] [Batch 251/360] [D loss: 0.251989] [G loss: 3.646068] time: 0:02:40.159261\n",
      "(30, 32, 32, 3)\n",
      "0.9175096\n",
      "[Epoch 3/5] [Batch 252/360] [D loss: 0.152090] [G loss: 3.791586] time: 0:02:40.267659\n",
      "(30, 32, 32, 3)\n",
      "0.8841103\n",
      "[Epoch 3/5] [Batch 253/360] [D loss: 0.343807] [G loss: 3.499400] time: 0:02:40.389144\n",
      "(30, 32, 32, 3)\n",
      "0.89879775\n",
      "[Epoch 3/5] [Batch 254/360] [D loss: 0.106382] [G loss: 3.737456] time: 0:02:40.496020\n",
      "(30, 32, 32, 3)\n",
      "0.904078\n",
      "[Epoch 3/5] [Batch 255/360] [D loss: 0.302709] [G loss: 3.712720] time: 0:02:40.603483\n",
      "(30, 32, 32, 3)\n",
      "0.8615324\n",
      "[Epoch 3/5] [Batch 256/360] [D loss: 0.137513] [G loss: 4.061790] time: 0:02:40.709488\n",
      "(30, 32, 32, 3)\n",
      "0.9131978\n",
      "[Epoch 3/5] [Batch 257/360] [D loss: 0.376038] [G loss: 3.312434] time: 0:02:40.816698\n",
      "(30, 32, 32, 3)\n",
      "0.9101982\n",
      "[Epoch 3/5] [Batch 258/360] [D loss: 0.131441] [G loss: 3.718710] time: 0:02:40.923179\n",
      "(30, 32, 32, 3)\n",
      "0.891547\n",
      "[Epoch 3/5] [Batch 259/360] [D loss: 0.266261] [G loss: 3.768113] time: 0:02:41.028684\n",
      "(30, 32, 32, 3)\n",
      "0.86659175\n",
      "[Epoch 3/5] [Batch 260/360] [D loss: 0.127130] [G loss: 3.442200] time: 0:02:41.141560\n",
      "(30, 32, 32, 3)\n",
      "0.8988752\n",
      "[Epoch 3/5] [Batch 261/360] [D loss: 0.227543] [G loss: 3.492442] time: 0:02:41.249268\n",
      "(30, 32, 32, 3)\n",
      "0.96026\n",
      "[Epoch 3/5] [Batch 262/360] [D loss: 0.180429] [G loss: 3.667881] time: 0:02:41.353397\n",
      "(30, 32, 32, 3)\n",
      "0.9444192\n",
      "[Epoch 3/5] [Batch 263/360] [D loss: 0.242280] [G loss: 3.277471] time: 0:02:41.459133\n",
      "(30, 32, 32, 3)\n",
      "0.9376757\n",
      "[Epoch 3/5] [Batch 264/360] [D loss: 0.173589] [G loss: 4.031379] time: 0:02:41.562475\n",
      "(30, 32, 32, 3)\n",
      "0.8889275\n",
      "[Epoch 3/5] [Batch 265/360] [D loss: 0.294925] [G loss: 3.174642] time: 0:02:41.670925\n",
      "(30, 32, 32, 3)\n",
      "0.8989046\n",
      "[Epoch 3/5] [Batch 266/360] [D loss: 0.122798] [G loss: 3.545222] time: 0:02:41.783488\n",
      "(30, 32, 32, 3)\n",
      "0.89444655\n",
      "[Epoch 3/5] [Batch 267/360] [D loss: 0.282263] [G loss: 3.464362] time: 0:02:41.892209\n",
      "(30, 32, 32, 3)\n",
      "0.87392133\n",
      "[Epoch 3/5] [Batch 268/360] [D loss: 0.169583] [G loss: 3.398451] time: 0:02:41.997540\n",
      "(30, 32, 32, 3)\n",
      "0.9158928\n",
      "[Epoch 3/5] [Batch 269/360] [D loss: 0.308215] [G loss: 3.373294] time: 0:02:42.102976\n",
      "(30, 32, 32, 3)\n",
      "0.91706175\n",
      "[Epoch 3/5] [Batch 270/360] [D loss: 0.148777] [G loss: 3.848589] time: 0:02:42.210341\n",
      "(30, 32, 32, 3)\n",
      "0.9241983\n",
      "[Epoch 3/5] [Batch 271/360] [D loss: 0.362420] [G loss: 3.110932] time: 0:02:42.321409\n",
      "(30, 32, 32, 3)\n",
      "0.8924177\n",
      "[Epoch 3/5] [Batch 272/360] [D loss: 0.144400] [G loss: 3.652833] time: 0:02:42.426103\n",
      "(30, 32, 32, 3)\n",
      "0.8900695\n",
      "[Epoch 3/5] [Batch 273/360] [D loss: 0.264029] [G loss: 3.457258] time: 0:02:42.531356\n",
      "(30, 32, 32, 3)\n",
      "0.9109699\n",
      "[Epoch 3/5] [Batch 274/360] [D loss: 0.173210] [G loss: 3.368549] time: 0:02:42.639148\n",
      "(30, 32, 32, 3)\n",
      "0.91771716\n",
      "[Epoch 3/5] [Batch 275/360] [D loss: 0.287927] [G loss: 3.300585] time: 0:02:42.745984\n",
      "(30, 32, 32, 3)\n",
      "0.88235456\n",
      "[Epoch 3/5] [Batch 276/360] [D loss: 0.143673] [G loss: 3.453683] time: 0:02:42.852698\n",
      "(30, 32, 32, 3)\n",
      "0.889484\n",
      "[Epoch 3/5] [Batch 277/360] [D loss: 0.244122] [G loss: 3.336837] time: 0:02:42.960459\n",
      "(30, 32, 32, 3)\n",
      "0.9048485\n",
      "[Epoch 3/5] [Batch 278/360] [D loss: 0.190619] [G loss: 3.276311] time: 0:02:43.064308\n",
      "(30, 32, 32, 3)\n",
      "0.89280987\n",
      "[Epoch 3/5] [Batch 279/360] [D loss: 0.219700] [G loss: 3.437824] time: 0:02:43.181352\n",
      "(30, 32, 32, 3)\n",
      "0.9142871\n",
      "[Epoch 3/5] [Batch 280/360] [D loss: 0.203435] [G loss: 3.383070] time: 0:02:43.286582\n",
      "(30, 32, 32, 3)\n",
      "0.90085053\n",
      "[Epoch 3/5] [Batch 281/360] [D loss: 0.245241] [G loss: 4.052059] time: 0:02:43.397214\n",
      "(30, 32, 32, 3)\n",
      "0.9180091\n",
      "[Epoch 3/5] [Batch 282/360] [D loss: 0.212045] [G loss: 3.325458] time: 0:02:43.501592\n",
      "(30, 32, 32, 3)\n",
      "0.8646774\n",
      "[Epoch 3/5] [Batch 283/360] [D loss: 0.163944] [G loss: 3.448660] time: 0:02:43.611518\n",
      "(30, 32, 32, 3)\n",
      "0.90884393\n",
      "[Epoch 3/5] [Batch 284/360] [D loss: 0.330228] [G loss: 3.469429] time: 0:02:43.716354\n",
      "(30, 32, 32, 3)\n",
      "0.93537515\n",
      "[Epoch 3/5] [Batch 285/360] [D loss: 0.121016] [G loss: 3.732578] time: 0:02:43.823402\n",
      "(30, 32, 32, 3)\n",
      "0.90305656\n",
      "[Epoch 3/5] [Batch 286/360] [D loss: 0.322522] [G loss: 3.105341] time: 0:02:43.928368\n",
      "(30, 32, 32, 3)\n",
      "0.8801985\n",
      "[Epoch 3/5] [Batch 287/360] [D loss: 0.161947] [G loss: 3.603981] time: 0:02:44.036007\n",
      "(30, 32, 32, 3)\n",
      "0.86826485\n",
      "[Epoch 3/5] [Batch 288/360] [D loss: 0.352680] [G loss: 2.781157] time: 0:02:44.143394\n",
      "(30, 32, 32, 3)\n",
      "0.935787\n",
      "[Epoch 3/5] [Batch 289/360] [D loss: 0.158538] [G loss: 3.711874] time: 0:02:44.250777\n",
      "(30, 32, 32, 3)\n",
      "0.9345489\n",
      "[Epoch 3/5] [Batch 290/360] [D loss: 0.256714] [G loss: 3.059230] time: 0:02:44.361807\n",
      "(30, 32, 32, 3)\n",
      "0.8903677\n",
      "[Epoch 3/5] [Batch 291/360] [D loss: 0.180971] [G loss: 3.365400] time: 0:02:44.472553\n",
      "(30, 32, 32, 3)\n",
      "0.8865439\n",
      "[Epoch 3/5] [Batch 292/360] [D loss: 0.229522] [G loss: 3.982831] time: 0:02:44.579254\n",
      "(30, 32, 32, 3)\n",
      "0.9180596\n",
      "[Epoch 3/5] [Batch 293/360] [D loss: 0.177725] [G loss: 3.335979] time: 0:02:44.686931\n",
      "(30, 32, 32, 3)\n",
      "0.9263189\n",
      "[Epoch 3/5] [Batch 294/360] [D loss: 0.215618] [G loss: 3.713078] time: 0:02:44.796309\n",
      "(30, 32, 32, 3)\n",
      "0.8900957\n",
      "[Epoch 3/5] [Batch 295/360] [D loss: 0.262507] [G loss: 3.681550] time: 0:02:44.903446\n",
      "(30, 32, 32, 3)\n",
      "0.91929454\n",
      "[Epoch 3/5] [Batch 296/360] [D loss: 0.250539] [G loss: 3.577994] time: 0:02:45.007630\n",
      "(30, 32, 32, 3)\n",
      "0.89392424\n",
      "[Epoch 3/5] [Batch 297/360] [D loss: 0.206149] [G loss: 3.540237] time: 0:02:45.122293\n",
      "(30, 32, 32, 3)\n",
      "0.86162823\n",
      "[Epoch 3/5] [Batch 298/360] [D loss: 0.183659] [G loss: 3.303968] time: 0:02:45.226048\n",
      "(30, 32, 32, 3)\n",
      "0.90256613\n",
      "[Epoch 3/5] [Batch 299/360] [D loss: 0.207058] [G loss: 3.540019] time: 0:02:45.333950\n",
      "(30, 32, 32, 3)\n",
      "0.9136181\n",
      "[Epoch 3/5] [Batch 300/360] [D loss: 0.218157] [G loss: 3.436287] time: 0:02:45.439353\n",
      "(30, 32, 32, 3)\n",
      "0.91817355\n",
      "[Epoch 3/5] [Batch 301/360] [D loss: 0.186463] [G loss: 3.644123] time: 0:02:45.551310\n",
      "(30, 32, 32, 3)\n",
      "0.9131735\n",
      "[Epoch 3/5] [Batch 302/360] [D loss: 0.233514] [G loss: 3.580729] time: 0:02:45.666360\n",
      "(30, 32, 32, 3)\n",
      "0.9184611\n",
      "[Epoch 3/5] [Batch 303/360] [D loss: 0.182222] [G loss: 3.372558] time: 0:02:45.775743\n",
      "(30, 32, 32, 3)\n",
      "0.8978262\n",
      "[Epoch 3/5] [Batch 304/360] [D loss: 0.252203] [G loss: 3.453578] time: 0:02:45.878904\n",
      "(30, 32, 32, 3)\n",
      "0.9119771\n",
      "[Epoch 3/5] [Batch 305/360] [D loss: 0.189826] [G loss: 3.450900] time: 0:02:45.992411\n",
      "(30, 32, 32, 3)\n",
      "0.9318846\n",
      "[Epoch 3/5] [Batch 306/360] [D loss: 0.270478] [G loss: 3.218302] time: 0:02:46.101513\n",
      "(30, 32, 32, 3)\n",
      "0.8590986\n",
      "[Epoch 3/5] [Batch 307/360] [D loss: 0.160064] [G loss: 3.346297] time: 0:02:46.207070\n",
      "(30, 32, 32, 3)\n",
      "0.87852114\n",
      "[Epoch 3/5] [Batch 308/360] [D loss: 0.267444] [G loss: 2.971592] time: 0:02:46.315163\n",
      "(30, 32, 32, 3)\n",
      "0.93313265\n",
      "[Epoch 3/5] [Batch 309/360] [D loss: 0.147844] [G loss: 3.568908] time: 0:02:46.423826\n",
      "(30, 32, 32, 3)\n",
      "0.8666835\n",
      "[Epoch 3/5] [Batch 310/360] [D loss: 0.216963] [G loss: 3.163456] time: 0:02:46.535847\n",
      "(30, 32, 32, 3)\n",
      "0.9559024\n",
      "[Epoch 3/5] [Batch 311/360] [D loss: 0.187895] [G loss: 3.496797] time: 0:02:46.644775\n",
      "(30, 32, 32, 3)\n",
      "0.8868089\n",
      "[Epoch 3/5] [Batch 312/360] [D loss: 0.297675] [G loss: 3.037389] time: 0:02:46.748784\n",
      "(30, 32, 32, 3)\n",
      "0.86977905\n",
      "[Epoch 3/5] [Batch 313/360] [D loss: 0.183470] [G loss: 3.586952] time: 0:02:46.855124\n",
      "(30, 32, 32, 3)\n",
      "0.857402\n",
      "[Epoch 3/5] [Batch 314/360] [D loss: 0.280777] [G loss: 3.103670] time: 0:02:46.958654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9290426\n",
      "[Epoch 3/5] [Batch 315/360] [D loss: 0.163855] [G loss: 3.555502] time: 0:02:47.083543\n",
      "(30, 32, 32, 3)\n",
      "0.9024234\n",
      "[Epoch 3/5] [Batch 316/360] [D loss: 0.306537] [G loss: 3.258541] time: 0:02:47.188241\n",
      "(30, 32, 32, 3)\n",
      "0.9187432\n",
      "[Epoch 3/5] [Batch 317/360] [D loss: 0.190998] [G loss: 4.072245] time: 0:02:47.295129\n",
      "(30, 32, 32, 3)\n",
      "0.9204549\n",
      "[Epoch 3/5] [Batch 318/360] [D loss: 0.244582] [G loss: 3.140960] time: 0:02:47.400321\n",
      "(30, 32, 32, 3)\n",
      "0.9204545\n",
      "[Epoch 3/5] [Batch 319/360] [D loss: 0.159829] [G loss: 3.562530] time: 0:02:47.510701\n",
      "(30, 32, 32, 3)\n",
      "0.89296407\n",
      "[Epoch 3/5] [Batch 320/360] [D loss: 0.246392] [G loss: 3.085676] time: 0:02:47.615314\n",
      "(30, 32, 32, 3)\n",
      "0.8720135\n",
      "[Epoch 3/5] [Batch 321/360] [D loss: 0.182704] [G loss: 3.372035] time: 0:02:47.724048\n",
      "(30, 32, 32, 3)\n",
      "0.913249\n",
      "[Epoch 3/5] [Batch 322/360] [D loss: 0.262099] [G loss: 3.478979] time: 0:02:47.827706\n",
      "(30, 32, 32, 3)\n",
      "0.91655856\n",
      "[Epoch 3/5] [Batch 323/360] [D loss: 0.142541] [G loss: 3.492007] time: 0:02:47.935504\n",
      "(30, 32, 32, 3)\n",
      "0.89542335\n",
      "[Epoch 3/5] [Batch 324/360] [D loss: 0.215550] [G loss: 3.788436] time: 0:02:48.043228\n",
      "(30, 32, 32, 3)\n",
      "0.88534975\n",
      "[Epoch 3/5] [Batch 325/360] [D loss: 0.183558] [G loss: 3.588491] time: 0:02:48.154634\n",
      "(30, 32, 32, 3)\n",
      "0.9446974\n",
      "[Epoch 3/5] [Batch 326/360] [D loss: 0.316609] [G loss: 3.306917] time: 0:02:48.258590\n",
      "(30, 32, 32, 3)\n",
      "0.8963032\n",
      "[Epoch 3/5] [Batch 327/360] [D loss: 0.196409] [G loss: 3.369411] time: 0:02:48.367023\n",
      "(30, 32, 32, 3)\n",
      "0.89656067\n",
      "[Epoch 3/5] [Batch 328/360] [D loss: 0.276392] [G loss: 3.295081] time: 0:02:48.478995\n",
      "(30, 32, 32, 3)\n",
      "0.89571685\n",
      "[Epoch 3/5] [Batch 329/360] [D loss: 0.208664] [G loss: 3.390370] time: 0:02:48.586220\n",
      "(30, 32, 32, 3)\n",
      "0.9550082\n",
      "[Epoch 3/5] [Batch 330/360] [D loss: 0.238549] [G loss: 3.475997] time: 0:02:48.695054\n",
      "(30, 32, 32, 3)\n",
      "0.8949237\n",
      "[Epoch 3/5] [Batch 331/360] [D loss: 0.199951] [G loss: 3.138918] time: 0:02:48.803749\n",
      "(30, 32, 32, 3)\n",
      "0.96310157\n",
      "[Epoch 3/5] [Batch 332/360] [D loss: 0.261045] [G loss: 3.353570] time: 0:02:48.907732\n",
      "(30, 32, 32, 3)\n",
      "0.92227393\n",
      "[Epoch 3/5] [Batch 333/360] [D loss: 0.180808] [G loss: 3.076712] time: 0:02:49.018289\n",
      "(30, 32, 32, 3)\n",
      "0.91806513\n",
      "[Epoch 3/5] [Batch 334/360] [D loss: 0.235222] [G loss: 3.730687] time: 0:02:49.128058\n",
      "(30, 32, 32, 3)\n",
      "0.8967081\n",
      "[Epoch 3/5] [Batch 335/360] [D loss: 0.216217] [G loss: 3.159031] time: 0:02:49.235211\n",
      "(30, 32, 32, 3)\n",
      "0.8935078\n",
      "[Epoch 3/5] [Batch 336/360] [D loss: 0.241146] [G loss: 3.033058] time: 0:02:49.339322\n",
      "(30, 32, 32, 3)\n",
      "0.91627073\n",
      "[Epoch 3/5] [Batch 337/360] [D loss: 0.205418] [G loss: 3.350399] time: 0:02:49.455772\n",
      "(30, 32, 32, 3)\n",
      "0.8820793\n",
      "[Epoch 3/5] [Batch 338/360] [D loss: 0.242539] [G loss: 2.943088] time: 0:02:49.567775\n",
      "(30, 32, 32, 3)\n",
      "0.9080794\n",
      "[Epoch 3/5] [Batch 339/360] [D loss: 0.238254] [G loss: 3.734365] time: 0:02:49.675016\n",
      "(30, 32, 32, 3)\n",
      "0.9581464\n",
      "[Epoch 3/5] [Batch 340/360] [D loss: 0.217673] [G loss: 3.376003] time: 0:02:49.784851\n",
      "(30, 32, 32, 3)\n",
      "0.8329043\n",
      "[Epoch 3/5] [Batch 341/360] [D loss: 0.171937] [G loss: 3.165295] time: 0:02:49.892177\n",
      "(30, 32, 32, 3)\n",
      "0.9092188\n",
      "[Epoch 3/5] [Batch 342/360] [D loss: 0.219459] [G loss: 3.085252] time: 0:02:49.999098\n",
      "(30, 32, 32, 3)\n",
      "0.87054795\n",
      "[Epoch 3/5] [Batch 343/360] [D loss: 0.210412] [G loss: 3.622287] time: 0:02:50.109684\n",
      "(30, 32, 32, 3)\n",
      "0.924794\n",
      "[Epoch 3/5] [Batch 344/360] [D loss: 0.252922] [G loss: 3.112203] time: 0:02:50.218871\n",
      "(30, 32, 32, 3)\n",
      "0.884614\n",
      "[Epoch 3/5] [Batch 345/360] [D loss: 0.204772] [G loss: 3.266611] time: 0:02:50.325434\n",
      "(30, 32, 32, 3)\n",
      "0.8679766\n",
      "[Epoch 3/5] [Batch 346/360] [D loss: 0.287618] [G loss: 3.195261] time: 0:02:50.440639\n",
      "(30, 32, 32, 3)\n",
      "0.90652436\n",
      "[Epoch 3/5] [Batch 347/360] [D loss: 0.180010] [G loss: 3.283681] time: 0:02:50.550262\n",
      "(30, 32, 32, 3)\n",
      "0.9690952\n",
      "[Epoch 3/5] [Batch 348/360] [D loss: 0.308473] [G loss: 2.946790] time: 0:02:50.655268\n",
      "(30, 32, 32, 3)\n",
      "0.9473336\n",
      "[Epoch 3/5] [Batch 349/360] [D loss: 0.198762] [G loss: 3.084041] time: 0:02:50.762568\n",
      "(30, 32, 32, 3)\n",
      "0.899416\n",
      "[Epoch 3/5] [Batch 350/360] [D loss: 0.330078] [G loss: 3.072754] time: 0:02:50.866299\n",
      "(30, 32, 32, 3)\n",
      "0.90205365\n",
      "[Epoch 3/5] [Batch 351/360] [D loss: 0.164054] [G loss: 3.201758] time: 0:02:50.976363\n",
      "(30, 32, 32, 3)\n",
      "0.8946113\n",
      "[Epoch 3/5] [Batch 352/360] [D loss: 0.281934] [G loss: 2.855461] time: 0:02:51.081065\n",
      "(30, 32, 32, 3)\n",
      "0.88541013\n",
      "[Epoch 3/5] [Batch 353/360] [D loss: 0.177797] [G loss: 3.219964] time: 0:02:51.188446\n",
      "(30, 32, 32, 3)\n",
      "0.8825719\n",
      "[Epoch 3/5] [Batch 354/360] [D loss: 0.277042] [G loss: 2.962168] time: 0:02:51.293702\n",
      "(30, 32, 32, 3)\n",
      "0.91917825\n",
      "[Epoch 3/5] [Batch 355/360] [D loss: 0.180380] [G loss: 3.378193] time: 0:02:51.404478\n",
      "(30, 32, 32, 3)\n",
      "0.89529467\n",
      "[Epoch 3/5] [Batch 356/360] [D loss: 0.381262] [G loss: 3.028489] time: 0:02:51.509169\n",
      "(30, 32, 32, 3)\n",
      "0.89848524\n",
      "[Epoch 3/5] [Batch 357/360] [D loss: 0.180557] [G loss: 3.500576] time: 0:02:51.619854\n",
      "(30, 32, 32, 3)\n",
      "0.8739398\n",
      "[Epoch 3/5] [Batch 358/360] [D loss: 0.319052] [G loss: 2.800673] time: 0:02:51.725045\n",
      "(30, 32, 32, 3)\n",
      "0.92797357\n",
      "[Epoch 3/5] [Batch 359/360] [D loss: 0.171588] [G loss: 3.476169] time: 0:02:51.834717\n",
      "(30, 32, 32, 3)\n",
      "0.8799693\n",
      "[Epoch 4/5] [Batch 0/360] [D loss: 0.245189] [G loss: 2.704445] time: 0:02:51.941540\n",
      "(30, 32, 32, 3)\n",
      "0.9580315\n",
      "[Epoch 4/5] [Batch 2/360] [D loss: 0.176766] [G loss: 3.481116] time: 0:02:52.059245\n",
      "(30, 32, 32, 3)\n",
      "0.9599471\n",
      "[Epoch 4/5] [Batch 3/360] [D loss: 0.261532] [G loss: 3.285398] time: 0:02:52.164655\n",
      "(30, 32, 32, 3)\n",
      "0.9063956\n",
      "[Epoch 4/5] [Batch 4/360] [D loss: 0.179990] [G loss: 3.134023] time: 0:02:52.271294\n",
      "(30, 32, 32, 3)\n",
      "0.89407414\n",
      "[Epoch 4/5] [Batch 5/360] [D loss: 0.237093] [G loss: 3.217656] time: 0:02:52.378256\n",
      "(30, 32, 32, 3)\n",
      "0.89861107\n",
      "[Epoch 4/5] [Batch 6/360] [D loss: 0.229273] [G loss: 3.042057] time: 0:02:52.484183\n",
      "(30, 32, 32, 3)\n",
      "0.89707613\n",
      "[Epoch 4/5] [Batch 7/360] [D loss: 0.238938] [G loss: 3.178536] time: 0:02:52.593159\n",
      "(30, 32, 32, 3)\n",
      "0.9426848\n",
      "[Epoch 4/5] [Batch 8/360] [D loss: 0.237763] [G loss: 3.028810] time: 0:02:52.698572\n",
      "(30, 32, 32, 3)\n",
      "0.92080563\n",
      "[Epoch 4/5] [Batch 9/360] [D loss: 0.192299] [G loss: 2.886115] time: 0:02:52.804725\n",
      "(30, 32, 32, 3)\n",
      "0.9027769\n",
      "[Epoch 4/5] [Batch 10/360] [D loss: 0.239321] [G loss: 2.976269] time: 0:02:52.914378\n",
      "(30, 32, 32, 3)\n",
      "0.9184968\n",
      "[Epoch 4/5] [Batch 11/360] [D loss: 0.216339] [G loss: 3.062206] time: 0:02:53.019249\n",
      "(30, 32, 32, 3)\n",
      "0.8976305\n",
      "[Epoch 4/5] [Batch 12/360] [D loss: 0.289565] [G loss: 3.077707] time: 0:02:53.133288\n",
      "(30, 32, 32, 3)\n",
      "0.8847696\n",
      "[Epoch 4/5] [Batch 13/360] [D loss: 0.216435] [G loss: 3.364232] time: 0:02:53.238631\n",
      "(30, 32, 32, 3)\n",
      "0.8912807\n",
      "[Epoch 4/5] [Batch 14/360] [D loss: 0.252717] [G loss: 3.059753] time: 0:02:53.347817\n",
      "(30, 32, 32, 3)\n",
      "0.9107645\n",
      "[Epoch 4/5] [Batch 15/360] [D loss: 0.203693] [G loss: 3.239331] time: 0:02:53.456669\n",
      "(30, 32, 32, 3)\n",
      "0.84959936\n",
      "[Epoch 4/5] [Batch 16/360] [D loss: 0.249379] [G loss: 3.164465] time: 0:02:53.565979\n",
      "(30, 32, 32, 3)\n",
      "0.88626033\n",
      "[Epoch 4/5] [Batch 17/360] [D loss: 0.224646] [G loss: 3.477336] time: 0:02:53.670882\n",
      "(30, 32, 32, 3)\n",
      "0.9312361\n",
      "[Epoch 4/5] [Batch 18/360] [D loss: 0.256568] [G loss: 3.229412] time: 0:02:53.778890\n",
      "(30, 32, 32, 3)\n",
      "0.93407273\n",
      "[Epoch 4/5] [Batch 19/360] [D loss: 0.200510] [G loss: 3.485975] time: 0:02:53.886465\n",
      "(30, 32, 32, 3)\n",
      "0.91044474\n",
      "[Epoch 4/5] [Batch 20/360] [D loss: 0.262368] [G loss: 3.037082] time: 0:02:53.993033\n",
      "(30, 32, 32, 3)\n",
      "0.91800123\n",
      "[Epoch 4/5] [Batch 21/360] [D loss: 0.196892] [G loss: 2.950861] time: 0:02:54.096183\n",
      "(30, 32, 32, 3)\n",
      "0.90477306\n",
      "[Epoch 4/5] [Batch 22/360] [D loss: 0.190381] [G loss: 3.411593] time: 0:02:54.203532\n",
      "(30, 32, 32, 3)\n",
      "0.92153054\n",
      "[Epoch 4/5] [Batch 23/360] [D loss: 0.222779] [G loss: 2.968196] time: 0:02:54.312778\n",
      "(30, 32, 32, 3)\n",
      "0.9222393\n",
      "[Epoch 4/5] [Batch 24/360] [D loss: 0.173021] [G loss: 2.882527] time: 0:02:54.420617\n",
      "(30, 32, 32, 3)\n",
      "0.9059836\n",
      "[Epoch 4/5] [Batch 25/360] [D loss: 0.255870] [G loss: 2.986640] time: 0:02:54.523910\n",
      "(30, 32, 32, 3)\n",
      "0.9078639\n",
      "[Epoch 4/5] [Batch 26/360] [D loss: 0.227914] [G loss: 2.751229] time: 0:02:54.641421\n",
      "(30, 32, 32, 3)\n",
      "0.88863397\n",
      "[Epoch 4/5] [Batch 27/360] [D loss: 0.254366] [G loss: 3.096581] time: 0:02:54.745212\n",
      "(30, 32, 32, 3)\n",
      "0.8950718\n",
      "[Epoch 4/5] [Batch 28/360] [D loss: 0.237877] [G loss: 2.686418] time: 0:02:54.856483\n",
      "(30, 32, 32, 3)\n",
      "0.89916366\n",
      "[Epoch 4/5] [Batch 29/360] [D loss: 0.225427] [G loss: 3.247130] time: 0:02:54.963042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.92926425\n",
      "[Epoch 4/5] [Batch 30/360] [D loss: 0.255281] [G loss: 2.974552] time: 0:02:55.076305\n",
      "(30, 32, 32, 3)\n",
      "0.88621205\n",
      "[Epoch 4/5] [Batch 31/360] [D loss: 0.216594] [G loss: 2.841325] time: 0:02:55.180967\n",
      "(30, 32, 32, 3)\n",
      "0.9148225\n",
      "[Epoch 4/5] [Batch 32/360] [D loss: 0.318035] [G loss: 3.178226] time: 0:02:55.291178\n",
      "(30, 32, 32, 3)\n",
      "0.93970627\n",
      "[Epoch 4/5] [Batch 33/360] [D loss: 0.173133] [G loss: 3.061610] time: 0:02:55.400346\n",
      "(30, 32, 32, 3)\n",
      "0.91145796\n",
      "[Epoch 4/5] [Batch 34/360] [D loss: 0.288793] [G loss: 2.808115] time: 0:02:55.508394\n",
      "(30, 32, 32, 3)\n",
      "0.94191617\n",
      "[Epoch 4/5] [Batch 35/360] [D loss: 0.185525] [G loss: 3.320125] time: 0:02:55.612345\n",
      "(30, 32, 32, 3)\n",
      "0.9235103\n",
      "[Epoch 4/5] [Batch 36/360] [D loss: 0.272858] [G loss: 3.048848] time: 0:02:55.719385\n",
      "(30, 32, 32, 3)\n",
      "0.8834438\n",
      "[Epoch 4/5] [Batch 37/360] [D loss: 0.208488] [G loss: 3.314281] time: 0:02:55.837881\n",
      "(30, 32, 32, 3)\n",
      "0.9147837\n",
      "[Epoch 4/5] [Batch 38/360] [D loss: 0.280720] [G loss: 3.010859] time: 0:02:55.944359\n",
      "(30, 32, 32, 3)\n",
      "0.9324476\n",
      "[Epoch 4/5] [Batch 39/360] [D loss: 0.238384] [G loss: 2.927506] time: 0:02:56.051020\n",
      "(30, 32, 32, 3)\n",
      "0.8933756\n",
      "[Epoch 4/5] [Batch 40/360] [D loss: 0.218737] [G loss: 2.970556] time: 0:02:56.157128\n",
      "(30, 32, 32, 3)\n",
      "0.89529544\n",
      "[Epoch 4/5] [Batch 41/360] [D loss: 0.217523] [G loss: 2.872069] time: 0:02:56.267236\n",
      "(30, 32, 32, 3)\n",
      "0.89125633\n",
      "[Epoch 4/5] [Batch 42/360] [D loss: 0.247634] [G loss: 3.015817] time: 0:02:56.373243\n",
      "(30, 32, 32, 3)\n",
      "0.86572886\n",
      "[Epoch 4/5] [Batch 43/360] [D loss: 0.191534] [G loss: 2.864137] time: 0:02:56.481196\n",
      "(30, 32, 32, 3)\n",
      "0.94666773\n",
      "[Epoch 4/5] [Batch 44/360] [D loss: 0.247068] [G loss: 3.068719] time: 0:02:56.588389\n",
      "(30, 32, 32, 3)\n",
      "0.9503501\n",
      "[Epoch 4/5] [Batch 45/360] [D loss: 0.223808] [G loss: 2.848242] time: 0:02:56.692551\n",
      "(30, 32, 32, 3)\n",
      "0.94882756\n",
      "[Epoch 4/5] [Batch 46/360] [D loss: 0.250757] [G loss: 2.771921] time: 0:02:56.808470\n",
      "(30, 32, 32, 3)\n",
      "0.8923084\n",
      "[Epoch 4/5] [Batch 47/360] [D loss: 0.256181] [G loss: 2.824344] time: 0:02:56.913395\n",
      "(30, 32, 32, 3)\n",
      "0.8893284\n",
      "[Epoch 4/5] [Batch 48/360] [D loss: 0.233840] [G loss: 2.791036] time: 0:02:57.021578\n",
      "(30, 32, 32, 3)\n",
      "0.92646474\n",
      "[Epoch 4/5] [Batch 49/360] [D loss: 0.241721] [G loss: 2.690891] time: 0:02:57.126239\n",
      "(30, 32, 32, 3)\n",
      "0.90495044\n",
      "[Epoch 4/5] [Batch 50/360] [D loss: 0.229205] [G loss: 2.760850] time: 0:02:57.236177\n",
      "(30, 32, 32, 3)\n",
      "0.8937068\n",
      "[Epoch 4/5] [Batch 51/360] [D loss: 0.248496] [G loss: 2.809704] time: 0:02:57.340702\n",
      "(30, 32, 32, 3)\n",
      "0.9451361\n",
      "[Epoch 4/5] [Batch 52/360] [D loss: 0.187513] [G loss: 2.820610] time: 0:02:57.447845\n",
      "(30, 32, 32, 3)\n",
      "0.96819085\n",
      "[Epoch 4/5] [Batch 53/360] [D loss: 0.288056] [G loss: 2.633271] time: 0:02:57.554289\n",
      "(30, 32, 32, 3)\n",
      "0.91936207\n",
      "[Epoch 4/5] [Batch 54/360] [D loss: 0.186192] [G loss: 3.264969] time: 0:02:57.661684\n",
      "(30, 32, 32, 3)\n",
      "0.92458564\n",
      "[Epoch 4/5] [Batch 55/360] [D loss: 0.301518] [G loss: 2.744220] time: 0:02:57.770837\n",
      "(30, 32, 32, 3)\n",
      "0.9202552\n",
      "[Epoch 4/5] [Batch 56/360] [D loss: 0.174874] [G loss: 3.105175] time: 0:02:57.875111\n",
      "(30, 32, 32, 3)\n",
      "0.90305024\n",
      "[Epoch 4/5] [Batch 57/360] [D loss: 0.293350] [G loss: 2.645561] time: 0:02:57.991329\n",
      "(30, 32, 32, 3)\n",
      "0.91927236\n",
      "[Epoch 4/5] [Batch 58/360] [D loss: 0.188329] [G loss: 2.992649] time: 0:02:58.099634\n",
      "(30, 32, 32, 3)\n",
      "0.9080151\n",
      "[Epoch 4/5] [Batch 59/360] [D loss: 0.310226] [G loss: 2.783584] time: 0:02:58.207106\n",
      "(30, 32, 32, 3)\n",
      "0.9367649\n",
      "[Epoch 4/5] [Batch 60/360] [D loss: 0.194544] [G loss: 3.683389] time: 0:02:58.315252\n",
      "(30, 32, 32, 3)\n",
      "0.8889176\n",
      "[Epoch 4/5] [Batch 61/360] [D loss: 0.274543] [G loss: 2.820273] time: 0:02:58.427282\n",
      "(30, 32, 32, 3)\n",
      "0.931777\n",
      "[Epoch 4/5] [Batch 62/360] [D loss: 0.173231] [G loss: 3.167946] time: 0:02:58.538970\n",
      "(30, 32, 32, 3)\n",
      "0.90554404\n",
      "[Epoch 4/5] [Batch 63/360] [D loss: 0.236019] [G loss: 2.786804] time: 0:02:58.643413\n",
      "(30, 32, 32, 3)\n",
      "0.8888803\n",
      "[Epoch 4/5] [Batch 64/360] [D loss: 0.184553] [G loss: 3.210278] time: 0:02:58.836947\n",
      "(30, 32, 32, 3)\n",
      "0.91313845\n",
      "[Epoch 4/5] [Batch 65/360] [D loss: 0.212260] [G loss: 2.938151] time: 0:02:58.942231\n",
      "(30, 32, 32, 3)\n",
      "0.9031573\n",
      "[Epoch 4/5] [Batch 66/360] [D loss: 0.265984] [G loss: 2.705824] time: 0:02:59.051314\n",
      "(30, 32, 32, 3)\n",
      "0.9107612\n",
      "[Epoch 4/5] [Batch 67/360] [D loss: 0.245836] [G loss: 3.513462] time: 0:02:59.155677\n",
      "(30, 32, 32, 3)\n",
      "0.8823695\n",
      "[Epoch 4/5] [Batch 68/360] [D loss: 0.244452] [G loss: 2.982065] time: 0:02:59.262269\n",
      "(30, 32, 32, 3)\n",
      "0.9119876\n",
      "[Epoch 4/5] [Batch 69/360] [D loss: 0.249044] [G loss: 2.900303] time: 0:02:59.371557\n",
      "(30, 32, 32, 3)\n",
      "0.9254449\n",
      "[Epoch 4/5] [Batch 70/360] [D loss: 0.257998] [G loss: 3.260454] time: 0:02:59.479972\n",
      "(30, 32, 32, 3)\n",
      "0.9195152\n",
      "[Epoch 4/5] [Batch 71/360] [D loss: 0.256676] [G loss: 2.999642] time: 0:02:59.584949\n",
      "(30, 32, 32, 3)\n",
      "0.8876486\n",
      "[Epoch 4/5] [Batch 72/360] [D loss: 0.217431] [G loss: 2.653589] time: 0:02:59.692905\n",
      "(30, 32, 32, 3)\n",
      "0.9061782\n",
      "[Epoch 4/5] [Batch 73/360] [D loss: 0.233753] [G loss: 2.897256] time: 0:02:59.799690\n",
      "(30, 32, 32, 3)\n",
      "0.8962807\n",
      "[Epoch 4/5] [Batch 74/360] [D loss: 0.217976] [G loss: 2.946965] time: 0:02:59.908876\n",
      "(30, 32, 32, 3)\n",
      "0.89922255\n",
      "[Epoch 4/5] [Batch 75/360] [D loss: 0.250863] [G loss: 2.738672] time: 0:03:00.016995\n",
      "(30, 32, 32, 3)\n",
      "0.90575725\n",
      "[Epoch 4/5] [Batch 76/360] [D loss: 0.247197] [G loss: 2.820180] time: 0:03:00.131665\n",
      "(30, 32, 32, 3)\n",
      "0.90903586\n",
      "[Epoch 4/5] [Batch 77/360] [D loss: 0.274396] [G loss: 2.791032] time: 0:03:00.236441\n",
      "(30, 32, 32, 3)\n",
      "0.89705783\n",
      "[Epoch 4/5] [Batch 78/360] [D loss: 0.220567] [G loss: 2.639753] time: 0:03:00.354114\n",
      "(30, 32, 32, 3)\n",
      "0.9203165\n",
      "[Epoch 4/5] [Batch 79/360] [D loss: 0.266700] [G loss: 2.701725] time: 0:03:00.462438\n",
      "(30, 32, 32, 3)\n",
      "0.9181867\n",
      "[Epoch 4/5] [Batch 80/360] [D loss: 0.242882] [G loss: 2.632701] time: 0:03:00.570361\n",
      "(30, 32, 32, 3)\n",
      "0.89488107\n",
      "[Epoch 4/5] [Batch 81/360] [D loss: 0.244695] [G loss: 2.717098] time: 0:03:00.674883\n",
      "(30, 32, 32, 3)\n",
      "0.9339567\n",
      "[Epoch 4/5] [Batch 82/360] [D loss: 0.239949] [G loss: 2.475745] time: 0:03:00.786951\n",
      "(30, 32, 32, 3)\n",
      "0.8892541\n",
      "[Epoch 4/5] [Batch 83/360] [D loss: 0.234723] [G loss: 2.611413] time: 0:03:00.890502\n",
      "(30, 32, 32, 3)\n",
      "0.8919564\n",
      "[Epoch 4/5] [Batch 84/360] [D loss: 0.236440] [G loss: 2.770617] time: 0:03:00.999506\n",
      "(30, 32, 32, 3)\n",
      "0.9580805\n",
      "[Epoch 4/5] [Batch 85/360] [D loss: 0.265988] [G loss: 2.940478] time: 0:03:01.104913\n",
      "(30, 32, 32, 3)\n",
      "0.92731446\n",
      "[Epoch 4/5] [Batch 86/360] [D loss: 0.226611] [G loss: 3.135377] time: 0:03:01.215977\n",
      "(30, 32, 32, 3)\n",
      "0.9305604\n",
      "[Epoch 4/5] [Batch 87/360] [D loss: 0.273873] [G loss: 2.668322] time: 0:03:01.326250\n",
      "(30, 32, 32, 3)\n",
      "0.9060464\n",
      "[Epoch 4/5] [Batch 88/360] [D loss: 0.225011] [G loss: 3.008168] time: 0:03:01.436210\n",
      "(30, 32, 32, 3)\n",
      "0.94528216\n",
      "[Epoch 4/5] [Batch 89/360] [D loss: 0.250319] [G loss: 2.695819] time: 0:03:01.541997\n",
      "(30, 32, 32, 3)\n",
      "0.9344911\n",
      "[Epoch 4/5] [Batch 90/360] [D loss: 0.207416] [G loss: 2.649031] time: 0:03:01.651493\n",
      "(30, 32, 32, 3)\n",
      "0.8733943\n",
      "[Epoch 4/5] [Batch 91/360] [D loss: 0.231160] [G loss: 2.921349] time: 0:03:01.759609\n",
      "(30, 32, 32, 3)\n",
      "0.9256093\n",
      "[Epoch 4/5] [Batch 92/360] [D loss: 0.268522] [G loss: 2.687038] time: 0:03:01.868578\n",
      "(30, 32, 32, 3)\n",
      "0.92152184\n",
      "[Epoch 4/5] [Batch 93/360] [D loss: 0.237392] [G loss: 2.830157] time: 0:03:01.975134\n",
      "(30, 32, 32, 3)\n",
      "0.8899862\n",
      "[Epoch 4/5] [Batch 94/360] [D loss: 0.263639] [G loss: 2.615363] time: 0:03:02.081987\n",
      "(30, 32, 32, 3)\n",
      "0.9274228\n",
      "[Epoch 4/5] [Batch 95/360] [D loss: 0.246232] [G loss: 2.588539] time: 0:03:02.188040\n",
      "(30, 32, 32, 3)\n",
      "0.9038744\n",
      "[Epoch 4/5] [Batch 96/360] [D loss: 0.261347] [G loss: 2.679093] time: 0:03:02.299019\n",
      "(30, 32, 32, 3)\n",
      "0.87489814\n",
      "[Epoch 4/5] [Batch 97/360] [D loss: 0.200309] [G loss: 2.809468] time: 0:03:02.404748\n",
      "(30, 32, 32, 3)\n",
      "0.9197777\n",
      "[Epoch 4/5] [Batch 98/360] [D loss: 0.265996] [G loss: 2.580592] time: 0:03:02.516798\n",
      "(30, 32, 32, 3)\n",
      "0.9621814\n",
      "[Epoch 4/5] [Batch 99/360] [D loss: 0.226038] [G loss: 2.752854] time: 0:03:02.620625\n",
      "(30, 32, 32, 3)\n",
      "0.93524784\n",
      "[Epoch 4/5] [Batch 100/360] [D loss: 0.249840] [G loss: 2.472850] time: 0:03:02.729245\n",
      "(30, 32, 32, 3)\n",
      "0.8827804\n",
      "[Epoch 4/5] [Batch 101/360] [D loss: 0.227156] [G loss: 3.131363] time: 0:03:02.838044\n",
      "(30, 32, 32, 3)\n",
      "0.9089444\n",
      "[Epoch 4/5] [Batch 102/360] [D loss: 0.259572] [G loss: 2.805963] time: 0:03:02.944927\n",
      "(30, 32, 32, 3)\n",
      "0.92531633\n",
      "[Epoch 4/5] [Batch 103/360] [D loss: 0.219715] [G loss: 3.015427] time: 0:03:03.055124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9489948\n",
      "[Epoch 4/5] [Batch 104/360] [D loss: 0.266585] [G loss: 3.137023] time: 0:03:03.170226\n",
      "(30, 32, 32, 3)\n",
      "0.8973231\n",
      "[Epoch 4/5] [Batch 105/360] [D loss: 0.225490] [G loss: 2.592709] time: 0:03:03.277158\n",
      "(30, 32, 32, 3)\n",
      "0.8557143\n",
      "[Epoch 4/5] [Batch 106/360] [D loss: 0.282883] [G loss: 2.445645] time: 0:03:03.381503\n",
      "(30, 32, 32, 3)\n",
      "0.87870336\n",
      "[Epoch 4/5] [Batch 107/360] [D loss: 0.186522] [G loss: 2.555438] time: 0:03:03.486721\n",
      "(30, 32, 32, 3)\n",
      "0.9176037\n",
      "[Epoch 4/5] [Batch 108/360] [D loss: 0.307527] [G loss: 2.767426] time: 0:03:03.598194\n",
      "(30, 32, 32, 3)\n",
      "0.9032931\n",
      "[Epoch 4/5] [Batch 109/360] [D loss: 0.185502] [G loss: 3.097021] time: 0:03:03.709846\n",
      "(30, 32, 32, 3)\n",
      "0.94184417\n",
      "[Epoch 4/5] [Batch 110/360] [D loss: 0.265737] [G loss: 2.703815] time: 0:03:03.820809\n",
      "(30, 32, 32, 3)\n",
      "0.90995806\n",
      "[Epoch 4/5] [Batch 111/360] [D loss: 0.226576] [G loss: 2.992418] time: 0:03:03.924199\n",
      "(30, 32, 32, 3)\n",
      "0.91611046\n",
      "[Epoch 4/5] [Batch 112/360] [D loss: 0.241522] [G loss: 3.107934] time: 0:03:04.031526\n",
      "(30, 32, 32, 3)\n",
      "0.93299705\n",
      "[Epoch 4/5] [Batch 113/360] [D loss: 0.185282] [G loss: 3.074711] time: 0:03:04.135534\n",
      "(30, 32, 32, 3)\n",
      "0.89219904\n",
      "[Epoch 4/5] [Batch 114/360] [D loss: 0.289573] [G loss: 2.883054] time: 0:03:04.244443\n",
      "(30, 32, 32, 3)\n",
      "0.8675285\n",
      "[Epoch 4/5] [Batch 115/360] [D loss: 0.186571] [G loss: 2.747302] time: 0:03:04.351816\n",
      "(30, 32, 32, 3)\n",
      "0.8993497\n",
      "[Epoch 4/5] [Batch 116/360] [D loss: 0.203285] [G loss: 2.541390] time: 0:03:04.457947\n",
      "(30, 32, 32, 3)\n",
      "0.91540545\n",
      "[Epoch 4/5] [Batch 117/360] [D loss: 0.230775] [G loss: 3.364161] time: 0:03:04.568235\n",
      "(30, 32, 32, 3)\n",
      "0.9131189\n",
      "[Epoch 4/5] [Batch 118/360] [D loss: 0.200801] [G loss: 2.955228] time: 0:03:04.679026\n",
      "(30, 32, 32, 3)\n",
      "0.8805289\n",
      "[Epoch 4/5] [Batch 119/360] [D loss: 0.174611] [G loss: 3.042811] time: 0:03:04.786949\n",
      "(30, 32, 32, 3)\n",
      "0.9100273\n",
      "[Epoch 4/5] [Batch 120/360] [D loss: 0.202049] [G loss: 3.201913] time: 0:03:04.894378\n",
      "(30, 32, 32, 3)\n",
      "0.9409515\n",
      "[Epoch 4/5] [Batch 121/360] [D loss: 0.286152] [G loss: 2.974149] time: 0:03:04.998757\n",
      "(30, 32, 32, 3)\n",
      "0.8942763\n",
      "[Epoch 4/5] [Batch 122/360] [D loss: 0.239833] [G loss: 3.245950] time: 0:03:05.108308\n",
      "(30, 32, 32, 3)\n",
      "0.899184\n",
      "[Epoch 4/5] [Batch 123/360] [D loss: 0.274153] [G loss: 2.758561] time: 0:03:05.217591\n",
      "(30, 32, 32, 3)\n",
      "0.9251736\n",
      "[Epoch 4/5] [Batch 124/360] [D loss: 0.231121] [G loss: 2.977669] time: 0:03:05.323557\n",
      "(30, 32, 32, 3)\n",
      "0.86853534\n",
      "[Epoch 4/5] [Batch 125/360] [D loss: 0.264935] [G loss: 2.710879] time: 0:03:05.427755\n",
      "(30, 32, 32, 3)\n",
      "0.87373614\n",
      "[Epoch 4/5] [Batch 126/360] [D loss: 0.252757] [G loss: 2.699355] time: 0:03:05.535370\n",
      "(30, 32, 32, 3)\n",
      "0.9097025\n",
      "[Epoch 4/5] [Batch 127/360] [D loss: 0.233717] [G loss: 2.753822] time: 0:03:05.642094\n",
      "(30, 32, 32, 3)\n",
      "0.89510584\n",
      "[Epoch 4/5] [Batch 128/360] [D loss: 0.254860] [G loss: 2.880289] time: 0:03:05.749033\n",
      "(30, 32, 32, 3)\n",
      "0.90842694\n",
      "[Epoch 4/5] [Batch 129/360] [D loss: 0.175452] [G loss: 3.465693] time: 0:03:05.854389\n",
      "(30, 32, 32, 3)\n",
      "0.94572324\n",
      "[Epoch 4/5] [Batch 130/360] [D loss: 0.287144] [G loss: 2.807299] time: 0:03:05.964344\n",
      "(30, 32, 32, 3)\n",
      "0.9343441\n",
      "[Epoch 4/5] [Batch 131/360] [D loss: 0.193941] [G loss: 3.220075] time: 0:03:06.072603\n",
      "(30, 32, 32, 3)\n",
      "0.91792536\n",
      "[Epoch 4/5] [Batch 132/360] [D loss: 0.306289] [G loss: 2.789632] time: 0:03:06.181769\n",
      "(30, 32, 32, 3)\n",
      "0.93235916\n",
      "[Epoch 4/5] [Batch 133/360] [D loss: 0.191915] [G loss: 2.986492] time: 0:03:06.286108\n",
      "(30, 32, 32, 3)\n",
      "0.9736461\n",
      "[Epoch 4/5] [Batch 134/360] [D loss: 0.290677] [G loss: 2.581057] time: 0:03:06.394075\n",
      "(30, 32, 32, 3)\n",
      "0.88295037\n",
      "[Epoch 4/5] [Batch 135/360] [D loss: 0.236393] [G loss: 2.546550] time: 0:03:06.499055\n",
      "(30, 32, 32, 3)\n",
      "0.8886152\n",
      "[Epoch 4/5] [Batch 136/360] [D loss: 0.258766] [G loss: 2.808178] time: 0:03:06.606636\n",
      "(30, 32, 32, 3)\n",
      "0.9184637\n",
      "[Epoch 4/5] [Batch 137/360] [D loss: 0.244803] [G loss: 2.795187] time: 0:03:06.710549\n",
      "(30, 32, 32, 3)\n",
      "0.9075737\n",
      "[Epoch 4/5] [Batch 138/360] [D loss: 0.244694] [G loss: 2.589576] time: 0:03:06.818592\n",
      "(30, 32, 32, 3)\n",
      "0.9152467\n",
      "[Epoch 4/5] [Batch 139/360] [D loss: 0.251750] [G loss: 2.629073] time: 0:03:06.923619\n",
      "(30, 32, 32, 3)\n",
      "0.8856535\n",
      "[Epoch 4/5] [Batch 140/360] [D loss: 0.248157] [G loss: 2.650330] time: 0:03:07.029488\n",
      "(30, 32, 32, 3)\n",
      "0.9157638\n",
      "[Epoch 4/5] [Batch 141/360] [D loss: 0.250512] [G loss: 2.737032] time: 0:03:07.136498\n",
      "(30, 32, 32, 3)\n",
      "0.90643984\n",
      "[Epoch 4/5] [Batch 142/360] [D loss: 0.259466] [G loss: 2.745559] time: 0:03:07.242748\n",
      "(30, 32, 32, 3)\n",
      "0.8895113\n",
      "[Epoch 4/5] [Batch 143/360] [D loss: 0.231731] [G loss: 2.718363] time: 0:03:07.347576\n",
      "(30, 32, 32, 3)\n",
      "0.869127\n",
      "[Epoch 4/5] [Batch 144/360] [D loss: 0.253547] [G loss: 2.803300] time: 0:03:07.455198\n",
      "(30, 32, 32, 3)\n",
      "0.9078126\n",
      "[Epoch 4/5] [Batch 145/360] [D loss: 0.212915] [G loss: 2.734566] time: 0:03:07.565790\n",
      "(30, 32, 32, 3)\n",
      "0.9335292\n",
      "[Epoch 4/5] [Batch 146/360] [D loss: 0.262583] [G loss: 2.698817] time: 0:03:07.673306\n",
      "(30, 32, 32, 3)\n",
      "0.971407\n",
      "[Epoch 4/5] [Batch 147/360] [D loss: 0.217726] [G loss: 2.672268] time: 0:03:07.780850\n",
      "(30, 32, 32, 3)\n",
      "0.9002366\n",
      "[Epoch 4/5] [Batch 148/360] [D loss: 0.249266] [G loss: 2.556231] time: 0:03:07.899823\n",
      "(30, 32, 32, 3)\n",
      "0.90213984\n",
      "[Epoch 4/5] [Batch 149/360] [D loss: 0.253926] [G loss: 3.052506] time: 0:03:08.004345\n",
      "(30, 32, 32, 3)\n",
      "0.9254334\n",
      "[Epoch 4/5] [Batch 150/360] [D loss: 0.223562] [G loss: 3.087989] time: 0:03:08.115385\n",
      "(30, 32, 32, 3)\n",
      "0.89760256\n",
      "[Epoch 4/5] [Batch 151/360] [D loss: 0.239016] [G loss: 2.528982] time: 0:03:08.219071\n",
      "(30, 32, 32, 3)\n",
      "0.88600874\n",
      "[Epoch 4/5] [Batch 152/360] [D loss: 0.207318] [G loss: 2.944809] time: 0:03:08.325898\n",
      "(30, 32, 32, 3)\n",
      "0.86691904\n",
      "[Epoch 4/5] [Batch 153/360] [D loss: 0.250703] [G loss: 2.552819] time: 0:03:08.430840\n",
      "(30, 32, 32, 3)\n",
      "0.8975816\n",
      "[Epoch 4/5] [Batch 154/360] [D loss: 0.236246] [G loss: 2.833354] time: 0:03:08.548133\n",
      "(30, 32, 32, 3)\n",
      "0.9023249\n",
      "[Epoch 4/5] [Batch 155/360] [D loss: 0.261783] [G loss: 2.547799] time: 0:03:08.657168\n",
      "(30, 32, 32, 3)\n",
      "0.8994759\n",
      "[Epoch 4/5] [Batch 156/360] [D loss: 0.208859] [G loss: 2.515358] time: 0:03:08.764291\n",
      "(30, 32, 32, 3)\n",
      "0.8412121\n",
      "[Epoch 4/5] [Batch 157/360] [D loss: 0.272742] [G loss: 2.349325] time: 0:03:08.877199\n",
      "(30, 32, 32, 3)\n",
      "0.954422\n",
      "[Epoch 4/5] [Batch 158/360] [D loss: 0.194000] [G loss: 3.116908] time: 0:03:08.988561\n",
      "(30, 32, 32, 3)\n",
      "0.9688571\n",
      "[Epoch 4/5] [Batch 159/360] [D loss: 0.312502] [G loss: 2.451658] time: 0:03:09.095497\n",
      "(30, 32, 32, 3)\n",
      "0.9078469\n",
      "[Epoch 4/5] [Batch 160/360] [D loss: 0.198382] [G loss: 2.785626] time: 0:03:09.204291\n",
      "(30, 32, 32, 3)\n",
      "0.93987864\n",
      "[Epoch 4/5] [Batch 161/360] [D loss: 0.299567] [G loss: 2.349927] time: 0:03:09.307553\n",
      "(30, 32, 32, 3)\n",
      "0.8734927\n",
      "[Epoch 4/5] [Batch 162/360] [D loss: 0.251937] [G loss: 2.822120] time: 0:03:09.415230\n",
      "(30, 32, 32, 3)\n",
      "0.93179625\n",
      "[Epoch 4/5] [Batch 163/360] [D loss: 0.250404] [G loss: 2.395442] time: 0:03:09.523449\n",
      "(30, 32, 32, 3)\n",
      "0.85163206\n",
      "[Epoch 4/5] [Batch 164/360] [D loss: 0.215686] [G loss: 2.508605] time: 0:03:09.630363\n",
      "(30, 32, 32, 3)\n",
      "0.88929576\n",
      "[Epoch 4/5] [Batch 165/360] [D loss: 0.283870] [G loss: 2.580148] time: 0:03:09.735347\n",
      "(30, 32, 32, 3)\n",
      "0.9511698\n",
      "[Epoch 4/5] [Batch 166/360] [D loss: 0.197710] [G loss: 2.663474] time: 0:03:09.841620\n",
      "(30, 32, 32, 3)\n",
      "0.9066933\n",
      "[Epoch 4/5] [Batch 167/360] [D loss: 0.272005] [G loss: 2.630833] time: 0:03:09.945352\n",
      "(30, 32, 32, 3)\n",
      "0.8825515\n",
      "[Epoch 4/5] [Batch 168/360] [D loss: 0.219692] [G loss: 2.666203] time: 0:03:10.055729\n",
      "(30, 32, 32, 3)\n",
      "0.94745874\n",
      "[Epoch 4/5] [Batch 169/360] [D loss: 0.289413] [G loss: 2.522005] time: 0:03:10.160004\n",
      "(30, 32, 32, 3)\n",
      "0.944025\n",
      "[Epoch 4/5] [Batch 170/360] [D loss: 0.268316] [G loss: 2.729442] time: 0:03:10.269394\n",
      "(30, 32, 32, 3)\n",
      "0.90590507\n",
      "[Epoch 4/5] [Batch 171/360] [D loss: 0.259109] [G loss: 2.689392] time: 0:03:10.377721\n",
      "(30, 32, 32, 3)\n",
      "0.92132014\n",
      "[Epoch 4/5] [Batch 172/360] [D loss: 0.248361] [G loss: 2.669713] time: 0:03:10.495588\n",
      "(30, 32, 32, 3)\n",
      "0.9360966\n",
      "[Epoch 4/5] [Batch 173/360] [D loss: 0.245528] [G loss: 2.497136] time: 0:03:10.601625\n",
      "(30, 32, 32, 3)\n",
      "0.96164376\n",
      "[Epoch 4/5] [Batch 174/360] [D loss: 0.207832] [G loss: 2.458802] time: 0:03:10.707717\n",
      "(30, 32, 32, 3)\n",
      "0.9012565\n",
      "[Epoch 4/5] [Batch 175/360] [D loss: 0.260883] [G loss: 2.443646] time: 0:03:10.812173\n",
      "(30, 32, 32, 3)\n",
      "0.9106676\n",
      "[Epoch 4/5] [Batch 176/360] [D loss: 0.234425] [G loss: 2.900646] time: 0:03:10.924824\n",
      "(30, 32, 32, 3)\n",
      "0.9380134\n",
      "[Epoch 4/5] [Batch 177/360] [D loss: 0.247928] [G loss: 2.533592] time: 0:03:11.033254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.916311\n",
      "[Epoch 4/5] [Batch 178/360] [D loss: 0.240217] [G loss: 2.483938] time: 0:03:11.139273\n",
      "(30, 32, 32, 3)\n",
      "0.9758255\n",
      "[Epoch 4/5] [Batch 179/360] [D loss: 0.268129] [G loss: 2.675939] time: 0:03:11.241610\n",
      "(30, 32, 32, 3)\n",
      "0.93183464\n",
      "[Epoch 4/5] [Batch 180/360] [D loss: 0.234047] [G loss: 2.542035] time: 0:03:11.347156\n",
      "(30, 32, 32, 3)\n",
      "0.9060548\n",
      "[Epoch 4/5] [Batch 181/360] [D loss: 0.257742] [G loss: 2.567840] time: 0:03:11.454518\n",
      "(30, 32, 32, 3)\n",
      "0.9470427\n",
      "[Epoch 4/5] [Batch 182/360] [D loss: 0.233213] [G loss: 2.393216] time: 0:03:11.563775\n",
      "(30, 32, 32, 3)\n",
      "0.8934727\n",
      "[Epoch 4/5] [Batch 183/360] [D loss: 0.248610] [G loss: 2.626778] time: 0:03:11.668492\n",
      "(30, 32, 32, 3)\n",
      "0.87358856\n",
      "[Epoch 4/5] [Batch 184/360] [D loss: 0.223634] [G loss: 2.400700] time: 0:03:11.783379\n",
      "(30, 32, 32, 3)\n",
      "0.89474505\n",
      "[Epoch 4/5] [Batch 185/360] [D loss: 0.232941] [G loss: 2.665513] time: 0:03:11.888249\n",
      "(30, 32, 32, 3)\n",
      "0.9051855\n",
      "[Epoch 4/5] [Batch 186/360] [D loss: 0.231050] [G loss: 2.491016] time: 0:03:11.999789\n",
      "(30, 32, 32, 3)\n",
      "0.87579125\n",
      "[Epoch 4/5] [Batch 187/360] [D loss: 0.261706] [G loss: 2.554088] time: 0:03:12.104741\n",
      "(30, 32, 32, 3)\n",
      "0.94551355\n",
      "[Epoch 4/5] [Batch 188/360] [D loss: 0.243102] [G loss: 2.319881] time: 0:03:12.211253\n",
      "(30, 32, 32, 3)\n",
      "0.9063061\n",
      "[Epoch 4/5] [Batch 189/360] [D loss: 0.267479] [G loss: 2.499612] time: 0:03:12.315851\n",
      "(30, 32, 32, 3)\n",
      "0.91598636\n",
      "[Epoch 4/5] [Batch 190/360] [D loss: 0.227544] [G loss: 3.227076] time: 0:03:12.425508\n",
      "(30, 32, 32, 3)\n",
      "0.8913626\n",
      "[Epoch 4/5] [Batch 191/360] [D loss: 0.276617] [G loss: 2.595355] time: 0:03:12.531929\n",
      "(30, 32, 32, 3)\n",
      "0.8796374\n",
      "[Epoch 4/5] [Batch 192/360] [D loss: 0.230590] [G loss: 2.721231] time: 0:03:12.645025\n",
      "(30, 32, 32, 3)\n",
      "0.92536116\n",
      "[Epoch 4/5] [Batch 193/360] [D loss: 0.258119] [G loss: 2.711591] time: 0:03:12.754664\n",
      "(30, 32, 32, 3)\n",
      "0.9459397\n",
      "[Epoch 4/5] [Batch 194/360] [D loss: 0.261406] [G loss: 2.577212] time: 0:03:12.859719\n",
      "(30, 32, 32, 3)\n",
      "0.9131395\n",
      "[Epoch 4/5] [Batch 195/360] [D loss: 0.209279] [G loss: 2.462113] time: 0:03:12.970832\n",
      "(30, 32, 32, 3)\n",
      "0.9024028\n",
      "[Epoch 4/5] [Batch 196/360] [D loss: 0.295076] [G loss: 2.387162] time: 0:03:13.078172\n",
      "(30, 32, 32, 3)\n",
      "0.913476\n",
      "[Epoch 4/5] [Batch 197/360] [D loss: 0.226409] [G loss: 2.563041] time: 0:03:13.182364\n",
      "(30, 32, 32, 3)\n",
      "0.90778255\n",
      "[Epoch 4/5] [Batch 198/360] [D loss: 0.253708] [G loss: 2.240144] time: 0:03:13.288507\n",
      "(30, 32, 32, 3)\n",
      "0.88824946\n",
      "[Epoch 4/5] [Batch 199/360] [D loss: 0.248144] [G loss: 2.679618] time: 0:03:13.395699\n",
      "(30, 32, 32, 3)\n",
      "0.89777\n",
      "[Epoch 4/5] [Batch 200/360] [D loss: 0.214728] [G loss: 2.633519] time: 0:03:13.503037\n",
      "(30, 32, 32, 3)\n",
      "0.9045136\n",
      "[Epoch 4/5] [Batch 201/360] [D loss: 0.258455] [G loss: 2.819082] time: 0:03:13.607235\n",
      "(30, 32, 32, 3)\n",
      "0.93509895\n",
      "[Epoch 4/5] [Batch 202/360] [D loss: 0.241327] [G loss: 2.328466] time: 0:03:13.712838\n",
      "(30, 32, 32, 3)\n",
      "0.91872984\n",
      "[Epoch 4/5] [Batch 203/360] [D loss: 0.234475] [G loss: 2.489479] time: 0:03:13.817430\n",
      "(30, 32, 32, 3)\n",
      "0.8736674\n",
      "[Epoch 4/5] [Batch 204/360] [D loss: 0.254940] [G loss: 2.434296] time: 0:03:13.925680\n",
      "(30, 32, 32, 3)\n",
      "0.94400764\n",
      "[Epoch 4/5] [Batch 205/360] [D loss: 0.251784] [G loss: 2.706240] time: 0:03:14.034773\n",
      "(30, 32, 32, 3)\n",
      "0.86109847\n",
      "[Epoch 4/5] [Batch 206/360] [D loss: 0.249110] [G loss: 2.640643] time: 0:03:14.139718\n",
      "(30, 32, 32, 3)\n",
      "0.87981766\n",
      "[Epoch 4/5] [Batch 207/360] [D loss: 0.245161] [G loss: 3.076366] time: 0:03:14.248671\n",
      "(30, 32, 32, 3)\n",
      "0.9054446\n",
      "[Epoch 4/5] [Batch 208/360] [D loss: 0.259739] [G loss: 2.496989] time: 0:03:14.360689\n",
      "(30, 32, 32, 3)\n",
      "0.9047819\n",
      "[Epoch 4/5] [Batch 209/360] [D loss: 0.235671] [G loss: 2.755762] time: 0:03:14.464864\n",
      "(30, 32, 32, 3)\n",
      "0.9693277\n",
      "[Epoch 4/5] [Batch 210/360] [D loss: 0.268796] [G loss: 2.528673] time: 0:03:14.570843\n",
      "(30, 32, 32, 3)\n",
      "0.9150529\n",
      "[Epoch 4/5] [Batch 211/360] [D loss: 0.232617] [G loss: 2.395588] time: 0:03:14.676323\n",
      "(30, 32, 32, 3)\n",
      "0.8691569\n",
      "[Epoch 4/5] [Batch 212/360] [D loss: 0.290580] [G loss: 2.520237] time: 0:03:14.788242\n",
      "(30, 32, 32, 3)\n",
      "0.8832362\n",
      "[Epoch 4/5] [Batch 213/360] [D loss: 0.210982] [G loss: 2.375954] time: 0:03:14.894880\n",
      "(30, 32, 32, 3)\n",
      "0.93800944\n",
      "[Epoch 4/5] [Batch 214/360] [D loss: 0.278474] [G loss: 2.438389] time: 0:03:15.001641\n",
      "(30, 32, 32, 3)\n",
      "0.90808743\n",
      "[Epoch 4/5] [Batch 215/360] [D loss: 0.223921] [G loss: 2.572100] time: 0:03:15.108553\n",
      "(30, 32, 32, 3)\n",
      "0.8953037\n",
      "[Epoch 4/5] [Batch 216/360] [D loss: 0.252635] [G loss: 2.482502] time: 0:03:15.216340\n",
      "(30, 32, 32, 3)\n",
      "0.90405864\n",
      "[Epoch 4/5] [Batch 217/360] [D loss: 0.247210] [G loss: 2.869133] time: 0:03:15.326351\n",
      "(30, 32, 32, 3)\n",
      "0.90362245\n",
      "[Epoch 4/5] [Batch 218/360] [D loss: 0.274260] [G loss: 2.278475] time: 0:03:15.432105\n",
      "(30, 32, 32, 3)\n",
      "0.9196375\n",
      "[Epoch 4/5] [Batch 219/360] [D loss: 0.228692] [G loss: 2.505885] time: 0:03:15.537345\n",
      "(30, 32, 32, 3)\n",
      "0.9493823\n",
      "[Epoch 4/5] [Batch 220/360] [D loss: 0.279264] [G loss: 2.362471] time: 0:03:15.648671\n",
      "(30, 32, 32, 3)\n",
      "0.89493734\n",
      "[Epoch 4/5] [Batch 221/360] [D loss: 0.221043] [G loss: 2.451756] time: 0:03:15.755271\n",
      "(30, 32, 32, 3)\n",
      "0.93248844\n",
      "[Epoch 4/5] [Batch 222/360] [D loss: 0.268404] [G loss: 2.720790] time: 0:03:15.864320\n",
      "(30, 32, 32, 3)\n",
      "0.93639135\n",
      "[Epoch 4/5] [Batch 223/360] [D loss: 0.210467] [G loss: 2.644019] time: 0:03:15.969281\n",
      "(30, 32, 32, 3)\n",
      "0.91493374\n",
      "[Epoch 4/5] [Batch 224/360] [D loss: 0.292599] [G loss: 2.555916] time: 0:03:16.074210\n",
      "(30, 32, 32, 3)\n",
      "0.87200826\n",
      "[Epoch 4/5] [Batch 225/360] [D loss: 0.218326] [G loss: 2.376394] time: 0:03:16.177918\n",
      "(30, 32, 32, 3)\n",
      "0.8867863\n",
      "[Epoch 4/5] [Batch 226/360] [D loss: 0.268429] [G loss: 2.363576] time: 0:03:16.290948\n",
      "(30, 32, 32, 3)\n",
      "0.87920856\n",
      "[Epoch 4/5] [Batch 227/360] [D loss: 0.228102] [G loss: 2.524015] time: 0:03:16.396850\n",
      "(30, 32, 32, 3)\n",
      "0.88157076\n",
      "[Epoch 4/5] [Batch 228/360] [D loss: 0.247277] [G loss: 2.207493] time: 0:03:16.505291\n",
      "(30, 32, 32, 3)\n",
      "0.92613196\n",
      "[Epoch 4/5] [Batch 229/360] [D loss: 0.269874] [G loss: 2.291426] time: 0:03:16.611170\n",
      "(30, 32, 32, 3)\n",
      "0.9372923\n",
      "[Epoch 4/5] [Batch 230/360] [D loss: 0.234177] [G loss: 2.395165] time: 0:03:16.722655\n",
      "(30, 32, 32, 3)\n",
      "0.88153094\n",
      "[Epoch 4/5] [Batch 231/360] [D loss: 0.243336] [G loss: 2.578001] time: 0:03:16.829600\n",
      "(30, 32, 32, 3)\n",
      "0.8917863\n",
      "[Epoch 4/5] [Batch 232/360] [D loss: 0.243979] [G loss: 2.291027] time: 0:03:16.940767\n",
      "(30, 32, 32, 3)\n",
      "0.97729546\n",
      "[Epoch 4/5] [Batch 233/360] [D loss: 0.246771] [G loss: 2.231755] time: 0:03:17.049027\n",
      "(30, 32, 32, 3)\n",
      "0.95346594\n",
      "[Epoch 4/5] [Batch 234/360] [D loss: 0.240372] [G loss: 2.333127] time: 0:03:17.156879\n",
      "(30, 32, 32, 3)\n",
      "0.9033706\n",
      "[Epoch 4/5] [Batch 235/360] [D loss: 0.252695] [G loss: 2.260699] time: 0:03:17.264188\n",
      "(30, 32, 32, 3)\n",
      "0.88622683\n",
      "[Epoch 4/5] [Batch 236/360] [D loss: 0.252225] [G loss: 2.502367] time: 0:03:17.371139\n",
      "(30, 32, 32, 3)\n",
      "0.91385865\n",
      "[Epoch 4/5] [Batch 237/360] [D loss: 0.274394] [G loss: 2.340864] time: 0:03:17.475772\n",
      "(30, 32, 32, 3)\n",
      "0.9212323\n",
      "[Epoch 4/5] [Batch 238/360] [D loss: 0.227938] [G loss: 2.471773] time: 0:03:17.584662\n",
      "(30, 32, 32, 3)\n",
      "0.9491778\n",
      "[Epoch 4/5] [Batch 239/360] [D loss: 0.275141] [G loss: 2.239278] time: 0:03:17.690226\n",
      "(30, 32, 32, 3)\n",
      "0.8878932\n",
      "[Epoch 4/5] [Batch 240/360] [D loss: 0.246061] [G loss: 2.976214] time: 0:03:17.805979\n",
      "(30, 32, 32, 3)\n",
      "0.85805243\n",
      "[Epoch 4/5] [Batch 241/360] [D loss: 0.228011] [G loss: 2.440377] time: 0:03:17.910484\n",
      "(30, 32, 32, 3)\n",
      "0.93980867\n",
      "[Epoch 4/5] [Batch 242/360] [D loss: 0.244883] [G loss: 2.415583] time: 0:03:18.018698\n",
      "(30, 32, 32, 3)\n",
      "0.9406324\n",
      "[Epoch 4/5] [Batch 243/360] [D loss: 0.225372] [G loss: 2.593365] time: 0:03:18.123407\n",
      "(30, 32, 32, 3)\n",
      "0.9536087\n",
      "[Epoch 4/5] [Batch 244/360] [D loss: 0.269607] [G loss: 2.517490] time: 0:03:18.232825\n",
      "(30, 32, 32, 3)\n",
      "0.9016718\n",
      "[Epoch 4/5] [Batch 245/360] [D loss: 0.235728] [G loss: 2.361652] time: 0:03:18.343879\n",
      "(30, 32, 32, 3)\n",
      "0.88486797\n",
      "[Epoch 4/5] [Batch 246/360] [D loss: 0.257646] [G loss: 2.489836] time: 0:03:18.450345\n",
      "(30, 32, 32, 3)\n",
      "0.94244576\n",
      "[Epoch 4/5] [Batch 247/360] [D loss: 0.253122] [G loss: 2.177387] time: 0:03:18.555144\n",
      "(30, 32, 32, 3)\n",
      "0.9248468\n",
      "[Epoch 4/5] [Batch 248/360] [D loss: 0.229892] [G loss: 2.371814] time: 0:03:18.663238\n",
      "(30, 32, 32, 3)\n",
      "0.92860395\n",
      "[Epoch 4/5] [Batch 249/360] [D loss: 0.247994] [G loss: 2.257105] time: 0:03:18.771886\n",
      "(30, 32, 32, 3)\n",
      "0.89927703\n",
      "[Epoch 4/5] [Batch 250/360] [D loss: 0.236907] [G loss: 2.586257] time: 0:03:18.879950\n",
      "(30, 32, 32, 3)\n",
      "0.90813047\n",
      "[Epoch 4/5] [Batch 251/360] [D loss: 0.257494] [G loss: 2.224622] time: 0:03:18.984113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.94494915\n",
      "[Epoch 4/5] [Batch 252/360] [D loss: 0.252120] [G loss: 2.424499] time: 0:03:19.091470\n",
      "(30, 32, 32, 3)\n",
      "0.9268792\n",
      "[Epoch 4/5] [Batch 253/360] [D loss: 0.266996] [G loss: 2.318520] time: 0:03:19.199163\n",
      "(30, 32, 32, 3)\n",
      "0.9420886\n",
      "[Epoch 4/5] [Batch 254/360] [D loss: 0.229063] [G loss: 2.435534] time: 0:03:19.308003\n",
      "(30, 32, 32, 3)\n",
      "0.916913\n",
      "[Epoch 4/5] [Batch 255/360] [D loss: 0.249932] [G loss: 2.314553] time: 0:03:19.412295\n",
      "(30, 32, 32, 3)\n",
      "0.94497633\n",
      "[Epoch 4/5] [Batch 256/360] [D loss: 0.228311] [G loss: 2.172278] time: 0:03:19.518632\n",
      "(30, 32, 32, 3)\n",
      "0.90199727\n",
      "[Epoch 4/5] [Batch 257/360] [D loss: 0.239331] [G loss: 2.017995] time: 0:03:19.626408\n",
      "(30, 32, 32, 3)\n",
      "0.8640971\n",
      "[Epoch 4/5] [Batch 258/360] [D loss: 0.231280] [G loss: 2.140234] time: 0:03:19.736607\n",
      "(30, 32, 32, 3)\n",
      "0.9337077\n",
      "[Epoch 4/5] [Batch 259/360] [D loss: 0.212888] [G loss: 2.356901] time: 0:03:19.841343\n",
      "(30, 32, 32, 3)\n",
      "0.9068525\n",
      "[Epoch 4/5] [Batch 260/360] [D loss: 0.260373] [G loss: 2.401643] time: 0:03:19.947548\n",
      "(30, 32, 32, 3)\n",
      "0.88840836\n",
      "[Epoch 4/5] [Batch 261/360] [D loss: 0.254498] [G loss: 2.730549] time: 0:03:20.051417\n",
      "(30, 32, 32, 3)\n",
      "0.91147023\n",
      "[Epoch 4/5] [Batch 262/360] [D loss: 0.267639] [G loss: 2.407614] time: 0:03:20.161360\n",
      "(30, 32, 32, 3)\n",
      "0.88467866\n",
      "[Epoch 4/5] [Batch 263/360] [D loss: 0.246547] [G loss: 2.306147] time: 0:03:20.267164\n",
      "(30, 32, 32, 3)\n",
      "0.8978146\n",
      "[Epoch 4/5] [Batch 264/360] [D loss: 0.259662] [G loss: 2.275792] time: 0:03:20.374428\n",
      "(30, 32, 32, 3)\n",
      "0.9021681\n",
      "[Epoch 4/5] [Batch 265/360] [D loss: 0.260613] [G loss: 2.073773] time: 0:03:20.478462\n",
      "(30, 32, 32, 3)\n",
      "0.93432647\n",
      "[Epoch 4/5] [Batch 266/360] [D loss: 0.200547] [G loss: 2.676456] time: 0:03:20.588024\n",
      "(30, 32, 32, 3)\n",
      "0.9685692\n",
      "[Epoch 4/5] [Batch 267/360] [D loss: 0.317560] [G loss: 2.293133] time: 0:03:20.696940\n",
      "(30, 32, 32, 3)\n",
      "0.92184097\n",
      "[Epoch 4/5] [Batch 268/360] [D loss: 0.209986] [G loss: 2.331728] time: 0:03:20.805617\n",
      "(30, 32, 32, 3)\n",
      "0.89659834\n",
      "[Epoch 4/5] [Batch 269/360] [D loss: 0.276194] [G loss: 2.254539] time: 0:03:20.910942\n",
      "(30, 32, 32, 3)\n",
      "0.9343571\n",
      "[Epoch 4/5] [Batch 270/360] [D loss: 0.243775] [G loss: 2.194627] time: 0:03:21.016943\n",
      "(30, 32, 32, 3)\n",
      "0.94594187\n",
      "[Epoch 4/5] [Batch 271/360] [D loss: 0.247227] [G loss: 2.170620] time: 0:03:21.129262\n",
      "(30, 32, 32, 3)\n",
      "0.8512692\n",
      "[Epoch 4/5] [Batch 272/360] [D loss: 0.241041] [G loss: 2.217118] time: 0:03:21.235825\n",
      "(30, 32, 32, 3)\n",
      "0.9245636\n",
      "[Epoch 4/5] [Batch 273/360] [D loss: 0.267690] [G loss: 2.243378] time: 0:03:21.343128\n",
      "(30, 32, 32, 3)\n",
      "0.93504554\n",
      "[Epoch 4/5] [Batch 274/360] [D loss: 0.235088] [G loss: 2.262623] time: 0:03:21.449000\n",
      "(30, 32, 32, 3)\n",
      "0.9470501\n",
      "[Epoch 4/5] [Batch 275/360] [D loss: 0.265551] [G loss: 2.271593] time: 0:03:21.564754\n",
      "(30, 32, 32, 3)\n",
      "0.87146324\n",
      "[Epoch 4/5] [Batch 276/360] [D loss: 0.244450] [G loss: 2.147891] time: 0:03:21.674192\n",
      "(30, 32, 32, 3)\n",
      "0.9254937\n",
      "[Epoch 4/5] [Batch 277/360] [D loss: 0.271810] [G loss: 2.030331] time: 0:03:21.780606\n",
      "(30, 32, 32, 3)\n",
      "0.94029444\n",
      "[Epoch 4/5] [Batch 278/360] [D loss: 0.219342] [G loss: 2.301158] time: 0:03:21.887129\n",
      "(30, 32, 32, 3)\n",
      "0.90571123\n",
      "[Epoch 4/5] [Batch 279/360] [D loss: 0.272350] [G loss: 2.112422] time: 0:03:21.994684\n",
      "(30, 32, 32, 3)\n",
      "0.92045087\n",
      "[Epoch 4/5] [Batch 280/360] [D loss: 0.235227] [G loss: 2.255770] time: 0:03:22.104351\n",
      "(30, 32, 32, 3)\n",
      "0.9013975\n",
      "[Epoch 4/5] [Batch 281/360] [D loss: 0.278121] [G loss: 2.252219] time: 0:03:22.209955\n",
      "(30, 32, 32, 3)\n",
      "0.90783405\n",
      "[Epoch 4/5] [Batch 282/360] [D loss: 0.217206] [G loss: 2.161617] time: 0:03:22.316872\n",
      "(30, 32, 32, 3)\n",
      "0.9118648\n",
      "[Epoch 4/5] [Batch 283/360] [D loss: 0.247315] [G loss: 2.625047] time: 0:03:22.424949\n",
      "(30, 32, 32, 3)\n",
      "0.9022543\n",
      "[Epoch 4/5] [Batch 284/360] [D loss: 0.267102] [G loss: 2.088384] time: 0:03:22.534827\n",
      "(30, 32, 32, 3)\n",
      "0.92381096\n",
      "[Epoch 4/5] [Batch 285/360] [D loss: 0.237598] [G loss: 2.414913] time: 0:03:22.646563\n",
      "(30, 32, 32, 3)\n",
      "0.90442675\n",
      "[Epoch 4/5] [Batch 286/360] [D loss: 0.250286] [G loss: 2.171389] time: 0:03:22.755316\n",
      "(30, 32, 32, 3)\n",
      "0.92593807\n",
      "[Epoch 4/5] [Batch 287/360] [D loss: 0.223590] [G loss: 2.311342] time: 0:03:22.860942\n",
      "(30, 32, 32, 3)\n",
      "0.9199979\n",
      "[Epoch 4/5] [Batch 288/360] [D loss: 0.268621] [G loss: 2.089593] time: 0:03:22.967304\n",
      "(30, 32, 32, 3)\n",
      "0.93039703\n",
      "[Epoch 4/5] [Batch 289/360] [D loss: 0.233687] [G loss: 2.137875] time: 0:03:23.076131\n",
      "(30, 32, 32, 3)\n",
      "0.9119981\n",
      "[Epoch 4/5] [Batch 290/360] [D loss: 0.257059] [G loss: 2.118205] time: 0:03:23.201665\n",
      "(30, 32, 32, 3)\n",
      "0.9217365\n",
      "[Epoch 4/5] [Batch 291/360] [D loss: 0.261747] [G loss: 2.346765] time: 0:03:23.308169\n",
      "(30, 32, 32, 3)\n",
      "0.8790466\n",
      "[Epoch 4/5] [Batch 292/360] [D loss: 0.230861] [G loss: 2.095414] time: 0:03:23.419992\n",
      "(30, 32, 32, 3)\n",
      "0.9309597\n",
      "[Epoch 4/5] [Batch 293/360] [D loss: 0.264104] [G loss: 2.007534] time: 0:03:23.534886\n",
      "(30, 32, 32, 3)\n",
      "0.86985064\n",
      "[Epoch 4/5] [Batch 294/360] [D loss: 0.232633] [G loss: 2.128462] time: 0:03:23.645185\n",
      "(30, 32, 32, 3)\n",
      "0.9036277\n",
      "[Epoch 4/5] [Batch 295/360] [D loss: 0.265828] [G loss: 2.180539] time: 0:03:23.749603\n",
      "(30, 32, 32, 3)\n",
      "0.9414501\n",
      "[Epoch 4/5] [Batch 296/360] [D loss: 0.223185] [G loss: 2.194852] time: 0:03:23.857589\n",
      "(30, 32, 32, 3)\n",
      "0.8967801\n",
      "[Epoch 4/5] [Batch 297/360] [D loss: 0.260219] [G loss: 2.207450] time: 0:03:23.961388\n",
      "(30, 32, 32, 3)\n",
      "0.93563557\n",
      "[Epoch 4/5] [Batch 298/360] [D loss: 0.249979] [G loss: 2.221442] time: 0:03:24.072532\n",
      "(30, 32, 32, 3)\n",
      "0.9458964\n",
      "[Epoch 4/5] [Batch 299/360] [D loss: 0.247613] [G loss: 2.051967] time: 0:03:24.181551\n",
      "(30, 32, 32, 3)\n",
      "0.9287152\n",
      "[Epoch 4/5] [Batch 300/360] [D loss: 0.238082] [G loss: 2.284930] time: 0:03:24.289475\n",
      "(30, 32, 32, 3)\n",
      "0.9231107\n",
      "[Epoch 4/5] [Batch 301/360] [D loss: 0.243861] [G loss: 2.158851] time: 0:03:24.392831\n",
      "(30, 32, 32, 3)\n",
      "0.88364524\n",
      "[Epoch 4/5] [Batch 302/360] [D loss: 0.258066] [G loss: 2.135091] time: 0:03:24.500201\n",
      "(30, 32, 32, 3)\n",
      "0.9196961\n",
      "[Epoch 4/5] [Batch 303/360] [D loss: 0.246445] [G loss: 2.391934] time: 0:03:24.609607\n",
      "(30, 32, 32, 3)\n",
      "0.9362673\n",
      "[Epoch 4/5] [Batch 304/360] [D loss: 0.251360] [G loss: 2.023306] time: 0:03:24.722757\n",
      "(30, 32, 32, 3)\n",
      "0.938801\n",
      "[Epoch 4/5] [Batch 305/360] [D loss: 0.261381] [G loss: 2.335735] time: 0:03:24.826676\n",
      "(30, 32, 32, 3)\n",
      "0.963192\n",
      "[Epoch 4/5] [Batch 306/360] [D loss: 0.229220] [G loss: 2.170643] time: 0:03:24.934684\n",
      "(30, 32, 32, 3)\n",
      "0.93519014\n",
      "[Epoch 4/5] [Batch 307/360] [D loss: 0.288387] [G loss: 2.329598] time: 0:03:25.044576\n",
      "(30, 32, 32, 3)\n",
      "0.9108865\n",
      "[Epoch 4/5] [Batch 308/360] [D loss: 0.227618] [G loss: 2.304589] time: 0:03:25.152469\n",
      "(30, 32, 32, 3)\n",
      "0.9360443\n",
      "[Epoch 4/5] [Batch 309/360] [D loss: 0.252704] [G loss: 2.246742] time: 0:03:25.263643\n",
      "(30, 32, 32, 3)\n",
      "0.9016709\n",
      "[Epoch 4/5] [Batch 310/360] [D loss: 0.240449] [G loss: 2.159652] time: 0:03:25.371518\n",
      "(30, 32, 32, 3)\n",
      "0.8486722\n",
      "[Epoch 4/5] [Batch 311/360] [D loss: 0.248782] [G loss: 2.204366] time: 0:03:25.483373\n",
      "(30, 32, 32, 3)\n",
      "0.8970342\n",
      "[Epoch 4/5] [Batch 312/360] [D loss: 0.248104] [G loss: 2.084688] time: 0:03:25.592877\n",
      "(30, 32, 32, 3)\n",
      "0.8686547\n",
      "[Epoch 4/5] [Batch 313/360] [D loss: 0.239726] [G loss: 2.448330] time: 0:03:25.702320\n",
      "(30, 32, 32, 3)\n",
      "0.9037223\n",
      "[Epoch 4/5] [Batch 314/360] [D loss: 0.257334] [G loss: 2.114049] time: 0:03:25.812677\n",
      "(30, 32, 32, 3)\n",
      "0.91253996\n",
      "[Epoch 4/5] [Batch 315/360] [D loss: 0.250053] [G loss: 2.247600] time: 0:03:25.916895\n",
      "(30, 32, 32, 3)\n",
      "0.8714723\n",
      "[Epoch 4/5] [Batch 316/360] [D loss: 0.276656] [G loss: 2.095603] time: 0:03:26.040415\n",
      "(30, 32, 32, 3)\n",
      "0.9358941\n",
      "[Epoch 4/5] [Batch 317/360] [D loss: 0.242283] [G loss: 2.644335] time: 0:03:26.144624\n",
      "(30, 32, 32, 3)\n",
      "0.9318935\n",
      "[Epoch 4/5] [Batch 318/360] [D loss: 0.216976] [G loss: 2.480698] time: 0:03:26.252282\n",
      "(30, 32, 32, 3)\n",
      "0.87993264\n",
      "[Epoch 4/5] [Batch 319/360] [D loss: 0.228770] [G loss: 2.537249] time: 0:03:26.356424\n",
      "(30, 32, 32, 3)\n",
      "0.9600544\n",
      "[Epoch 4/5] [Batch 320/360] [D loss: 0.245698] [G loss: 2.531310] time: 0:03:26.462798\n",
      "(30, 32, 32, 3)\n",
      "0.9155984\n",
      "[Epoch 4/5] [Batch 321/360] [D loss: 0.242462] [G loss: 2.331237] time: 0:03:26.570971\n",
      "(30, 32, 32, 3)\n",
      "0.8817382\n",
      "[Epoch 4/5] [Batch 322/360] [D loss: 0.260483] [G loss: 2.756499] time: 0:03:26.682458\n",
      "(30, 32, 32, 3)\n",
      "0.85852224\n",
      "[Epoch 4/5] [Batch 323/360] [D loss: 0.260776] [G loss: 2.320364] time: 0:03:26.793652\n",
      "(30, 32, 32, 3)\n",
      "0.8839076\n",
      "[Epoch 4/5] [Batch 324/360] [D loss: 0.231715] [G loss: 2.277113] time: 0:03:26.904035\n",
      "(30, 32, 32, 3)\n",
      "0.86655194\n",
      "[Epoch 4/5] [Batch 325/360] [D loss: 0.262784] [G loss: 2.739774] time: 0:03:27.018014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 32, 32, 3)\n",
      "0.9334894\n",
      "[Epoch 4/5] [Batch 326/360] [D loss: 0.253511] [G loss: 2.586312] time: 0:03:27.131690\n",
      "(30, 32, 32, 3)\n",
      "0.8869097\n",
      "[Epoch 4/5] [Batch 327/360] [D loss: 0.263157] [G loss: 2.397327] time: 0:03:27.240774\n",
      "(30, 32, 32, 3)\n",
      "0.9348299\n",
      "[Epoch 4/5] [Batch 328/360] [D loss: 0.254202] [G loss: 2.276065] time: 0:03:27.345229\n",
      "(30, 32, 32, 3)\n",
      "0.905732\n",
      "[Epoch 4/5] [Batch 329/360] [D loss: 0.254366] [G loss: 2.216895] time: 0:03:27.448055\n",
      "(30, 32, 32, 3)\n",
      "0.9259682\n",
      "[Epoch 4/5] [Batch 330/360] [D loss: 0.271789] [G loss: 2.299426] time: 0:03:27.556910\n",
      "(30, 32, 32, 3)\n",
      "0.8993947\n",
      "[Epoch 4/5] [Batch 331/360] [D loss: 0.236714] [G loss: 2.502393] time: 0:03:27.664138\n",
      "(30, 32, 32, 3)\n",
      "0.9155021\n",
      "[Epoch 4/5] [Batch 332/360] [D loss: 0.277089] [G loss: 2.262060] time: 0:03:27.770919\n",
      "(30, 32, 32, 3)\n",
      "0.88067466\n",
      "[Epoch 4/5] [Batch 333/360] [D loss: 0.232050] [G loss: 2.257247] time: 0:03:27.874623\n",
      "(30, 32, 32, 3)\n",
      "0.92977285\n",
      "[Epoch 4/5] [Batch 334/360] [D loss: 0.252297] [G loss: 2.213200] time: 0:03:27.982885\n",
      "(30, 32, 32, 3)\n",
      "0.90326613\n",
      "[Epoch 4/5] [Batch 335/360] [D loss: 0.235133] [G loss: 2.573350] time: 0:03:28.087803\n",
      "(30, 32, 32, 3)\n",
      "0.93224233\n",
      "[Epoch 4/5] [Batch 336/360] [D loss: 0.244984] [G loss: 2.317001] time: 0:03:28.193254\n",
      "(30, 32, 32, 3)\n",
      "0.9016037\n",
      "[Epoch 4/5] [Batch 337/360] [D loss: 0.246106] [G loss: 2.640296] time: 0:03:28.296548\n",
      "(30, 32, 32, 3)\n",
      "0.9380309\n",
      "[Epoch 4/5] [Batch 338/360] [D loss: 0.273419] [G loss: 2.436245] time: 0:03:28.400928\n",
      "(30, 32, 32, 3)\n",
      "0.8959573\n",
      "[Epoch 4/5] [Batch 339/360] [D loss: 0.240425] [G loss: 2.512545] time: 0:03:28.508648\n",
      "(30, 32, 32, 3)\n",
      "0.9474409\n",
      "[Epoch 4/5] [Batch 340/360] [D loss: 0.265962] [G loss: 2.123441] time: 0:03:28.614314\n",
      "(30, 32, 32, 3)\n",
      "0.9359538\n",
      "[Epoch 4/5] [Batch 341/360] [D loss: 0.231428] [G loss: 2.368164] time: 0:03:28.719061\n",
      "(30, 32, 32, 3)\n",
      "0.93448895\n",
      "[Epoch 4/5] [Batch 342/360] [D loss: 0.245073] [G loss: 2.128338] time: 0:03:28.830190\n",
      "(30, 32, 32, 3)\n",
      "0.9325586\n",
      "[Epoch 4/5] [Batch 343/360] [D loss: 0.240575] [G loss: 2.181967] time: 0:03:28.938269\n",
      "(30, 32, 32, 3)\n",
      "0.9168339\n",
      "[Epoch 4/5] [Batch 344/360] [D loss: 0.240307] [G loss: 2.077111] time: 0:03:29.045891\n",
      "(30, 32, 32, 3)\n",
      "0.92425495\n",
      "[Epoch 4/5] [Batch 345/360] [D loss: 0.241609] [G loss: 2.067680] time: 0:03:29.150614\n",
      "(30, 32, 32, 3)\n",
      "0.8653509\n",
      "[Epoch 4/5] [Batch 346/360] [D loss: 0.248527] [G loss: 2.130645] time: 0:03:29.256317\n",
      "(30, 32, 32, 3)\n",
      "0.9481054\n",
      "[Epoch 4/5] [Batch 347/360] [D loss: 0.224474] [G loss: 2.095136] time: 0:03:29.359793\n",
      "(30, 32, 32, 3)\n",
      "0.8868727\n",
      "[Epoch 4/5] [Batch 348/360] [D loss: 0.282905] [G loss: 2.155715] time: 0:03:29.467433\n",
      "(30, 32, 32, 3)\n",
      "0.8948123\n",
      "[Epoch 4/5] [Batch 349/360] [D loss: 0.218191] [G loss: 2.126233] time: 0:03:29.574007\n",
      "(30, 32, 32, 3)\n",
      "0.9427829\n",
      "[Epoch 4/5] [Batch 350/360] [D loss: 0.264205] [G loss: 1.915860] time: 0:03:29.679019\n",
      "(30, 32, 32, 3)\n",
      "0.9283388\n",
      "[Epoch 4/5] [Batch 351/360] [D loss: 0.232434] [G loss: 2.272705] time: 0:03:29.786736\n",
      "(30, 32, 32, 3)\n",
      "0.9153884\n",
      "[Epoch 4/5] [Batch 352/360] [D loss: 0.245914] [G loss: 2.129512] time: 0:03:29.896559\n",
      "(30, 32, 32, 3)\n",
      "0.92267036\n",
      "[Epoch 4/5] [Batch 353/360] [D loss: 0.253718] [G loss: 2.028873] time: 0:03:30.001705\n",
      "(30, 32, 32, 3)\n",
      "0.9142515\n",
      "[Epoch 4/5] [Batch 354/360] [D loss: 0.244625] [G loss: 2.179540] time: 0:03:30.108579\n",
      "(30, 32, 32, 3)\n",
      "0.96562177\n",
      "[Epoch 4/5] [Batch 355/360] [D loss: 0.256440] [G loss: 2.266709] time: 0:03:30.213668\n",
      "(30, 32, 32, 3)\n",
      "0.9755351\n",
      "[Epoch 4/5] [Batch 356/360] [D loss: 0.248039] [G loss: 2.163993] time: 0:03:30.318092\n",
      "(30, 32, 32, 3)\n",
      "0.8989432\n",
      "[Epoch 4/5] [Batch 357/360] [D loss: 0.264968] [G loss: 2.340902] time: 0:03:30.426081\n",
      "(30, 32, 32, 3)\n",
      "0.89108616\n",
      "[Epoch 4/5] [Batch 358/360] [D loss: 0.249524] [G loss: 2.006522] time: 0:03:30.537651\n",
      "(30, 32, 32, 3)\n",
      "0.9121402\n",
      "[Epoch 4/5] [Batch 359/360] [D loss: 0.256028] [G loss: 2.050711] time: 0:03:30.644654\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "import numpy as np \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "test_first_imgs, test_last_imgs,_ = next(test_batch_generator)\n",
    "\n",
    "for epoch in range(cfg.NUM_EPOCHS):\n",
    "    steps_per_epoch = (nbr_train_data // cfg.BATCH_SIZE) \n",
    "    for batch_i in range(steps_per_epoch):\n",
    "        first_frames, last_frames,_= next(train_batch_generator)\n",
    "        if first_frames.shape[0] == cfg.BATCH_SIZE: \n",
    "             \n",
    "            # Condition on the first frame and generate the last frame\n",
    "            fake_last_frames = modelObj.generator.predict(first_frames)\n",
    "            #plt.imshow(fake_last_frames[1])\n",
    "            print(fake_last_frames.shape)\n",
    "            #print(tf.keras.backend.mean(fake_last_frames[0]))\n",
    "            print(np.mean(fake_last_frames[0]))\n",
    "\n",
    "            # Train the discriminator with combined loss  \n",
    "            d_loss_real = modelObj.discriminator.train_on_batch([last_frames, first_frames], valid)\n",
    "            d_loss_fake = modelObj.discriminator.train_on_batch([fake_last_frames, first_frames], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    " \n",
    "            # Train the generator\n",
    "            g_loss = modelObj.combined.train_on_batch([last_frames, first_frames], [valid, last_frames])\n",
    "\n",
    "            elapsed_time = datetime.now() - start_time \n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] time: %s\" % (epoch, cfg.NUM_EPOCHS,\n",
    "                                                                                               batch_i,\n",
    "                                                                                               steps_per_epoch,\n",
    "                                                                                               d_loss[0], \n",
    "                                                                                               g_loss[0],\n",
    "                                                                                               elapsed_time))\n",
    "            # run some tests to check how the generated images evolve during training\n",
    "            test_fake_last_imgs = modelObj.generator.predict(test_first_imgs)\n",
    "            test_img_name = output_log_dir + \"/gen_img_epoc_\" + str(epoch) + \".png\"\n",
    "            merged_img = np.vstack((first_frames[0],last_frames[0],fake_last_frames[0]))\n",
    "            imageio.imwrite(test_img_name, img_as_ubyte(merged_img)) #scipy.misc.imsave(test_img_name, merged_img)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) We can test the model with 100 test data which will be saved as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i in range(100):\n",
    "    test_first_imgs, test_last_imgs,_ = next(test_batch_generator)\n",
    "    test_fake_last_imgs = modelObj.generator.predict(test_first_imgs) \n",
    "\n",
    "    test_img_name = output_log_dir + \"/gen_img_test_\" + str(batch_i) + \".png\"\n",
    "    merged_img = np.vstack((test_first_imgs[0],test_last_imgs[0],test_fake_last_imgs[0]))\n",
    "    imageio.imwrite(test_img_name, img_as_ubyte(merged_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1)\n",
    "Update the network architecture given in  **build_generator**  and  **build_discriminator**  of the class GANModel. Please note that the current image resolution is set to 32x32 (i.e. IMAGE_WIDTH and IMAGE_HEIGHT values) in the file configGAN.py. \n",
    "This way initial experiements can run faster. Once you implement the inital version of the network, please set the resolution values back to 128x128. Experimental results should be provided for this high resolution images.  \n",
    "\n",
    "**Hint:** As a generator model, you can use the segmentation model implemented in lab03. Do not forget to adapt the input and output shapes of the generator model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2) \n",
    "Use different **optimization** (e.g. ADAM, SGD, etc) and **regularization** (e.g. data augmentation, dropout) methods to increase the network accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
